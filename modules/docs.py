# modules/docs.py
from __future__ import annotations
import json, datetime, os, uuid, re
from io import BytesIO

# Flask's send_file must be explicitly imported
from flask import Blueprint, request, jsonify, send_file, after_this_request
from db import db
from utils import send_response, jload, jdump, dver, none_if_blank, new_token
from DocxDefinition import get_docx


BASE_DIR = "docxTemp"
os.makedirs(BASE_DIR, exist_ok=True)

bp = Blueprint("docs", __name__)

# ---- Attributes ------------------------------------------------
@bp.post("/init")
def init_doc():
    body = request.get_json(silent=True) or {}
    doc_type = int(body.get("document_type", 0))
    token = new_token()
    with db() as (conn, cur):
        cur.execute("""
          INSERT INTO rms_document_attributes
          (document_type, EIP_id, status, document_token, document_version, issue_date)
          VALUES (%s,%s,%s,%s,1.00,NOW())
        """, (doc_type, None, 0, token))
    return jsonify({"success": True, "token": token})

@bp.post("/attributes/save")
def save_attributes():
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip() or new_token()
    form  = body.get("form") or {}

    # map
    f = {
      "document_type": int(form.get("documentType", 0)),
      "prev_token": none_if_blank(form.get("previousDocumentToken")),
      "doc_id": none_if_blank(form.get("documentID")),
      "doc_name": none_if_blank(form.get("documentName")),
      "doc_ver": dver(form.get("documentVersion", 1.0)),
      "dept": none_if_blank(form.get("department")),
      "author_id": none_if_blank(form.get("author_id")),
      "author": none_if_blank(form.get("author")),
      "approver": none_if_blank(form.get("approver")),
      "confirmer": none_if_blank(form.get("confirmer")),
      "chg_reason": none_if_blank(form.get("reviseReason")),
      "chg_summary": none_if_blank(form.get("revisePoint")),
      "purpose": none_if_blank(form.get("documentPurpose")),
      "attr_json": jdump(form.get("attribute") or {}),
    }

    with db() as (conn, cur):
        cur.execute("""
          UPDATE rms_document_attributes
          SET document_type=%s, previous_document_token=%s,
              document_id=%s, document_name=%s, document_version=%s,
              attribute=%s, department=%s, author_id=%s, author=%s,
              approver=%s, confirmer=%s, change_reason=%s, change_summary=%s, purpose=%s,
              issue_date=NOW()
          WHERE document_token=%s
        """, (f["document_type"], f["prev_token"],
              f["doc_id"], f["doc_name"], f["doc_ver"],
              f["attr_json"], f["dept"], f["author_id"], f["author"],
              f["approver"], f["confirmer"], f["chg_reason"], f["chg_summary"], f["purpose"],
              token))
        if cur.rowcount == 0:
            cur.execute("""
              INSERT INTO rms_document_attributes
              (document_type, EIP_id, status, document_token, previous_document_token,
               document_id, document_name, document_version, attribute, department,
               author_id, author, approver, confirmer, issue_date, change_reason, change_summary, purpose)
              VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s,%s,%s)
            """, (f["document_type"], None, 0, token, f["prev_token"],
                  f["doc_id"], f["doc_name"], f["doc_ver"], f["attr_json"], f["dept"],
                  f["author_id"], f["author"], f["approver"], f["confirmer"],
                  f["chg_reason"], f["chg_summary"], f["purpose"]))

        cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (token,))
        row = cur.fetchone()

        attr = jload(row[8], {}) if row else {}
    resp_form = {
        "documentType": row[0] if row else 0,
        "documentID": row[5] if row else "",
        "documentName": row[6] if row else "",
        "documentVersion": float(row[7] or 1.0) if row else 1.0,
        "attribute": attr,
        "department": row[9] if row else "",
        "author_id": row[10] if row else "",
        "author": row[11] if row else "",
        "approver": row[12] if row else "",
        "confirmer": row[13] if row else "",
        "documentPurpose": row[19] if row else "",
        "reviseReason": row[16] if row else "",
        "revisePoint": row[17] if row else "",
        "previousDocumentToken": row[4] if row else "",  # ğŸ”¸ æ–°å¢
    }

    issue = row[15].strftime("%Y-%m-%d %H:%M:%S") if (row and row[15]) else None
    return jsonify({"success": True, "token": token, "issueTime": issue, "form": resp_form})

@bp.get("/<token>/attributes")
def load_attributes(token):
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (token,))
        r = cur.fetchone()
        if not r: return send_response(404, False, "Not found")
        attr = jload(r.get("attribute"), {}) or {}
        return jsonify({
            "success": True,
            "token": r["document_token"],
            "status": r["status"],
            "issueTime": r["issue_date"].strftime("%Y-%m-%d %H:%M:%S") if r["issue_date"] else None,
            "form": {
                "documentType": r["document_type"],
                "documentID": r["document_id"] or "",
                "documentName": r["document_name"] or "",
                "documentVersion": float(r["document_version"] or 1.0),
                "attribute": attr,
                "department": r["department"] or "",
                "author_id": r["author_id"] or "",
                "author": r["author"] or "",
                "approver": r["approver"] or "",
                "confirmer": r["confirmer"] or "",
                "documentPurpose": r["purpose"] or "",
                "reviseReason": r["change_reason"] or "",
                "revisePoint": r["change_summary"] or "",
                "previousDocumentToken": r["previous_document_token"] or "",  # ğŸ”¸ æ–°å¢
            }
        })

# ---- Dynamic Blocks (generic) --------------------------------
@bp.get("/<token>/blocks")
def load_blocks(token):
    step_type = request.args.get("step_type", type=int)
    if step_type is None:                          # allow 0, only reject missing
        return send_response(400, False, "missing step_type")
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT tier_no, sub_no, content_type, header_json, content_json, files
          FROM rms_block_content
          WHERE document_token=%s AND step_type=%s
          ORDER BY tier_no ASC, sub_no ASC
        """, (token, step_type))
        rows = cur.fetchall() or []

    grouped = {}
    for r in rows:
        t = int(r["tier_no"])
        grouped.setdefault(t, []).append({
            "option": int(r["content_type"]),
            "jsonHeader": jload(r["header_json"]),
            "jsonContent": jload(r["content_json"]),
            "files": jload(r["files"], []) or [],
        })
    data = [{"id": f"{step_type}-{t}", "step": step_type, "tier": t, "data": grouped[t]} for t in sorted(grouped)]
    return jsonify({"success": True, "blocks": data})

# POST /blocks/save
@bp.post("/blocks/save")
def save_blocks():
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip()
    step_type = body.get("step_type")
    if not token or step_type is None:             # allow 0
        return send_response(400, False, "missing token or step_type")
    step_type = int(step_type)

    blocks = body.get("blocks") or []
    with db() as (conn, cur):
        cur.execute("DELETE FROM rms_block_content WHERE document_token=%s AND step_type=%s", (token, step_type))
        ins = """
          INSERT INTO rms_block_content
          (content_id, document_token, step_type, tier_no, sub_no, content_type,
           header_text, header_json, content_text, content_json, files, metadata,
           created_at, updated_at)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())
        """
        for blk in blocks:
            tier = int(blk.get("tier", 1))
            for idx, it in enumerate(blk.get("data") or [], start=1):
                cur.execute(ins, (
                    new_token(), token, step_type, tier, idx, int(it.get("option", 0)),
                    None, jdump(it.get("jsonHeader")), None, jdump(it.get("jsonContent")),
                    jdump(it.get("files") or []), jdump({"source":"dynamic"})
                ))
    return jsonify({"success": True, "count": sum(len(b.get('data') or []) for b in blocks)})

# ---- Manufacturing Condition Rules (step_type = 2) ------------
@bp.post("/params/save")
def save_params():
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip()
    blocks = body.get("blocks") or []
    step_type = int(body.get("step_type", 2))  # default 2 for MCR
    if not token:
        return send_response(400, False, "missing token")

    with db() as (conn, cur):
        # wipe this step
        cur.execute("DELETE FROM rms_block_content WHERE document_token=%s AND step_type=%s", (token, step_type))

        ins = """
          INSERT INTO rms_block_content
          (content_id, document_token, step_type, tier_no, sub_no, content_type,
           header_text, header_json, content_text, content_json, files, metadata,
           created_at, updated_at)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())
        """

        for b in blocks:
            tier = int(b.get("tier_no", 1))
            code = b.get("code") or f"XXXX{tier}"

            # Parameter table (sub 0)
            param_json = b.get("jsonParameterContent")  # TipTap JSON (optional)
            param_arr  = b.get("arrayParameterData") or []  # 2D array
            cur.execute(ins, (
                new_token(), token, step_type, tier, 0, 2,
                code, None,
                jdump(param_arr),  # content_text
                jdump(param_json), # content_json
                jdump([]),         # files
                jdump({"kind": "mcr-parameter", **b.get("metadata", {})})
            ))

            # Condition table (sub 1)
            cond_json = b.get("jsonConditionContent")
            cond_arr  = b.get("arrayConditionData") or []
            cur.execute(ins, (
                new_token(), token, step_type, tier, 1, 2,
                code, None,
                jdump(cond_arr),
                jdump(cond_json),
                jdump([]),
                jdump({"kind": "mcr-condition"})
            ))

    return jsonify({"success": True, "count": len(blocks)})

@bp.get("/<token>/params")
def load_params(token):
    step_type = int(request.args.get("step_type", 2))  # default 2 for MCR
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT tier_no, sub_no, header_text, content_text, content_json, metadata
          FROM rms_block_content
          WHERE document_token=%s AND step_type=%s
          ORDER BY tier_no ASC, sub_no ASC
        """, (token, step_type))
        rows = cur.fetchall() or []

    # Group by tier_no and stitch sub 0/1 back together
    out = {}
    for r in rows:
        t = int(r["tier_no"])
        sub = int(r["sub_no"])
        out.setdefault(t, {
            "code": f"XXXX{t}",
            "jsonParameterContent": None,
            "arrayParameterData": [],
            "jsonConditionContent": None,
            "arrayConditionData": [],
            "metadata": None
        })
        if sub == 0:
            out[t]["code"] = r["header_text"] or out[t]["code"]
            out[t]["arrayParameterData"] = jload(r["content_text"], []) or []
            out[t]["jsonParameterContent"] = jload(r["content_json"])
            out[t]["metadata"] = jload(r["metadata"])
        elif sub == 1:
            out[t]["arrayConditionData"] = jload(r["content_text"], []) or []
            out[t]["jsonConditionContent"] = jload(r["content_json"])

    blocks = []
    for i, t in enumerate(sorted(out.keys()), start=1):
        b = out[t]
        blocks.append({
            "id": f"p-{t}",
            "code": b["code"] or f"XXXX{t}",
            "jsonParameterContent": b["jsonParameterContent"],
            "arrayParameterData": b["arrayParameterData"],
            "jsonConditionContent": b["jsonConditionContent"],
            "arrayConditionData": b["arrayConditionData"],
            "metadata": b["metadata"]
        })

    return jsonify({"success": True, "blocks": blocks})

from oracle_db import ora_cursor  # ä¸‹æ®µè¼ªå·¡æœƒç”¨åˆ°ï¼Œé †ä¾¿å…ˆ import

@bp.post("/revise")
def create_revision():
    """
    å»ºç«‹æ–°ä¸€ç‰ˆï¼š
      - ç”±å‰ä¸€ç‰ˆ previous_token è¤‡è£½ä¸€ä»½
      - document_version + 1.00
      - status = 0 (æ–°çš„è‰ç¨¿)
      - previous_document_token æŒ‡å‘èˆŠ token
      - document_id ç›´æ¥æ²¿ç”¨èˆŠç‰ˆï¼ˆå¯èƒ½æ˜¯ NULLï¼Œè¡¨ç¤ºåˆç‰ˆå°šæœªç”¢ç”Ÿæ–‡ä»¶ï¼‰
    """
    body = request.get_json(silent=True) or {}
    prev_token = (body.get("previous_token") or "").strip()
    if not prev_token:
        return send_response(400, False, "previous_token is required")

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (prev_token,))
        r = cur.fetchone()
        if not r:
            return send_response(404, False, "previous document not found")

        new_token_ = new_token()
        old_ver = float(r["document_version"] or 1.0)
        new_ver = dver(old_ver + 1.0)

        doc_id = r["document_id"]  # ğŸ”¸ è®Šç‰ˆæ²¿ç”¨åŒä¸€å€‹ document_IDï¼ˆå¯èƒ½æ˜¯ NULLï¼‰
        cur.execute("""
          INSERT INTO rms_document_attributes
          (document_type, EIP_id, status, document_token, previous_document_token,
           document_id, document_name, document_version, attribute, department,
           author_id, author, approver, confirmer, issue_date,
           change_reason, change_summary, reject_reason, purpose)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s,%s,%s,%s)
        """, (
            r["document_type"], None, 0, new_token_, prev_token, doc_id, r["document_name"], new_ver, 
            r["attribute"], r["department"], r["author_id"], r["author"], r["approver"], r["confirmer"], "", "", None, r["purpose"],
        ))

        conn.commit()

    return jsonify({
        "success": True,
        "token": new_token_,
        "form": {
            "documentType": r["document_type"],
            "documentID": doc_id or "",
            "documentName": r["document_name"] or "",
            "documentVersion": float(new_ver),
            "attribute": jload(r["attribute"], {}) or {},
            "department": r["department"] or "",
            "author_id": r["author_id"] or "",
            "author": r["author"] or "",
            "approver": r["approver"] or "",
            "confirmer": r["confirmer"] or "",
            "documentPurpose": r["purpose"] or "",
            "reviseReason": "",
            "revisePoint": "",
            "previousDocumentToken": prev_token,
        }
    })

def _status_from_eip_flags(signed_val, rejected_val):
    signed = str(signed_val).upper() == "TRUE"
    rejected = str(rejected_val).upper() == "TRUE"

    if not signed and not rejected:
        return 1  # å·²é€å¯©ï¼ˆEIP æœ‰è³‡æ–™ä½†å°šæœªç°½æ ¸ / é€€å›ï¼‰
    if signed and not rejected:
        return 2  # å·²ç°½æ ¸
    if not signed and rejected:
        return 3  # å·²é€€å›
    # å…¶ä»–çµ„åˆç›®å‰ä¸å®šç¾©ï¼Œå°±ç¶­æŒåŸç‹€
    return None

@bp.post("/sync-eip")
def sync_eip():
    """
    å¾ Oracle IDBUSER.EIP_DOCUMENT_TABLE åŒæ­¥ç‹€æ…‹åˆ° MySQLï¼š
      - ä»¥ (Document_ID, Document_version, Document_name) å°æ‡‰
      - æ›´æ–° EIP_id / status / rejecter / reject_reason
    """
    updated = 0

    # 1) å¾ Oracle æŠ“è³‡æ–™
    with ora_cursor() as cur_ora:
        cur_ora.execute("""
          SELECT
            EIP_ID,
            Document_ID,
            Document_version,
            Document_name,
            signed,
            rejected,
            rejecter,
            rejected_reason
          FROM IDBUSER.EIP_DOCUMENT_TABLE
        """)
        rows = cur_ora.fetchall() or []

    if not rows:
        return jsonify({"success": True, "updated": 0})

    # 2) ä¸€ç­†ä¸€ç­†å°åˆ° MySQL
    with db(dict_cursor=True) as (conn, cur):
        for r in rows:
            # oracledb é è¨­å› tupleï¼Œç…§æ¬„ä½é †åºå–
            eip_id          = r[0]
            doc_id          = r[1]
            doc_ver         = float(r[2])
            doc_name        = r[3]
            signed_val      = r[4]
            rejected_val    = r[5]
            rejecter        = r[6]
            rejected_reason = r[7]

            cur.execute("""
              SELECT document_token, status
              FROM rms_document_attributes
              WHERE document_id=%s
                AND document_version=%s
                AND document_name=%s
            """, (doc_id, doc_ver, doc_name))
            my = cur.fetchone()
            if not my:
                continue

            new_status = _status_from_eip_flags(signed_val, rejected_val)
            if new_status is None:
                continue

            cur.execute("""
              UPDATE rms_document_attributes
              SET EIP_id=%s,
                  status=%s,
                  rejecter=%s,
                  reject_reason=%s
              WHERE document_token=%s
            """, (
                eip_id,
                new_status,
                rejecter if new_status == 3 else None,
                rejected_reason if new_status == 3 else None,
                my["document_token"],
            ))
            updated += 1

        conn.commit()

    return jsonify({"success": True, "updated": updated})

def next_document_id(prefix: str) -> str:
    """
    ä¾ç…§ PROJECT_CODE å‰ä¸‰ç¢¼ + ä¸‰ä½æµæ°´è™Ÿç”¢ç”Ÿ document_idï¼š
      WMA â†’ WMA001, WMA002, ...
    """
    if not prefix or len(prefix) < 3:
        prefix = "XXX"
    prefix = prefix[:3]

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT document_id
          FROM rms_document_attributes
          WHERE document_id LIKE %s
          ORDER BY document_id DESC
          LIMIT 1
        """, (prefix + "%",))
        row = cur.fetchone()

        if not row or not row["document_id"]:
            return f"{prefix}001"

        tail = row["document_id"][-3:]
        try:
            num = int(tail)
        except ValueError:
            num = 0

        return f"{prefix}{num + 1:03d}"

def next_monthly_document_id(prefix: str = "W") -> str:
    """
    ä¾ç…§ W_YY_MM_XXX è¦å‰‡ç”¢ç”Ÿ document_idï¼š
      W_25_11_001, W_25_11_002, ...
    """
    now = datetime.datetime.now()
    yy = now.year % 100
    mm = now.month

    base = f"{prefix}_{yy:02d}_{mm:02d}_"

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT document_id
          FROM rms_document_attributes
          WHERE document_id LIKE %s
          ORDER BY document_id DESC
          LIMIT 1
        """, (base + "%",))
        row = cur.fetchone()

        if not row or not row["document_id"]:
            return f"{base}001"

        tail = row["document_id"][-3:]
        try:
            num = int(tail)
        except ValueError:
            num = 0

        return f"{base}{num + 1:03d}"

@bp.post("/clear-doc-id")
def clear_doc_id():
    """
    å‰ç«¯åœ¨è®Šæ›´é©ç”¨å·¥ç¨‹å¾Œå‘¼å«ï¼Œæ¸…é™¤è©² token çš„ document_idã€‚
    """
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip()
    if not token:
        return send_response(400, False, "missing token")

    with db() as (conn, cur):
        cur.execute("""
          UPDATE rms_document_attributes
          SET document_id=NULL
          WHERE document_token=%s
        """, (token,))
    return jsonify({"success": True})


@bp.get("/drafts")
def list_drafts():
    """
    Query params:
      - user_id      (required):  è¦æŸ¥çš„ä½œè€…/ä½¿ç”¨è€… id -> å°æ‡‰ DB æ¬„ä½ author_id
      - status       (optional):  é è¨­ 0 ç•¶ä½œè‰ç¨¿ï¼›å¦‚éœ€æŸ¥æ ¸/ç™¼ä½ˆå¯æ”¹å€¼
      - keyword      (optional):  é‡å° document_nameã€document_id æ¨¡ç³ŠæŸ¥è©¢
      - page         (optional):  é è¨­ 1
      - page_size    (optional):  é è¨­ 20
      - sort         (optional):  æ’åºæ¬„ä½ï¼Œå…è¨±: issue_date, document_version, document_name
      - order        (optional):  asc/descï¼Œé è¨­ desc
    Response:
      {
        "success": true,
        "items": [
          {
            "documentToken": "...",
            "documentName": "...",
            "documentVersion": 1.20,
            "author": "...",
            "authorId": "...",
            "issueDate": "2025-11-04T18:00:00",
            "documentId": "WMH250"          # æ–¹ä¾¿å‰ç«¯é¡¯ç¤ºï¼ˆå¯æ‹¿æ‰ï¼‰
          }
        ],
        "total": 123,
        "page": 1,
        "pageSize": 20
      }
    """
    user_id   = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    # defaults
    try:
        status    = int(request.args.get("status", 0))
    except ValueError:
        return jsonify({"success": False, "error": "status must be int"}), 400

    keyword   = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_map  = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_key  = request.args.get("sort", "issue_date").lower()
    order     = request.args.get("order", "desc").lower()
    sort_col  = sort_map.get(sort_key, "issue_date")
    order_sql = "DESC" if order not in ("asc", "ASC") else "ASC"

    offset = (page - 1) * page_size

    base_where = ["author_id = %s", "status = %s"]
    params = [user_id, status]

    if keyword:
        base_where.append("(document_name LIKE %s OR document_id LIKE %s)")
        like_kw = f"%{keyword}%"
        params.extend([like_kw, like_kw])

    where_sql = " AND ".join(base_where)

    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM rms_document_attributes
      WHERE {where_sql}
    """

    data_sql = f"""
      SELECT
        document_type, document_token, document_name, document_version, author, author_id, issue_date, document_id
      FROM rms_document_attributes
      WHERE {where_sql}
      ORDER BY {sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (conn, cur):
        # total count
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        # page data
        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(row):
        # issue_date è½‰ ISOï¼ˆæ²’æœ‰å°± Noneï¼‰
        iso_date = None
        if row.get("issue_date"):
            try:
                iso_date = row["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(row["issue_date"])

        # å›å‚³å‰ç«¯éœ€è¦çš„ camelCase
        return {
            "documentType": row["document_type"],
            "documentToken": row["document_token"],
            "documentName": row["document_name"],
            "documentVersion": float(row["document_version"]) if row["document_version"] is not None else None,
            "author": row["author"],
            "authorId": row["author_id"],
            "issueDate": iso_date,
            "documentId": row.get("document_id"),
        }

    items = [to_item(r) for r in rows]

    return jsonify({
        "success": True,
        "items": items,
        "total": total,
        "page": page,
        "pageSize": page_size,
    })

@bp.delete("/<document_token>")
def delete_draft(document_token):
    """
    Delete a draft by its document_token.
    Only rows with status = 0 (draft) can be deleted.

    Path:
      DELETE /docs/<document_token>

    Response:
      200 { success: True, deleted: 1 }
      404 { success: False, error: "not found" }              # no such token
      409 { success: False, error: "not a draft" }            # exists but status != 0
    """
    token = (document_token or "").strip()
    if not token:
        return jsonify({"success": False, "error": "document_token is required"}), 400

    with db(dict_cursor=True) as (conn, cur):
        # Is there a record?
        cur.execute("SELECT status FROM rms_document_attributes WHERE document_token=%s", (token,))
        row = cur.fetchone()

        if not row:
            return jsonify({"success": False, "error": "not found"}), 404

        # Only allow deleting drafts
        if int(row.get("status", 1)) != 0:
            return jsonify({"success": False, "error": "not a draft"}), 409

        # Delete
        cur.execute("DELETE FROM rms_document_attributes WHERE document_token=%s AND status=0", (token,))
        conn.commit()
        deleted = cur.rowcount or 0

    # (Optional) clean temp files if you keep any by token under BASE_DIR
    try:
        # Example: remove /docxTemp/<token>.docx if you create such files.
        # from pathlib import Path
        # p = Path(BASE_DIR) / f"{token}.docx"
        # if p.exists():
        #     p.unlink()
        pass
    except Exception:
        # Non-fatal: ignore file cleanup errors
        pass

    return jsonify({"success": True, "deleted": deleted}), 200

def _build_keyword_predicate(keyword: str):
    """
    Returns (sql_snippet, params) for robust keyword search.
    - Matches: document_name, author, document_id (LIKE)
    - Also matches document_version:
        * if keyword is numeric (int/float), add exact equality on document_version
        * always also add LIKE(cast(document_version as char)) for partial text matches
    """
    if not keyword:
        return "", []

    likes = []
    params = []

    # name / id / author LIKE
    likes.append("document_name LIKE %s")
    params.append(f"%{keyword}%")
    likes.append("document_id LIKE %s")
    params.append(f"%{keyword}%")
    likes.append("author LIKE %s")
    params.append(f"%{keyword}%")

    # version: support numeric equality + textual LIKE
    numeric = None
    try:
        numeric = float(keyword)
    except Exception:
        pass

    # MySQL: CAST(document_version AS CHAR) for LIKE
    likes.append("CAST(document_version AS CHAR) LIKE %s")
    params.append(f"%{keyword}%")

    eq = []
    if numeric is not None:
        eq.append("document_version = %s")
        params.append(numeric)

    # Combine
    if eq:
        where_piece = "(" + " OR ".join(likes + eq) + ")"
    else:
        where_piece = "(" + " OR ".join(likes) + ")"
    return where_piece, params

def _parse_doc_types(s: str | None) -> list[str] | None:
    """
    Accepts:
      - None / ""  -> no filtering
      - single or comma list: "Instruction", "Specification", or mix
    Returns a normalized list using DB values: ["Instruction", "Specification"].
    Raises ValueError if any entry is invalid.
    """
    if s is None or str(s).strip() == "":
        return None

    allowed = {
        "instruction": 0,
        "specification": 1,
    }
    out = []
    for part in str(s).split(","):
        key = part.strip().lower()
        if not key:
            continue
        if key not in allowed:
            raise ValueError("document_type must be in {Instruction, Specification}")
        out.append(allowed[key])
    if not out:
        return None
    return out

def _parse_statuses(v):
    """
    Accepts either:
      - single int string: "0"
      - comma list: "1,3"
    Returns a validated list of ints (subset of {0,1,2,3}), or raises ValueError.
    """
    if v is None:
        raise ValueError("status is required")
    try:
        parts = [p.strip() for p in str(v).split(",")]
        nums = [int(p) for p in parts if p != ""]
    except Exception:
        raise ValueError("status must be int or comma-separated ints")
    allowed = {0, 1, 2, 3}
    for n in nums:
        if n not in allowed:
            raise ValueError("status must be in {0,1,2,3}")
    if not nums:
        raise ValueError("status is required")
    return nums

def _parse_statuses(s: str) -> list[int]:
    if s is None or str(s).strip() == "":
        raise ValueError("status is required")
    out = []
    for part in str(s).split(","):
        part = part.strip()
        if not part:
            continue
        try:
            out.append(int(part))
        except ValueError:
            raise ValueError(f"invalid status: {part}")
    if not out:
        raise ValueError("status is required")
    return out

def _list_documents_impl(
    *,
    user_id: str | None,         # allow None for "all" search
    statuses: list[int],
    keyword: str = "",
    page: int = 1,
    page_size: int = 20,
    sort_key: str = "issue_date",
    order: str = "desc",
    doc_types: list[str] | None = None,
    scope: str = "mine",         # "mine" | "all"
):
    sort_map = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_col = sort_map.get((sort_key or "issue_date").lower(), "issue_date")
    order_sql = "DESC" if (order or "desc").lower() not in ("asc", "ASC") else "ASC"

    where = []
    params = []

    # scope
    if scope == "mine":
        if not user_id:
            raise ValueError("user_id is required for scope=mine")
        where.append("author_id = %s")
        params.append(user_id)

    # statuses (required)
    where.append(f"status IN ({', '.join(['%s'] * len(statuses))})")
    params.extend(statuses)

    # doc types (optional)
    if doc_types:
        where.append(f"document_type IN ({', '.join(['%s'] * len(doc_types))})")
        params.extend(doc_types)

    # robust keyword
    kw_sql, kw_params = _build_keyword_predicate(keyword)
    if kw_sql:
        where.append(kw_sql)
        params.extend(kw_params)

    where_sql = " AND ".join(where) if where else "1=1"
    offset = (page - 1) * page_size

    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM rms_document_attributes
      WHERE {where_sql}
    """
    data_sql = f"""
      SELECT
        document_type,
        document_token,
        document_name,
        document_version,
        author,
        author_id,
        issue_date,
        document_id,
        status,
        rejecter,
        reject_reason
      FROM rms_document_attributes
      WHERE {where_sql}
      ORDER BY {sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (_, cur):
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(r):
        iso_date = None
        if r.get("issue_date"):
            try:
                iso_date = r["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(r["issue_date"])
        return {
            "documentType": r["document_type"],
            "documentToken": r["document_token"],
            "documentName": r["document_name"],
            "documentVersion": float(r["document_version"]) if r["document_version"] is not None else None,
            "author": r["author"],
            "authorId": r["author_id"],
            "issueDate": iso_date,
            "documentId": r.get("document_id"),
            "status": r.get("status"),
            "rejecter": r.get("rejecter"),
            "rejectReason": r.get("reject_reason"),
        }

    return {
        "success": True,
        "items": [to_item(r) for r in rows],
        "total": total,
        "page": page,
        "pageSize": page_size,
    }

@bp.get("/all")
def list_all_documents():
    # statuses required (same as /documents)
    try:
        statuses = _parse_statuses(request.args.get("status"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=None,           # no author filter â†’ all authors
        statuses=statuses,
        keyword=keyword,        # strong search: name/author/version/id
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
        doc_types=None,         # <â€” IMPORTANT: do not filter by type
        scope="all",
    )
    return jsonify(data), 200

@bp.get("/passed")
def list_passed():
    # å›ºå®š status = 2 (é€šé/å·²ç°½æ ¸)
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    # document_type: optional ("Instruction", "Specification"), comma-separated ok
    try:
        doc_types = _parse_doc_types(request.args.get("document_type"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=user_id,
        statuses=[2],              # <â€” PASSED
        keyword=keyword,
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
        doc_types=doc_types,       # <â€” filter if provided
    )
    return jsonify(data), 200

@bp.get("/documents")
def list_documents():
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    try:
        statuses = _parse_statuses(request.args.get("status"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=user_id,
        statuses=statuses,
        keyword=keyword,
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
        scope="mine",
    )
    return jsonify(data), 200

@bp.get("/submitted")
def list_submitted():
    # å›ºå®š status = 1
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=user_id,
        statuses=[1],
        keyword=keyword,
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
    )
    return jsonify(data), 200

@bp.get("/rejected")
def list_rejected():
    # å›ºå®š status = 3
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=user_id,
        statuses=[3],
        keyword=keyword,
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
    )
    return jsonify(data), 200

# ---- References ----------------------------------------------
@bp.post("/references/save")
def save_references():
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip()
    if not token: return send_response(400, False, "missing token")
    documents = body.get("documents") or []
    forms     = body.get("forms") or []
    with db() as (conn, cur):
        cur.execute("DELETE FROM rms_references WHERE document_token=%s", (token,))
        ins = """
          INSERT INTO rms_references (document_token, refer_type, refer_document, refer_document_name, created_at)
          VALUES (%s,%s,%s,%s,NOW())
        """
        for d in documents:
            cur.execute(ins, (token, 0, (d.get("docId") or "").strip(), (d.get("docName") or "").strip()))
        for f in forms:
            cur.execute(ins, (token, 1, (f.get("formId") or "").strip(), (f.get("formName") or "").strip()))
    return jsonify({"success": True})

@bp.get("/<token>/references")
def load_references(token):
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT refer_type, refer_document, refer_document_name
          FROM rms_references WHERE document_token=%s ORDER BY refer_type ASC, id ASC
        """, (token,))
        rows = cur.fetchall() or []
    docs, forms = [], []
    for r in rows:
        if int(r["refer_type"]) == 0:
            docs.append({"docId": r["refer_document"], "docName": r["refer_document_name"]})
        else:
            forms.append({"formId": r["refer_document"], "formName": r["refer_document_name"]})
    return jsonify({"success": True, "documents": docs, "forms": forms})

def _build_doc_payload_from_token(token: str) -> dict:
    """
    çµ¦å®š document_tokenï¼š
      - çµ„å‡º data["attribute"]ï¼šç›®å‰ç‰ˆæœ¬ + æœ€å¤š 2 å€‹å‰ç‰ˆæœ¬ï¼ˆåªéœ€è¦ attribute / åŸºæœ¬æ¬„ä½ï¼‰
      - çµ„å‡º data["content"]ï¼šåªæœ‰ã€Œç›®å‰é€™ä¸€ä»½æ–‡ä»¶ã€çš„å…§å®¹ blocks + åƒæ•¸ blocks
      - çµ„å‡º data["reference"]ï¼šç›®å‰é€™ä¸€ä»½æ–‡ä»¶çš„ reference åˆ—è¡¨
    é€™å€‹çµæ§‹æœƒç›´æ¥ä¸Ÿçµ¦ get_docx ä½¿ç”¨ã€‚
    """
    with db(dict_cursor=True) as (conn, cur):
        # ---------- 1) attributesï¼šæ²¿ previous_document_token å¾€å›è¿½ ----------
        attrs = []
        hops = 0
        seen = set()
        current_token = token

        while current_token and current_token not in seen and hops < 3:  # ç›®å‰ + æœ€å¤š 2 ä»½èˆŠç‰ˆ = 3
            seen.add(current_token)
            cur.execute(
                "SELECT * FROM rms_document_attributes WHERE document_token=%s",
                (current_token,),
            )
            r = cur.fetchone()
            if not r:
                break

            attr_json = jload(r.get("attribute"), {}) or {}

            # é€™è£¡æˆ‘å€‘çµ„æˆä¸€å€‹ã€Œformã€é•·ç›¸çš„ dictï¼Œå°é½Šä½ å‰ç«¯é€é€² generate/word çš„çµæ§‹
            attrs.append({
                "documentType":     r["document_type"],
                "documentID":       r["document_id"] or "",
                "documentName":     r["document_name"] or "",
                "documentVersion":  float(r["document_version"] or 1.0),
                "attribute":        attr_json,                     # å“ç›® / å·¥ç¨‹ / å¼æ¨£ç­‰ç­‰
                "department":       r["department"] or "",
                "author_id":        r["author_id"] or "",
                "author":           r["author"] or "",
                "approver":         r["approver"] or "",
                "confirmer":        r["confirmer"] or "",
                "issueDate":        r["issue_date"].strftime("%Y/%m/%d") if r["issue_date"] else "",
                "reviseReason":     r["change_reason"] or "",
                "revisePoint":      r["change_summary"] or "",
                "documentPurpose":  r["purpose"] or "",
            })

            current_token = r.get("previous_document_token")
            hops += 1

        # attrs ç›®å‰æ˜¯ [æœ€æ–°, å‰ä¸€ç‰ˆ, å‰å‰ç‰ˆ...]ï¼Œç‚ºäº†è®“ REV1/2/3 æ¯”è¼ƒåƒã€Œç”±èˆŠåˆ°æ–°ã€ï¼Œ
        # æˆ‘å€‘å¯ä»¥ reverse ä¸€ä¸‹ï¼Œæœ€å¾Œä¸€å€‹å°±æ˜¯ get_docx çœ‹åˆ°çš„ã€Œæœ€æ–°ã€ã€‚
        attrs.reverse()
        if not attrs:
            raise ValueError("document not found")

        # ---------- 2) contentï¼šåªæœ‰ã€Œç›®å‰é€™ä»½ã€çš„ blocks + åƒæ•¸ ----------
        cur.execute("""
            SELECT step_type, tier_no, sub_no, content_type,
                   header_text, header_json,
                   content_text, content_json,
                   files, metadata
            FROM rms_block_content
            WHERE document_token=%s
            ORDER BY step_type ASC, tier_no ASC, sub_no ASC
        """, (token,))
        rows = cur.fetchall() or []

        # ä¸€èˆ¬ blocksï¼ˆè£½é€ æµç¨‹ / ç®¡ç†æ¢ä»¶ / å“è³ªå…§å®¹ / å…¶ä»– ç­‰ï¼‰
        block_groups = {}      # key = (step_type, tier_no)
        # åƒæ•¸ blocksï¼ˆstep_type 2: è£½é€ æ¢ä»¶åƒæ•¸ä¸€è¦½è¡¨ / 5: è£½é€ åƒæ•¸ä¸€è¦½è¡¨ï¼‰
        param_groups = {}      # key = tier_no

        for r in rows:
            st  = int(r["step_type"])
            t   = int(r["tier_no"])
            sub = int(r["sub_no"])

            # åƒæ•¸é¡ï¼šè·Ÿ load_params çš„é‚è¼¯ä¸€æ¨£ï¼ŒæŠŠ sub 0/1 ç¸«å›å»
            if st in (2, 5):
                g = param_groups.setdefault(t, {
                    "step_type":            st,
                    "tier_no":              t,
                    "code":                 f"XXXX{t}",
                    "jsonParameterContent": None,
                    "arrayParameterData":   [],
                    "jsonConditionContent": None,
                    "arrayConditionData":   [],
                    "metadata":             None,
                })
                if sub == 0:
                    g["code"]                 = r["header_text"] or g["code"]
                    g["arrayParameterData"]   = jload(r["content_text"], []) or []
                    g["jsonParameterContent"] = jload(r["content_json"])
                    g["metadata"]             = jload(r["metadata"])
                elif sub == 1:
                    g["arrayConditionData"]   = jload(r["content_text"], []) or []
                    g["jsonConditionContent"] = jload(r["content_json"])
                continue

            # ä¸€èˆ¬å…§å®¹é¡ï¼šè·Ÿ /<token>/blocks çš„ grouped çµæ§‹ä¸€æ¨£
            g = block_groups.setdefault((st, t), {
                "step_type": st,
                "tier":      t,
                "data":      [],
            })
            g["data"].append({
                "option":      int(r["content_type"]),
                "jsonHeader":  jload(r["header_json"]),
                "jsonContent": jload(r["content_json"]),
                "files":       jload(r["files"], []) or [],
            })

        contents = []
        # blocks æŒ‰ step_type, tier_no æ’åº
        for (st, t) in sorted(block_groups.keys()):
            contents.append(block_groups[(st, t)])
        # åƒæ•¸ blocks æŒ‰ tier æ’åº
        for t in sorted(param_groups.keys()):
            contents.append(param_groups[t])

        # ---------- 3) references ----------
        cur.execute("""
            SELECT refer_type, refer_document, refer_document_name
            FROM rms_references
            WHERE document_token=%s
            ORDER BY refer_type ASC, id ASC
        """, (token,))
        ref_rows = cur.fetchall() or []
        references = [
            {
                "referenceType":        int(r["refer_type"]),
                "referenceDocumentID":  r["refer_document"],
                "referenceDocumentName": r["refer_document_name"],
            }
            for r in ref_rows
        ]

    return {
        "attribute": attrs,     # list[form-like dict]
        "content":   contents,  # list[blocks + params]
        "reference": references,
    }

@bp.get("/view/<token>/docx")
def view_docx_from_token(token):
    """
    ä¾ document_token å¾ DB æ’ˆå‡º attribute/content/referenceï¼Œ
    ä¸²æˆ payload ä¸Ÿçµ¦ get_docxï¼Œç”¢ç”Ÿä¸€ä»½æš«å­˜ DOCXï¼Œ
    å›å‚³çµ¦å‰ç«¯åšã€Œå…¨é é è¦½ã€ï¼ˆå‰ç«¯ç›´æ¥ window.open é€™å€‹ URLï¼‰ã€‚
    """
    try:
        data = _build_doc_payload_from_token(token)
    except Exception as e:
        print("[view_docx_from_token] error:", e)
        return jsonify({"ok": False, "error": "document not found"}), 404

    # æª”åï¼šå„ªå…ˆç”¨æ–‡ä»¶åç¨± / ç·¨è™Ÿ
    try:
        attr_last = data["attribute"][-1]
        raw_name  = attr_last.get("documentName") or attr_last.get("documentID") or token
        doc_name  = _safe_docname(raw_name)
    except Exception:
        doc_name = token

    # æš«å­˜ç›®éŒ„
    view_dir = os.path.join(BASE_DIR, "_view")
    os.makedirs(view_dir, exist_ok=True)

    fname    = f"{doc_name}-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.docx"
    out_path = os.path.join(view_dir, fname)

    # ç”¢ç”Ÿ Word
    if data["attribute"][-1]["documentType"] == 1:
        get_docx(out_path, data, "docx-template/example4.docx")
    else:
        get_docx(out_path, data)

    # å›å‚³å¾Œåˆªæ‰æš«å­˜æª”
    @after_this_request
    def remove_file(response):
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception as e:
            print("[view_docx_from_token] remove temp file error:", e)
        return response

    return send_file(
        out_path,
        as_attachment=False,  # ğŸ”‘ ä¸å¼·åˆ¶ä¸‹è¼‰ï¼Œè®“ç€è¦½å™¨ï¼ç³»çµ±è‡ªå·±æ±ºå®šç”¨ä»€éº¼é–‹
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

def _safe_docname(name: str) -> str:
    name = (name or "").strip()
    if not name:
        return "document"
    # ç°¡å–®å»æ‰ä¸é©åˆç•¶æª”åçš„å­—å…ƒ
    name = re.sub(r'[\\/:*?"<>|]+', "_", name)
    return name[:80]

@bp.post("/generate/word")
def generate_word():
    """
    Accept JSON body {token, attribute, content, reference}ï¼š
    - è‹¥æœ‰ tokenï¼š
        1) ç”¨ _build_doc_payload_from_token(token) æŠŠã€Œå‰å¹¾ç‰ˆ + ç›®å‰ç‰ˆã€æ’ˆå‡ºä¾†
        2) ç”¨å‰ç«¯å‚³é€²ä¾†çš„æœ€æ–° attribute/content/reference è¦†è“‹ã€Œæœ€æ–°é‚£ä¸€ç‰ˆã€
        3) è‹¥ç‚ºåˆç‰ˆä¸”å°šç„¡ document_id â†’ ä¾é©ç”¨å·¥ç¨‹å‰ä¸‰ç¢¼ç”¢ç”Ÿä¸€å€‹ï¼Œå¯«å› DB
    - è‹¥æ²’æœ‰ tokenï¼šé€€å›èˆŠè¡Œç‚ºï¼Œç›´æ¥ç”¨ body çš„è³‡æ–™ç”¢ç”Ÿ Word
    """
    if not request.is_json:
        return jsonify({"ok": False, "error": "JSON body required"}), 400

    data = request.get_json(silent=True) or {}
    data.setdefault("attribute", [])
    data.setdefault("content", [])
    data.setdefault("reference", [])

    token = (data.get("token") or "").strip()

    # -------------------------------------------------------
    # A) æœ‰ tokenï¼šèµ°ã€ŒDB + å‰å¹¾ç‰ˆã€è·¯ç·š
    # -------------------------------------------------------
    if token:
        try:
            payload = _build_doc_payload_from_token(token)  # {attribute, content, reference}
        except Exception as e:
            print("[generate_word] _build_doc_payload_from_token error:", e)
            return send_response(404, False, "document not found")

        # 1) å…ˆæŠ“å‡ºæœ€æ–°é‚£ä¸€ç‰ˆï¼ˆattribute æœ€å¾Œä¸€å€‹ï¼‰
        latest_attr = payload["attribute"][-1]

        # 2) è‹¥å‰ç«¯æœ‰å‚³ attributeï¼Œå°±ç”¨æœ€å¾Œä¸€å€‹è¦†è“‹ã€Œæœ€æ–°é‚£ä¸€ç‰ˆã€çš„æ¬„ä½
        if data["attribute"]:
            override_attr = data["attribute"][-1]
            # åªè¦†è“‹æœ‰å®šç¾©çš„ keyï¼Œé¿å…æ•´å€‹ä¸Ÿæ‰å‰å¹¾ç‰ˆå¿…é ˆæ¬„ä½
            for k, v in override_attr.items():
                # å¦‚æœæƒ³ä¿ç•™å‰å¹¾ç‰ˆè³‡è¨Šï¼Œåªå‹• attribute / documentPurpose / reviseReason ç­‰æ¬„ä½
                latest_attr[k] = v

        # 3) è‹¥å‰ç«¯æœ‰ content/referenceï¼Œä»£è¡¨ä½¿ç”¨è€…ç›®å‰ç•«é¢æœ‰ã€Œæœ€æ–°è‰ç¨¿ã€å…§å®¹ï¼Œè¦è¦†è“‹ DB å…§å®¹
        if data["content"]:
            payload["content"] = data["content"]
        if data["reference"]:
            payload["reference"] = data["reference"]

        # ---------------------------------------------------
        # 4) è¨ˆç®—/æ›´æ–° document_idï¼ˆåªçœ‹æœ€æ–°é‚£ä¸€ç‰ˆï¼‰
        # ---------------------------------------------------
        with db(dict_cursor=True) as (conn, cur):
            cur.execute("""
            SELECT document_type, document_id, document_version, attribute
            FROM rms_document_attributes
            WHERE document_token=%s
            """, (token,))
            r = cur.fetchone()
            if not r:
                return send_response(404, False, "document not found")

            doc_type = int(r["document_type"] or 0)
            doc_id   = r["document_id"]
            doc_ver  = float(r["document_version"] or 1.0)
            attr_json = jload(r["attribute"], {}) or {}

            latest_attr_json = latest_attr.get("attribute") or {}
            attr_json.update(latest_attr_json)

            # åˆç‰ˆä¸”å°šç„¡ document_id â†’ ä¾æ–‡ä»¶é¡å‹æ±ºå®šç·¨ç¢¼è¦å‰‡
            if doc_ver == 1.0 and not doc_id:
                if doc_type == 1:
                    # Specificationï¼šW_YY_MM_XXX
                    doc_id = next_monthly_document_id("W")
                else:
                    # Instructionï¼šé©ç”¨å·¥ç¨‹å‰ä¸‰ç¢¼ + æµæ°´è™Ÿ
                    apply_project = (attr_json.get("applyProject") or "").strip()
                    prefix = (apply_project[:3] or "XXX").upper()
                    doc_id = next_document_id(prefix)

            cur.execute("""UPDATE rms_document_attributes SET document_id=%s, attribute=%s WHERE document_token=%s""", (doc_id, jdump(attr_json), token))
            conn.commit()

        # 5) æŠŠ docID å¡å›æœ€æ–°é‚£ä¸€ç‰ˆçµ¦ get_docx ç”¨
        latest_attr["documentID"] = doc_id or ""
        if data["attribute"]:
            data["attribute"][-1]["documentID"] = doc_id or ""

        # 6) æª”åï¼šç”¨æœ€æ–°é‚£ä¸€ç‰ˆ
        try:
            # doc_name = _safe_docname(latest_attr.get("documentName") or latest_attr.get("documentID") or doc_id or "document")
            print(f'1 {latest_attr.get("documentName")}{latest_attr.get("documentVersion"):.1f}')
            doc_name = _safe_docname(f'{latest_attr.get("documentName")}{latest_attr.get("documentVersion"):.1f}')
        except Exception:
            doc_name = "document"

        print(f'3 doc_name: {doc_name}')
        out_path = os.path.join(BASE_DIR, f"{doc_name}.docx")
        # ç”¢ç”Ÿ Word
        if data["attribute"][-1]["documentType"] == 1:
            get_docx(out_path, data, "docx-template/example4.docx")
        else:
            get_docx(out_path, data)

        @after_this_request
        def add_docid_header(response):
            if doc_id:
                response.headers["X-Document-ID"] = doc_id
            # è®“ç€è¦½å™¨å…è¨± JS è®€å–é€™å€‹è‡ªè¨‚ headerï¼ˆè·¨ç¶²åŸŸæƒ…æ³ä¸‹å¾ˆé‡è¦ï¼‰
            existing = response.headers.get("Access-Control-Expose-Headers", "")
            expose = "X-Document-ID"
            if existing:
                # é¿å…é‡è¤‡ï¼ŒåŠ åœ¨å¾Œé¢
                if expose not in existing:
                    response.headers["Access-Control-Expose-Headers"] = existing + "," + expose
            else:
                response.headers["Access-Control-Expose-Headers"] = expose
            return response


        return send_file(
            out_path,
            as_attachment=True,
            download_name=f"{doc_name}.docx",
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    # -------------------------------------------------------
    # B) æ²’æœ‰ tokenï¼šä¿ç•™èˆŠçš„ fallback è¡Œç‚º
    # -------------------------------------------------------
    # é€™æ”¯åˆ†æ”¯å¯ä»¥å¾ˆç°¡å–®ï¼šæ²¿ç”¨ä½ ä¹‹å‰çš„ generate_word å¯«æ³•ï¼ˆä¸æ•´åˆ DBï¼‰
    try:
        attr_last = data["attribute"][-1]
        # doc_name = _safe_docname(attr_last.get("documentName") or attr_last.get("documentID") or "document")
        doc_name = _safe_docname(f'{attr_last.get("documentName")}{attr_last.get("documentVersion"):.1f}')
        print(f'2 {attr_last.get("documentName")}{attr_last.get("documentVersion"):.1f}')
    except Exception:
        doc_name = "document"

    print(f'4 doc_name: {doc_name}')
    out_path = os.path.join(BASE_DIR, f"{doc_name}.docx")
    # ç”¢ç”Ÿ Word
    if data["attribute"][-1]["documentType"] == 1:
        get_docx(out_path, data, "docx-template/example4.docx")
    else:
        get_docx(out_path, data)

    return send_file(
        out_path,
        as_attachment=True,
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

@bp.post("/preview/docx")
def preview_docx():
    """
    æ¥æ”¶ {token?, attribute?, content?, reference?}ï¼š
      - è‹¥æœ‰ tokenï¼š
          1) å…ˆç”¨ _build_doc_payload_from_token(token) â†’ å¸¶å‡ºå‰å¹¾ç‰ˆ + ç›®å‰ç‰ˆ
          2) å‰ç«¯è‹¥å‚³ attribute/content/referenceï¼Œå°±è¦†è“‹ã€Œæœ€æ–°é‚£ä¸€ç‰ˆã€åŠå…¶å…§å®¹
      - è‹¥ç„¡ tokenï¼š
          ä¿ç•™èˆŠè¡Œç‚ºï¼Œç›´æ¥ç”¨ body çš„è³‡æ–™ previewã€‚
    """
    if not request.is_json:
        return jsonify({"ok": False, "error": "JSON body required"}), 400

    data = request.get_json(silent=True) or {}
    data.setdefault("attribute", [])
    data.setdefault("content", [])
    data.setdefault("reference", [])

    token = (data.get("token") or "").strip()

    # -------------------------------------------------------
    # A) æœ‰ tokenï¼šç”¨ DB + å‰å¹¾ç‰ˆ + å‰ç«¯è¦†è“‹æœ€æ–°ç‰ˆ
    # -------------------------------------------------------
    if token:
        try:
            payload = _build_doc_payload_from_token(token)
        except Exception as e:
            print("[preview_docx] _build_doc_payload_from_token error:", e)
            return jsonify({"ok": False, "error": "document not found"}), 404

        latest_attr = payload["attribute"][-1]

        # å‰ç«¯è‹¥æœ‰å‚³ attributeï¼Œå°±è¦†è“‹æœ€æ–°ç‰ˆæ¬„ä½
        if data["attribute"]:
            override_attr = data["attribute"][-1]
            for k, v in override_attr.items():
                latest_attr[k] = v

        # content/reference è‹¥å‰ç«¯æœ‰å‚³ï¼Œå°±è¦†è“‹ DB çš„
        if data["content"]:
            payload["content"] = data["content"]
        if data["reference"]:
            payload["reference"] = data["reference"]

        base_payload = payload

    else:
        # ---------------------------------------------------
        # B) æ²’ tokenï¼šç¶­æŒèˆŠæœ‰è¡Œç‚ºï¼Œç›´æ¥ç”¨ body
        # ---------------------------------------------------
        base_payload = data


    # ç”¢ç”Ÿä¸€å€‹ payload_idï¼Œç•¶æš«å­˜æª”åçš„ä¸€éƒ¨åˆ†
    payload_id = f"{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"

    # å–æª”æ¡ˆåç¨±ï¼šå„ªå…ˆç”¨ã€Œæœ€æ–°ç‰ˆã€çš„æ–‡ä»¶åç¨± / æ–‡ç®¡ç·¨è™Ÿ
    try:
        if base_payload["attribute"]:
            attr_last = base_payload["attribute"][-1]
        else:
            attr_last = {}
        raw_name = attr_last.get("documentName") or attr_last.get("documentID") or payload_id
        doc_name = _safe_docname(raw_name)
    except Exception:
        doc_name = payload_id

    preview_dir = os.path.join(BASE_DIR, "_preview")
    os.makedirs(preview_dir, exist_ok=True)

    out_path = os.path.join(preview_dir, f"{doc_name}-{payload_id}.docx")

    # ç”¢ç”Ÿ Word
    if data["attribute"][-1]["documentType"] == 1:
        get_docx(out_path, data, "docx-template/example4.docx")
    else:
        get_docx(out_path, data)

    @after_this_request
    def remove_file(response):
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception as e:
            print("[preview_docx] remove temp file error:", e)
        return response

    return send_file(
        out_path,
        as_attachment=False,
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )
