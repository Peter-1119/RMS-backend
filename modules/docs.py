# modules/docs.py
from __future__ import annotations
import datetime, os, uuid, re, json
from decimal import Decimal

# Flask's send_file must be explicitly imported
from flask import Blueprint, request, jsonify, send_file, after_this_request
from db import db
from oracle_db import ora_cursor as odb
from utils import send_response, jload, jdump, dver, none_if_blank, new_token
from DocxDefinition import get_docx

BASE_DIR = "docxTemp"
os.makedirs(BASE_DIR, exist_ok=True)

bp = Blueprint("docs", __name__)

LOCK_STATUS_SET = {"å¯©æ ¸ä¸­", "å·²ç°½æ ¸", "ä½œå»¢", "å¦æ±º", "é€€å›ç”³è«‹è€…"}
STATUS_MAP = {"å¯©æ ¸ä¸­": 1, "æ­£å¸¸çµæ¡ˆ": 2, "ä½œå»¢": 3, "å¦æ±º": 4, "é€€å›ç”³è«‹è€…": 5}

# ---- Attributes ------------------------------------------------
@bp.post("/init")
def init_doc():
    body = request.get_json(silent=True) or {}
    doc_type = int(body.get("document_type", 0))
    token = new_token()
    with db() as (conn, cur):
        cur.execute("""
          INSERT INTO rms_document_attributes
          (document_type, EIP_id, status, document_token, document_version, issue_date)
          VALUES (%s,%s,%s,%s,1.00,NOW())
        """, (doc_type, None, 0, token))
    return jsonify({"success": True, "token": token})

@bp.get("/get-personnel")
def get_personnel():
    emp_id = request.args.get("emp_id")
    if emp_id == None:
        return send_response(400, True, "å·¥è™Ÿæœªæä¾›", {"message": "è«‹æä¾›å·¥è™Ÿ"})
    
    try:
        with odb() as cur:
            sql = f"""
                SELECT A.EMP_NO, A.EMPNAME, A.IN_DATE, C.EMP_NO, C.EMPNAME, B.LEV, E.EMP_NO, E.EMPNAME, D.LEV FROM IDBUSER.RMS_USERS A
                INNER JOIN IDBUSER.RMS_DEPT B ON A.DEPT_NO = B.DEPT_NO
                LEFT JOIN IDBUSER.RMS_USERS C ON B.LEADER_EMP_ID = C.EMP_NO
                LEFT JOIN IDBUSER.RMS_DEPT D ON B.GL_DEPARTMENT_CODE = D.DEPT_NO
                LEFT JOIN IDBUSER.RMS_USERS E ON D.LEADER_EMP_ID = E.EMP_NO
                WHERE A.OUT_DATE IS NULL AND A.EMP_NO = '{emp_id}'
            """
            cur.execute(sql)
            personnelInfo = cur.fetchall()[0]
    
    except Exception as e:
        print(f"error result: {e}")
        return send_response(400, True, "è«‹æ±‚è³‡æ–™", {"message": "ç„¡æ³•å–å¾—äººå“¡è³‡æ–™ï¼Œè«‹é‡æ–°å˜—è©¦"})

    personnel = {"confirmer": personnelInfo[4], "approver": personnelInfo[7]}
    return send_response(200, True, "è«‹æ±‚æˆåŠŸ", {"personnel": personnel})

@bp.post("/draft/save-all")
def save_draft_all():
    """
    ä¸€æ¬¡æŠŠï¼š
      - attributes
      - å¤šå€‹ step_type çš„ blocks
      - å¤šå€‹ step_type çš„ params
      - references
    å…¨éƒ¨å­˜èµ·ä¾†ï¼ˆå–®ä¸€ transactionï¼‰
    body å½¢ç‹€å¤§è‡´ç‚ºï¼š
    {
      "token": "...",
      "form": {...},                # åŸæœ¬ save_attributes form
      "blockRequests": [            # å°æ‡‰åŸæœ¬ /blocks/save
        { "step_type": 0, "blocks": [...] },
        { "step_type": 1, "blocks": [...] },
        ...
      ],
      "paramRequests": [            # å°æ‡‰åŸæœ¬ /params/save
        { "step_type": 2, "blocks": [...] },
        { "step_type": 5, "blocks": [...] },
      ],
      "references": {               # å°æ‡‰åŸæœ¬ /references/save
        "documents": [...],
        "forms": [...]
      }
    }
    """
    body = request.get_json(silent=True) or {}

    token = (body.get("token") or "").strip() or new_token()
    form  = body.get("form") or {}
    block_requests = body.get("blockRequests") or []
    param_requests = body.get("paramRequests") or []
    refs          = body.get("references") or {}

    # ğŸ”’ å…ˆæª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨ EIP ç”¢ç”Ÿæ­£å¼ç‹€æ…‹
    # æ³¨æ„ï¼šå¦‚æœæ˜¯æ–°å»ºã€ç¬¬ä¸€æ¬¡å„²å­˜ï¼Œtoken å¯èƒ½é‚„æŸ¥ä¸åˆ° document_idï¼Œis_document_locked æœƒå› False
    if is_document_locked(token):
        return send_response(
            409, False,
            "æ­¤æ–‡ä»¶å·²é€å‡ºæˆ–å·²çµæ¡ˆï¼Œç¦æ­¢å†ä¿®æ”¹è‰ç¨¿å…§å®¹ã€‚è«‹é‡æ–°é–‹å•Ÿæ–°ç‰ˆæœ¬ã€‚",
            {"message": "EIP ç‹€æ…‹å·²æ›´æ–°ï¼Œç„¡æ³•å†å„²å­˜ã€‚"}
        )

    # ---------- 1) attributesï¼šæ²¿ç”¨ä½ åŸæœ¬ save_attributes çš„ mapping ----------
    f = {
        "document_type": int(form.get("documentType", 0) or 0),
        "prev_token": none_if_blank(form.get("previousDocumentToken")),
        "doc_id": none_if_blank(form.get("documentID")),
        "doc_name": none_if_blank(form.get("documentName")),
        "doc_ver": dver(form.get("documentVersion", 1.0)),
        "dept": none_if_blank(form.get("department")),
        "author_id": none_if_blank(form.get("author_id")),
        "author": none_if_blank(form.get("author")),
        "approver": none_if_blank(form.get("approver")),
        "confirmer": none_if_blank(form.get("confirmer")),
        "chg_reason": none_if_blank(form.get("reviseReason")),
        "chg_summary": none_if_blank(form.get("revisePoint")),
        "purpose": none_if_blank(form.get("documentPurpose")),
        "attr_json": jdump(form.get("attribute") or {}),
    }

    issue_time_str = None
    resp_form = None

    with db() as (conn, cur):

        # --- 1.1 upsert attributesï¼ˆè·Ÿ save_attributes å¹¾ä¹ä¸€æ¨£ï¼‰ ---
        cur.execute("""
          UPDATE rms_document_attributes
          SET document_type=%s, previous_document_token=%s,
              document_id=%s, document_name=%s, document_version=%s,
              attribute=%s, department=%s, author_id=%s, author=%s,
              approver=%s, confirmer=%s, change_reason=%s, change_summary=%s, purpose=%s,
              issue_date=NOW()
          WHERE document_token=%s
        """, (f["document_type"], f["prev_token"],
              f["doc_id"], f["doc_name"], f["doc_ver"],
              f["attr_json"], f["dept"], f["author_id"], f["author"],
              f["approver"], f["confirmer"], f["chg_reason"], f["chg_summary"], f["purpose"],
              token))

        if cur.rowcount == 0:
            cur.execute("""
              INSERT INTO rms_document_attributes
              (document_type, EIP_id, status, document_token, previous_document_token,
               document_id, document_name, document_version, attribute, department,
               author_id, author, approver, confirmer, issue_date, change_reason, change_summary, purpose)
              VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s,%s,%s)
            """, (f["document_type"], None, 0, token, f["prev_token"],
                  f["doc_id"], f["doc_name"], f["doc_ver"], f["attr_json"], f["dept"],
                  f["author_id"], f["author"], f["approver"], f["confirmer"],
                  f["chg_reason"], f["chg_summary"], f["purpose"]))

        # é‡æ–°æ’ˆä¸€æ¬¡ rowï¼Œç”¨ä¾†å›å‚³ issueTime & form
        cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (token,))
        row = cur.fetchone()
        if row:
            # æ³¨æ„ï¼šé€™è£¡æ²¿ç”¨ä½ åŸæœ¬ save_attributes çš„ index å¯«æ³•
            attr = jload(row[8], {}) or {}
            issue_time_str = row[15].strftime("%Y-%m-%d %H:%M:%S") if row[15] else None
            resp_form = {
                "documentType": row[0] or 0,
                "documentID": row[5] or "",
                "documentName": row[6] or "",
                "documentVersion": float(row[7] or 1.0),
                "attribute": attr,
                "department": row[9] or "",
                "author_id": row[10] or "",
                "author": row[11] or "",
                "approver": row[12] or "",
                "confirmer": row[13] or "",
                "documentPurpose": row[19] or "",
                "reviseReason": row[16] or "",
                "revisePoint": row[17] or "",
                "previousDocumentToken": row[4] or "",
            }

        # ---------- 2) blocksï¼šæŠŠå¤šå€‹ step_type ä¸€æ¬¡è™•ç† ----------
        ins_block_sql = """
          INSERT INTO rms_block_content
          (content_id, document_token, step_type, tier_no, sub_no, content_type,
           header_text, header_json, content_text, content_json, files, metadata,
           created_at, updated_at)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())
        """

        for br in block_requests:
            step_type = br.get("step_type", None)
            if step_type is None:
                continue
            step_type = int(step_type)
            blocks = br.get("blocks") or []

            # å…ˆæ¸…æ‰è©² step_type çš„èˆŠè³‡æ–™
            cur.execute(
                "DELETE FROM rms_block_content WHERE document_token=%s AND step_type=%s",
                (token, step_type)
            )

            # å†ä¾ç…§ä½ åŸæœ¬ /blocks/save çš„é‚è¼¯ insert
            for blk in blocks:
                tier = int(blk.get("tier", 1))
                for idx, it in enumerate(blk.get("data") or [], start=1):
                    cur.execute(ins_block_sql, (
                        new_token(), token, step_type, tier, idx,
                        int(it.get("option", 0)),
                        None,
                        jdump(it.get("jsonHeader")),
                        None,
                        jdump(it.get("jsonContent")),
                        jdump(it.get("files") or []),
                        jdump({"source": "dynamic"}),
                    ))

        # ---------- 3) paramsï¼šå¤šå€‹ step_type ä¸€æ¬¡è™•ç† ----------
        ins_param_sql = """
          INSERT INTO rms_block_content
          (content_id, document_token, step_type, tier_no, sub_no, content_type,
           header_text, header_json, content_text, content_json, files, metadata,
           created_at, updated_at)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())
        """

        for pr in param_requests:
            step_type = int(pr.get("step_type", 2))
            blocks = pr.get("blocks") or []

            # å…ˆæ¸…æ‰è©² step çš„èˆŠè³‡æ–™
            cur.execute(
                "DELETE FROM rms_block_content WHERE document_token=%s AND step_type=%s",
                (token, step_type)
            )

            for b in blocks:
                tier = int(b.get("tier_no", 1))

                # sub 0 : parameter
                param_json = b.get("jsonParameterContent")
                param_arr  = b.get("arrayParameterData") or []
                meta = b.get("metadata") or {}

                cur.execute(ins_param_sql, (
                    new_token(), token, step_type, tier, 0, 2,
                    None, None,
                    jdump(param_arr),
                    jdump(param_json),
                    jdump([]),
                    jdump({"kind": "mcr-parameter", **meta}),
                ))

                # sub 1 : conditionï¼ˆåªæœ‰ step_type == 2 çš„ MCR æ‰æœ‰ï¼‰
                if step_type == 2:
                    cond_json = b.get("jsonConditionContent")
                    cond_arr  = b.get("arrayConditionData") or []
                    cur.execute(ins_param_sql, (
                        new_token(), token, step_type, tier, 1, 2,
                        None, None,
                        jdump(cond_arr),
                        jdump(cond_json),
                        jdump([]),
                        jdump({"kind": "mcr-condition", **meta}),
                    ))

        # ---------- 4) references ----------
        documents = refs.get("documents") or []
        forms     = refs.get("forms")     or []

        # å…ˆåˆªé™¤å†æ–°å¢
        cur.execute("DELETE FROM rms_references WHERE document_token=%s", (token,))
        if documents or forms:
            ins_ref_sql = """
              INSERT INTO rms_references
              (document_token, refer_type, refer_document, refer_document_name, created_at)
              VALUES (%s,%s,%s,%s,NOW())
            """
            for d in documents:
                cur.execute(ins_ref_sql, (
                    token, 0,
                    (d.get("docId") or "").strip(),
                    (d.get("docName") or "").strip()
                ))
            for f_ in forms:
                cur.execute(ins_ref_sql, (
                    token, 1,
                    (f_.get("formId") or "").strip(),
                    (f_.get("formName") or "").strip()
                ))

    # transaction çµæŸ
    return jsonify({
        "success": True,
        "token": token,
        "issueTime": issue_time_str,
        "form": resp_form,
    })

def is_document_locked(token: str) -> bool:
    """
    è‹¥æ­¤ token å°æ‡‰çš„æ–‡ä»¶å·²åœ¨ EIP æœ‰ä»»ä½•ç‹€æ…‹ï¼Œå°±é–ä½ã€‚
   ï¼ˆé¿å…ä½¿ç”¨è€…åœ¨ç€è¦½å™¨æ²’é—œçš„æƒ…æ³ä¸‹ç¹¼çºŒå­˜è‰ç¨¿ï¼Œç ´å£å¿«ç…§ä¸€è‡´æ€§ï¼‰
    """
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            SELECT document_id, document_version
            FROM rms_document_attributes
            WHERE document_token=%s
        """, (token,))
        row = cur.fetchone()

    if not row:
        return False  # æ‰¾ä¸åˆ°å°±ç•¶æ²’é–ï¼ˆä¹Ÿå¯ä»¥é¸æ“‡ raiseï¼‰

    doc_id = (row["document_id"] or "").strip()
    doc_ver = float(row["document_version"] or 1.0)

    if not doc_id:
        # é‚„æ²’ç”¢ Word â†’ ä¸€å®šæ²’æœ‰ EIP ç´€éŒ„
        return False
    
    # print(f"doc_id: {doc_id}, doc_ver: {doc_ver}")

    # æŸ¥ Oracle
    with odb() as cur_o:
        cur_o.execute(f"""
            SELECT EIP_STATUS, EIP_CREATEDT, EIPNO FROM IDBUSER.RMS_DCC2EIP
            WHERE RMS_DCCNO = '{doc_id}' AND EIP_STATUS = 'å·²ç°½æ ¸' AND RMS_VER = '{int(doc_ver)}'
            ORDER BY EIP_CREATEDT DESC
        """)
        r = cur_o.fetchone()

    if not r:
        return False

    eip_status = (r[0] or "").strip()
    eip_created = r[1]
    eipno = (r[2] or "").strip()

    # åªè¦æœ‰ä»»ä¸€æŒ‡æ¨™ï¼Œå°±ç•¶ä½œå·²é€² EIP æµç¨‹ â†’ é–ä½
    if eip_status in LOCK_STATUS_SET or eip_created or eipno:
        return True

    return False

@bp.get("/<token>/draft-all")
def load_draft_all(token):
    """
    Query string:
      - attrs=0/1 (é è¨­ 1)
      - blocks=0,1,3,4,...
      - params=2,5,...
      - refs=0/1 (é è¨­ 1)
    å›å‚³ï¼š
    {
      "success": true,
      "token": "...",
      "attributes": { success, status, issueTime, form },
      "blocks": {
        "0": { success, blocks:[...] },
        "1": { success, blocks:[...] },
        ...
      },
      "params": {
        "2": { success, blocks:[...] },
        "5": { success, blocks:[...] },
        ...
      },
      "references": { success, documents, forms }
    }
    """
    include_attrs = (request.args.get("attrs", "1") != "0")
    block_str = (request.args.get("blocks") or "").strip()
    param_str = (request.args.get("params") or "").strip()
    include_refs = (request.args.get("refs", "1") != "0")

    block_steps = []
    if block_str:
        for p in block_str.split(","):
            p = p.strip()
            if p:
                try:
                    block_steps.append(int(p))
                except ValueError:
                    pass

    param_steps = []
    if param_str:
        for p in param_str.split(","):
            p = p.strip()
            if p:
                try:
                    param_steps.append(int(p))
                except ValueError:
                    pass

    out = {
        "success": True,
        "token": token,
        "attributes": None,
        "blocks": {},
        "params": {},
        "references": None,
    }

    with db(dict_cursor=True) as (conn, cur):

        # ---------- 1) attributes ----------
        if include_attrs:
            cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (token,))
            r = cur.fetchone()
            if not r:
                out["attributes"] = {"success": False, "message": "Not found"}
            else:
                attr = jload(r.get("attribute"), {}) or {}
                issue = r["issue_date"].strftime("%Y-%m-%d %H:%M:%S") if r["issue_date"] else None
                out["attributes"] = {
                    "success": True,
                    "token": r["document_token"],
                    "status": r["status"],
                    "issueTime": issue,
                    "form": {
                        "documentType": r["document_type"],
                        "documentID": r["document_id"] or "",
                        "documentName": r["document_name"] or "",
                        "documentVersion": float(r["document_version"] or 1.0),
                        "attribute": attr,
                        "department": r["department"] or "",
                        "author_id": r["author_id"] or "",
                        "author": r["author"] or "",
                        "approver": r["approver"] or "",
                        "confirmer": r["confirmer"] or "",
                        "documentPurpose": r["purpose"] or "",
                        "reviseReason": r["change_reason"] or "",
                        "revisePoint": r["change_summary"] or "",
                        "previousDocumentToken": r["previous_document_token"] or "",
                    },
                }

        # ---------- 2) blocks ----------
        for st in block_steps:
            cur.execute("""
              SELECT tier_no, sub_no, content_type, header_json, content_json, files FROM rms_block_content
              WHERE document_token=%s AND step_type=%s
              ORDER BY tier_no ASC, sub_no ASC
            """, (token, st))
            rows = cur.fetchall() or []

            grouped = {}
            for r in rows:
                t = int(r["tier_no"])
                grouped.setdefault(t, []).append({
                    "option": int(r["content_type"]),
                    "jsonHeader": jload(r["header_json"]),
                    "jsonContent": jload(r["content_json"]),
                    "files": jload(r["files"], []) or [],
                })

            data = [{"id": f"{st}-{t}", "step": st, "tier": t, "data": grouped[t]} for t in sorted(grouped)]
            out["blocks"][str(st)] = {"success": True, "blocks": data}

        # ---------- 3) params ----------
        for st in param_steps:
            cur.execute("""
              SELECT tier_no, sub_no, header_text, content_text, content_json, metadata FROM rms_block_content
              WHERE document_token=%s AND step_type=%s
              ORDER BY tier_no ASC, sub_no ASC
            """, (token, st))
            rows = cur.fetchall() or []

            merged = {}
            for r in rows:
                t = int(r["tier_no"])
                sub = int(r["sub_no"])
                merged.setdefault(t, {
                    "code": f"XXXX{t}",
                    "jsonParameterContent": None,
                    "arrayParameterData": [],
                    "jsonConditionContent": None,
                    "arrayConditionData": [],
                    "metadata": None,
                })
                if sub == 0:
                    merged[t]["code"] = r["header_text"] or merged[t]["code"]
                    merged[t]["arrayParameterData"] = jload(r["content_text"], []) or []
                    merged[t]["jsonParameterContent"] = jload(r["content_json"])
                    merged[t]["metadata"] = jload(r["metadata"])
                elif sub == 1:
                    merged[t]["arrayConditionData"] = jload(r["content_text"], []) or []
                    merged[t]["jsonConditionContent"] = jload(r["content_json"])

            blocks = []
            for i, t in enumerate(sorted(merged.keys()), start=1):
                b = merged[t]
                blocks.append({
                    "id": f"p-{t}",
                    "code": b["code"] or f"XXXX{t}",
                    "jsonParameterContent": b["jsonParameterContent"],
                    "arrayParameterData": b["arrayParameterData"],
                    "jsonConditionContent": b["jsonConditionContent"],
                    "arrayConditionData": b["arrayConditionData"],
                    "metadata": b["metadata"],
                })

            out["params"][str(st)] = {"success": True, "blocks": blocks}

        # ---------- 4) references ----------
        if include_refs:
            cur.execute("""
              SELECT refer_type, refer_document, refer_document_name FROM rms_references
              WHERE document_token=%s
              ORDER BY refer_type ASC, id ASC
            """, (token,))
            rows = cur.fetchall() or []

            docs, forms = [], []
            for r in rows:
                if int(r["refer_type"]) == 0:
                    docs.append({
                        "docId": r["refer_document"],
                        "docName": r["refer_document_name"],
                    })
                else:
                    forms.append({
                        "formId": r["refer_document"],
                        "formName": r["refer_document_name"],
                    })
            out["references"] = {
                "success": True,
                "documents": docs,
                "forms": forms,
            }

    return jsonify(out)

@bp.get("/<token>/snapshot-draft-all")
def load_snapshot_draft_all(token):
    """
    å¾ rms_document_snapshots è®€å¿«ç…§è³‡æ–™ã€‚
    æ”¯æ´ Query string:
      - attrs=0/1
      - blocks=0,1,3,...
      - params=2,5,...
      - refs=0/1
      - rms_id=xxx   â˜… æ–°å¢ï¼Œç”¨ä¾†é–å®šæŸä¸€å¼µ RMS å–®å°æ‡‰çš„ snapshot
    """
    include_attrs = (request.args.get("attrs", "1") != "0")
    block_str = (request.args.get("blocks") or "").strip()
    param_str = (request.args.get("params") or "").strip()
    include_refs = (request.args.get("refs", "1") != "0")
    rms_id = (request.args.get("rms_id") or "").strip()

    block_steps = []
    if block_str:
        for p in block_str.split(","):
            p = p.strip()
            if not p:
                continue
            try:
                block_steps.append(int(p))
            except ValueError:
                pass

    param_steps = []
    if param_str:
        for p in param_str.split(","):
            p = p.strip()
            if not p:
                continue
            try:
                param_steps.append(int(p))
            except ValueError:
                pass

    # ---------- å…ˆæŠ“ snapshot row ----------
    with db(dict_cursor=True) as (conn, cur):
        where = ["document_token = %s"]
        params = [token]

        if rms_id:
            # å¦‚æœæœ‰å¸¶ rms_idï¼Œå°±é–å®šåœ¨é€™å¼µ RMS å–®çš„ snapshot
            where.append("rms_id = %s")
            params.append(rms_id)

        where_sql = " AND ".join(where)

        cur.execute(f"""
            SELECT *
            FROM rms_document_snapshots
            WHERE {where_sql}
            ORDER BY created_at DESC
            LIMIT 1
        """, params)
        snap = cur.fetchone()

    if not snap:
        return jsonify({
            "success": False,
            "message": "snapshot not found for this token / rms_id"
        }), 404

    # ä¸‹é¢ç…§ä½ åŸæœ¬çš„é‚è¼¯å°±å¥½
    doc_row   = jload(snap["document_row"], {}) or {}
    blocks_rs = jload(snap["blocks_rows"], []) or []
    refs_rs   = jload(snap["references_rows"], []) or []

    out = {
        "success": True,
        "token": token,
        "attributes": None,
        "blocks": {},
        "params": {},
        "references": None,
    }

    # ---------- 1) attributes ----------
    if include_attrs:
        issue = doc_row.get("issue_date")
        if hasattr(issue, "strftime"):
            issue_str = issue.strftime("%Y-%m-%d %H:%M:%S")
        else:
            issue_str = issue

        attr_json = jload(doc_row.get("attribute"), {}) or {}

        out["attributes"] = {
            "success": True,
            "token": doc_row.get("document_token") or token,
            "status": doc_row.get("status"),
            "issueTime": issue_str,
            "form": {
                "documentType": doc_row.get("document_type") or 0,
                "documentID": doc_row.get("document_id") or "",
                "documentName": doc_row.get("document_name") or "",
                "documentVersion": float(doc_row.get("document_version") or 1.0),
                "attribute": attr_json,
                "department": doc_row.get("department") or "",
                "author_id": doc_row.get("author_id") or "",
                "author": doc_row.get("author") or "",
                "approver": doc_row.get("approver") or "",
                "confirmer": doc_row.get("confirmer") or "",
                "documentPurpose": doc_row.get("purpose") or "",
                "reviseReason": doc_row.get("change_reason") or "",
                "revisePoint": doc_row.get("change_summary") or "",
                "previousDocumentToken": doc_row.get("previous_document_token") or "",
            },
        }

    # ---------- 2) blocks ----------
    by_step = {}
    for r in blocks_rs:
        try:
            st = int(r.get("step_type"))
        except (TypeError, ValueError):
            continue
        by_step.setdefault(st, []).append(r)

    for st in block_steps:
        rows = by_step.get(st, [])
        grouped = {}
        for r in rows:
            t = int(r.get("tier_no"))
            grouped.setdefault(t, []).append({
                "option": int(r.get("content_type") or 0),
                "jsonHeader": _normalize_metadata(r.get("header_json")),
                "jsonContent": _normalize_metadata(r.get("content_json")),
                "files": _normalize_metadata(r.get("files")) or [],
            })

        data = [{
            "id": f"{st}-{t}",
            "step": st,
            "tier": t,
            "data": grouped[t]
        } for t in sorted(grouped.keys())]

        out["blocks"][str(st)] = {"success": True, "blocks": data}

    # ---------- 3) params ----------
    for st in param_steps:
        rows = [r for r in blocks_rs if int(r.get("step_type")) == st]
        merged = {}
        for r in rows:
            t = int(r.get("tier_no"))
            sub = int(r.get("sub_no"))
            merged.setdefault(t, {
                "code": f"XXXX{t}",
                "jsonParameterContent": None,
                "arrayParameterData": [],
                "jsonConditionContent": None,
                "arrayConditionData": [],
                "metadata": None,
            })
            if sub == 0:
                merged[t]["code"] = r.get("header_text") or merged[t]["code"]
                merged[t]["arrayParameterData"] = jload(r.get("content_text"), []) or []
                merged[t]["jsonParameterContent"] = _normalize_metadata(r.get("content_json"))
                merged[t]["metadata"] = _normalize_metadata(r.get("metadata"))
            elif sub == 1:
                merged[t]["arrayConditionData"] = jload(r.get("content_text"), []) or []
                merged[t]["jsonConditionContent"] = _normalize_metadata(r.get("content_json"))

        blocks = []
        for t in sorted(merged.keys()):
            b = merged[t]
            blocks.append({
                "id": f"p-{t}",
                "code": b["code"] or f"XXXX{t}",
                "jsonParameterContent": b["jsonParameterContent"],
                "arrayParameterData": b["arrayParameterData"],
                "jsonConditionContent": b["jsonConditionContent"],
                "arrayConditionData": b["arrayConditionData"],
                "metadata": b["metadata"],
            })

        out["params"][str(st)] = {"success": True, "blocks": blocks}

    # ---------- 4) references ----------
    if include_refs:
        docs, forms = [], []
        for r in refs_rs:
            if int(r.get("refer_type") or 0) == 0:
                docs.append({
                    "docId": r.get("refer_document"),
                    "docName": r.get("refer_document_name"),
                })
            else:
                forms.append({
                    "formId": r.get("refer_document"),
                    "formName": r.get("refer_document_name"),
                })
        out["references"] = {
            "success": True,
            "documents": docs,
            "forms": forms,
        }

    return jsonify(out)

@bp.post("/revise")
def create_revision():
    """
    å»ºç«‹æ–°ä¸€ç‰ˆï¼š
      - ç”±å‰ä¸€ç‰ˆ previous_token è¤‡è£½ä¸€ä»½
      - document_version + 1.00
      - status = 0 (æ–°çš„è‰ç¨¿)
      - previous_document_token æŒ‡å‘èˆŠ token
      - document_id ç›´æ¥æ²¿ç”¨èˆŠç‰ˆï¼ˆå¯èƒ½æ˜¯ NULLï¼Œè¡¨ç¤ºåˆç‰ˆå°šæœªç”¢ç”Ÿæ–‡ä»¶ï¼‰
      - ğŸ”¥ åŒæ™‚è¤‡è£½ blocks / references åˆ°æ–° token
    """
    body = request.get_json(silent=True) or {}
    prev_token = (body.get("previous_token") or "").strip()
    if not prev_token:
        return send_response(400, False, "previous_token is required")

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("SELECT * FROM rms_document_attributes WHERE document_token=%s", (prev_token,))
        r = cur.fetchone()
        if not r:
            return send_response(404, False, "previous document not found")

        new_token_ = new_token()
        old_ver = float(r["document_version"] or 1.0)
        new_ver = dver(old_ver + 1.0)

        doc_id = r["document_id"]  # ğŸ”¸ è®Šç‰ˆæ²¿ç”¨åŒä¸€å€‹ document_IDï¼ˆå¯èƒ½æ˜¯ NULLï¼‰
        # 1) æ–°å¢ attributes
        cur.execute("""
          INSERT INTO rms_document_attributes
          (document_type, EIP_id, status, document_token, previous_document_token,
           document_id, document_name, document_version, attribute, department,
           author_id, author, approver, confirmer, issue_date,
           change_reason, change_summary, reject_reason, purpose)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),%s,%s,%s,%s)
        """, (
            r["document_type"], None, 0, new_token_, prev_token,
            doc_id, r["document_name"], new_ver,
            r["attribute"], r["department"], r["author_id"], r["author"],
            r["approver"], r["confirmer"],
            "", "", None, r["purpose"],
        ))

        # 2) è¤‡è£½ blocksï¼ˆæµç¨‹ / ç®¡ç†æ¢ä»¶ / MCR / ç•°å¸¸è™•ç½®...ï¼‰
        cur.execute("""
          SELECT step_type, tier_no, sub_no, content_type,
                 header_text, header_json, content_text, content_json, files, metadata
          FROM rms_block_content
          WHERE document_token = %s
        """, (prev_token,))
        old_blocks = cur.fetchall() or []

        ins_blk_sql = """
          INSERT INTO rms_block_content
          (content_id, document_token, step_type, tier_no, sub_no, content_type,
           header_text, header_json, content_text, content_json, files, metadata,
           created_at, updated_at)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,NOW(),NOW())
        """
        for b in old_blocks:
            cur.execute(ins_blk_sql, (
                new_token(),            # æ–° content_id
                new_token_,             # ğŸ”¥ æ”¹æˆæ–° token
                b["step_type"],
                b["tier_no"],
                b["sub_no"],
                b["content_type"],
                b["header_text"],
                b["header_json"],
                b["content_text"],
                b["content_json"],
                b["files"],
                b["metadata"],
            ))

        # 3) è¤‡è£½ references
        cur.execute("""
          SELECT refer_type, refer_document, refer_document_name
          FROM rms_references
          WHERE document_token = %s
        """, (prev_token,))
        old_refs = cur.fetchall() or []

        ins_ref_sql = """
          INSERT INTO rms_references
          (document_token, refer_type, refer_document, refer_document_name, created_at)
          VALUES (%s,%s,%s,%s,NOW())
        """
        for r_ref in old_refs:
            cur.execute(ins_ref_sql, (
                new_token_,
                r_ref["refer_type"],
                r_ref["refer_document"],
                r_ref["refer_document_name"],
            ))

        conn.commit()

    return jsonify({
        "success": True,
        "token": new_token_,
        "form": {
            "documentType": r["document_type"],
            "documentID": doc_id or "",
            "documentName": r["document_name"] or "",
            "documentVersion": float(new_ver),
            "attribute": jload(r["attribute"], {}) or {},
            "department": r["department"] or "",
            "author_id": r["author_id"] or "",
            "author": r["author"] or "",
            "approver": r["approver"] or "",
            "confirmer": r["confirmer"] or "",
            "documentPurpose": r["purpose"] or "",
            "reviseReason": "",
            "revisePoint": "",
            "previousDocumentToken": prev_token,
        }
    })

# ----- EIP Process ----- #

def apply_snapshot_to_main_db(snap_row, oracle_row):
    """
    snap_row: ä¾†è‡ª rms_document_snapshots çš„ä¸€åˆ—ï¼ˆåªæœ‰ metaï¼Œæœ‰ snapshot_idï¼‰
    oracle_row: ä¾†è‡ª Oracle.RMS_DCC2EIP çš„ä¸€åˆ—
    """
    token   = snap_row["document_token"]
    rms_id  = snap_row["rms_id"]
    snap_id = snap_row["snapshot_id"]

    # ğŸ”¹ å¾ payload table æ’ˆ JSON
    payload     = _load_snapshot_payload(snap_id)
    doc_snap    = payload["document_row"]     or {}
    blocks_snap = payload["blocks_rows"]      or []
    refs_snap   = payload["references_rows"]  or []

    # è§£æ Oracle æ¬„ä½ï¼ˆä¾ä½ å¯¦éš›æ¬„ä½é †åºèª¿æ•´ indexï¼‰
    RMS_ID          = oracle_row[0]
    RMS_DCCNO       = oracle_row[1]
    RMS_VER         = float(oracle_row[2] or snap_row["document_version"] or 1.0)
    RMS_DCCNAME     = oracle_row[3]
    RMS_INSDT       = oracle_row[4]
    EIPNO           = oracle_row[5]
    EIP_USER        = oracle_row[6]
    EIP_CREATEDT    = oracle_row[7]
    EIP_STATUS_STR  = (oracle_row[8] or "").strip()
    DECISION_USER   = oracle_row[9]
    DECISION_COMMENT= oracle_row[10]

    status_int = STATUS_MAP.get(EIP_STATUS_STR, 2)  # æ‰¾ä¸åˆ°å°±ç•¶æ­£å¸¸çµæ¡ˆ

    with db(dict_cursor=True) as (conn, cur):
        # 1) ç”¨ snapshot çš„ document_row å›å¯«å¤§éƒ¨åˆ†æ¬„ä½ï¼Œå†ç–Š Oracle è³‡è¨Š
        cur.execute("""
            UPDATE rms_document_attributes
            SET document_type   = %s,
                EIP_id          = %s,
                status          = %s,
                document_id     = %s,
                document_name   = %s,
                document_version= %s,
                attribute       = %s,
                department      = %s,
                author_id       = %s,
                author          = %s,
                approver        = %s,
                confirmer       = %s,
                rejecter        = %s,
                issue_date      = %s,
                change_reason   = %s,
                change_summary  = %s,
                reject_reason   = %s,
                purpose         = %s
            WHERE document_token = %s
        """, (
            doc_snap.get("document_type"),
            EIPNO,                             # EIP_id
            status_int,                        # status
            RMS_DCCNO or doc_snap.get("document_id"),
            RMS_DCCNAME or doc_snap.get("document_name"),
            RMS_VER,
            jdump(_normalize_metadata(doc_snap.get("attribute"))),
            doc_snap.get("department"),
            doc_snap.get("author_id"),
            doc_snap.get("author"),
            doc_snap.get("approver"),
            doc_snap.get("confirmer"),
            DECISION_USER or doc_snap.get("rejecter"),
            EIP_CREATEDT or RMS_INSDT or doc_snap.get("issue_date"),
            doc_snap.get("change_reason"),
            doc_snap.get("change_summary"),
            DECISION_COMMENT or doc_snap.get("reject_reason"),
            doc_snap.get("purpose"),
            token,
        ))

        # 2) å…ˆåˆªæ‰ç¾åœ¨ä¸»è¡¨çš„ blocks / refsï¼Œå†ç”¨ snapshot é‡çŒ
        cur.execute("DELETE FROM rms_block_content WHERE document_token=%s", (token,))
        cur.execute("DELETE FROM rms_references WHERE document_token=%s", (token,))

        # 2-1) é‚„åŸ blocks
        if blocks_snap:
            ins_blk = """
              INSERT INTO rms_block_content
              (content_id, document_token, step_type, tier_no, sub_no, content_type,
               header_text, header_json, content_text, content_json, files, metadata,
               created_at, updated_at)
              VALUES
              (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
            """
            for b in blocks_snap:
                cur.execute(ins_blk, (
                    b.get("content_id") or new_token(),
                    b.get("document_token") or token,
                    b.get("step_type"),
                    b.get("tier_no"),
                    b.get("sub_no"),
                    b.get("content_type"),
                    b.get("header_text"),
                    jdump(_normalize_metadata(b.get("header_json"))),
                    b.get("content_text"),
                    jdump(_normalize_metadata(b.get("content_json"))),
                    jdump(_normalize_metadata(b.get("files"))),
                    jdump(_normalize_metadata(b.get("metadata"))),
                    b.get("created_at") or datetime.datetime.now(),
                    b.get("updated_at") or datetime.datetime.now(),
                ))

        # 2-2) é‚„åŸ references
        if refs_snap:
            ins_ref = """
              INSERT INTO rms_references
              (id, document_token, refer_type, refer_document, refer_document_name, created_at)
              VALUES (%s,%s,%s,%s,%s,%s)
            """
            for r in refs_snap:
                cur.execute(ins_ref, (
                    r.get("id"),
                    r.get("document_token") or token,
                    r.get("refer_type"),
                    r.get("refer_document"),
                    r.get("refer_document_name"),
                    r.get("created_at") or datetime.datetime.now(),
                ))

        # 3) æ›´æ–° snapshot æœ¬èº«ç‹€æ…‹ï¼ˆé€™è£¡ç•™è‘—ä¹Ÿæ²’é—œä¿‚ï¼Œç­‰ä¸€ä¸‹æœƒæ•´æ‰¹åˆªæ‰ï¼‰
        cur.execute("""
            UPDATE rms_document_snapshots
            SET sync_status = 2, synced_at = NOW()
            WHERE document_token = %s AND rms_id <> %s AND sync_status = 0
        """, (token, rms_id))

        cur.execute("""
            UPDATE rms_document_snapshots
            SET sync_status = 1, synced_at = NOW()
            WHERE snapshot_id = %s
        """, (snap_row["snapshot_id"],))

        conn.commit()

def _apply_reject_status_to_main_attributes(snap_row, oracle_row):
    token = snap_row["document_token"]

    RMS_DCCNO       = oracle_row[1]
    RMS_VER         = oracle_row[2]
    RMS_DCCNAME     = oracle_row[3]
    DECISION_USER   = oracle_row[9]
    DECISION_COMMENT= oracle_row[10]

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            UPDATE rms_document_attributes
            SET rejecter        = %s,
                reject_reason   = %s,
                document_id     = COALESCE(%s, document_id),
                document_name   = COALESCE(%s, document_name),
                document_version= COALESCE(%s, document_version)
            WHERE document_token = %s
        """, (
            DECISION_USER,
            DECISION_COMMENT,
            RMS_DCCNO,
            RMS_DCCNAME,
            RMS_VER,
            token,
        ))
        conn.commit()

def _load_snapshot_payload(snapshot_id: int):
    """
    ä¾ snapshot_id å¾ rms_document_snapshot_payloads æ’ˆå‡º
    document_row / blocks_rows / references_rowsã€‚
    å›å‚³ dictï¼š{"document_row": dict, "blocks_rows": list, "references_rows": list}
    """
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            SELECT document_row, blocks_rows, references_rows
            FROM rms_document_snapshot_payloads
            WHERE snapshot_id = %s
        """, (snapshot_id,))
        row = cur.fetchone()

    if not row:
        raise RuntimeError(f"snapshot payload not found for snapshot_id={snapshot_id}")

    # MySQL JSON type æœƒç›´æ¥çµ¦ dict/listï¼›ç‚ºå®‰å…¨èµ·è¦‹ï¼Œç”¨ _normalize_metadata / jload å†è™•ç†ä¸€æ¬¡
    doc_row   = _normalize_metadata(row.get("document_row"))   or {}
    blocks_rs = _normalize_metadata(row.get("blocks_rows"))    or []
    refs_rs   = _normalize_metadata(row.get("references_rows")) or []

    return {
        "document_row":   doc_row,
        "blocks_rows":    blocks_rs,
        "references_rows": refs_rs,
    }

def _normalize_metadata(raw):
    """
    ç¢ºä¿ metadata æ˜¯ dict/listï¼Œè€Œä¸æ˜¯è¢« double-JSON çš„å­—ä¸²ã€‚
    e.g. "\"{\\\"kind\\\": ...}\"" -> {"kind": ...}
    """
    if raw is None:
        return None
    if isinstance(raw, (dict, list)):
        return raw

    v = raw
    # æœ€å¤šè§£å…©å±¤ï¼Œé¿å…ç„¡é™ loop
    for _ in range(2):
        if not isinstance(v, str):
            break
        parsed = jload(v, default=None)
        if parsed is None or parsed == v:
            break
        v = parsed
    return v

def _rebind_mcr_program_codes(cur, latest_token: str):
    """
    é‡æ–°æŠŠ MCR çš„ç¨‹å¼ç¢¼ (program_code) ç¶åˆ°æœ€æ–°ç‰ˆæœ¬çš„ document_token ä¸Šã€‚

    è¦å‰‡ï¼š
      - å¾æœ€æ–°ç‰ˆæœ¬è©²æ–‡ä»¶çš„ rms_block_content.metadata ä¸­æ‰¾å‡º
        kind = "mcr-parameter" çš„è³‡æ–™
      - å–å‡ºæ‰€æœ‰ programs[].programCode
      - åœ¨ rms_program_code ä¸­ç”¨é€™äº› program_code æ›´æ–° document_token = latest_token, status = 1
    """
    # 1) æŠŠé€™ä»½æ–‡ä»¶æ‰€æœ‰ block çš„ metadata æŠ“å‡ºä¾†
    cur.execute("""
        SELECT metadata
        FROM rms_block_content
        WHERE document_token = %s
          AND metadata IS NOT NULL
    """, (latest_token,))
    rows = cur.fetchall() or []

    program_codes = set()

    for r in rows:
        meta = _normalize_metadata(r.get("metadata"))
        if not isinstance(meta, dict):
            continue
        if meta.get("kind") != "mcr-parameter":
            continue

        for p in meta.get("programs") or []:
            code = (p.get("programCode") or "").strip()
            if code:
                program_codes.add(code)

    if not program_codes:
        return  # æ²’æœ‰ä»»ä½•ç¨‹å¼ç¢¼è¦ç¶å®š

    placeholders = ",".join(["%s"] * len(program_codes))
    sql = f"""
        UPDATE rms_program_code
        SET document_token = %s,
            status = 1
        WHERE program_code IN ({placeholders})
    """
    params = [latest_token] + list(program_codes)
    cur.execute(sql, params)

def _row_ts(row):
    """
    Oracle row çš„æ™‚é–“æ¬„ä½ï¼š
      - å„ªå…ˆ EIP_CREATEDT (idx=7)
      - é€€è€Œæ±‚å…¶æ¬¡ RMS_INSDT (idx=4)
    """
    return row[7] or row[4] or datetime.datetime.min

@bp.post("/sync-eip")
def sync_eip():
    """
    EIP åŒæ­¥ï¼ˆç°¡åŒ–ç‰ˆé‚è¼¯ï¼‰ï¼š

    å°æ¯ä¸€å€‹ document_idï¼š
      1) åªè™•ç† sync_status = 0 çš„ snapshotï¼ˆå¾…åŒæ­¥ï¼‰
      2) å¾ Oracle æŠ“å‡ºæ‰€æœ‰è©²æ–‡ä»¶çš„ç´€éŒ„ï¼ˆEIP_STATUS IS NOT NULLï¼‰
      3) å°æ¯ä¸€å€‹ç‰ˆæœ¬ï¼ˆRMS_VERï¼‰åšåˆ¤æ–·ï¼š

         (A) è‹¥è©²ç‰ˆæœ¬æœ‰ã€Œå·²ç°½æ ¸ã€ï¼š
             -> ç”¨å°æ‡‰ snapshot å›å¯«ä¸»è¡¨
             -> åˆªé™¤è©²æ–‡ä»¶ + è©²ç‰ˆæœ¬çš„æ‰€æœ‰ snapshots

         (B) è‹¥è©²ç‰ˆæœ¬æ²’æœ‰ã€Œå·²ç°½æ ¸ã€ï¼Œä½†æœ‰ã€Œå¦æ±º / é€€å›ç”³è«‹è€…ã€ï¼š
             -> æ‰¾å‡ºè©²ç‰ˆæœ¬æœ€æ–°çš„ä¸€ç­† å¦æ±º/é€€å› Oracle row
             -> æ‰¾åˆ°å°æ‡‰ RMS_ID çš„ snapshot
             -> å›å¯«é€€å›è³‡è¨Šåˆ°ä¸»è¡¨
             -> æ¨™è¨˜é€™ä¸€ç­† snapshot ç‚º sync_status = 2
             -> åˆªé™¤åŒä¸€å€‹ document + version ä¸‹ã€Œå…¶å®ƒ sync_status = 2 çš„èˆŠé€€å› snapshotã€
                ï¼ˆä¿ç•™è‰ç¨¿/å·²ä¸‹è¼‰/å¯©æ ¸ä¸­çš„ snapshotï¼Œä¸åˆªï¼‰

         (C) å…¶å®ƒç‹€æ…‹ï¼šç•¥é
    """
    pending = _get_pending_snapshots_grouped_by_doc_id()  # åªåŒ…å« sync_status=0
    doc_ids = list(pending.keys())

    if not doc_ids:
        return jsonify({"success": True, "updated": 0, "message": "no pending snapshots"})

    # 1) å…ˆæŒ‘å‡º EIP_STATUS IS NOT NULL çš„ oracle rows
    #    _fetch_oracle_rows_for_doc_ids é è¨­å°±åªæŠ“ EIP_STATUS IS NOT NULL
    oracle_map = _fetch_oracle_rows_for_doc_ids(doc_ids, include_NULL=False)

    updated = 0

    for doc_id, snaps in pending.items():
        o_rows = oracle_map.get(doc_id) or []
        if not o_rows:
            # é€™å€‹æ–‡ä»¶åœ¨ EIP é‚„æ²’æœ‰ä»»ä½•æœ‰ç‹€æ…‹çš„ç´€éŒ„ï¼šç›´æ¥è·³é
            continue

        # ---- ä¾ç‰ˆæœ¬ï¼ˆRMS_VERï¼‰åˆ†çµ„ Oracle rows ----
        by_ver = {}
        for r in o_rows:
            try:
                ver = float(r[2]) if r[2] is not None else None  # RMS_VER
            except ValueError:
                ver = None
            by_ver.setdefault(ver, []).append(r)

        # å°æ¯ä¸€å€‹ç‰ˆæœ¬åšè™•ç†
        for ver, rows_for_ver in by_ver.items():
            # æ”¶é›†é€™å€‹ç‰ˆæœ¬æ‰€æœ‰éç©ºçš„ EIP_STATUS
            statuses = {(r[8] or "").strip() for r in rows_for_ver if (r[8] or "").strip()}

                        # ----------------------------------------------------------
            # Case 2: è‹¥æœ‰ã€Œå·²ç°½æ ¸ã€ â†’ æ­£å¸¸çµæ¡ˆï¼Œåˆªæ‰æ‰€æœ‰ snapshots
            # ----------------------------------------------------------
            if "å·²ç°½æ ¸" in statuses:
                # æŒ‘è©²ç‰ˆæœ¬ä¸­ã€Œå·²ç°½æ ¸ã€ä¸”æœ€æ–°çš„ä¸€ç­† Oracle ç´€éŒ„
                ver_signed_rows = [r for r in rows_for_ver if (r[8] or "").strip() == "å·²ç°½æ ¸"]
                target_row = max(ver_signed_rows, key=_row_ts)
                target_rms_id = target_row[0]  # RMS_ID

                # å…ˆç”¨ version + rms_id åšç²¾æº–å°æ‡‰
                snap_candidates = []
                for s in snaps:
                    try:
                        s_ver = float(s.get("document_version") or 1.0)
                    except (TypeError, ValueError):
                        s_ver = 1.0

                    if ver is not None and abs(s_ver - ver) >= 1e-6:
                        continue

                    if s.get("rms_id") == target_rms_id:
                        snap_candidates.append(s)

                # å¦‚æœçœŸçš„æ‰¾ä¸åˆ° rms_id å°æ‡‰ï¼ˆç†è«–ä¸Šä¸æ‡‰è©²ç™¼ç”Ÿï¼‰ï¼Œæ‰é€€ä¸€æ­¥åªçœ‹ version
                if not snap_candidates:
                    for s in snaps:
                        try:
                            s_ver = float(s.get("document_version") or 1.0)
                        except (TypeError, ValueError):
                            s_ver = 1.0
                        if ver is None or abs(s_ver - ver) < 1e-6:
                            snap_candidates.append(s)

                if not snap_candidates:
                    print("[sync-eip] no snapshot found for signed doc", doc_id, ver, target_rms_id)
                    continue

                snap = max(snap_candidates, key=lambda s: s.get("created_at") or datetime.datetime.min)


                try:
                    apply_snapshot_to_main_db(snap, target_row)
                    updated += 1
                except Exception as e:
                    print("[sync-eip] apply snapshot failed (å·²ç°½æ ¸)", doc_id, ver, e)
                    continue

                # æ³¨æ„ï¼šé€™è£¡è¦–ä¹ Oracle çš„ RMS_VER æ˜¯å¦ä¸€å®šå­˜åœ¨
                ver_value = ver if ver is not None else float(snap.get("document_version") or 1.0)
                latest_token = snap["document_token"]

                with db(dict_cursor=True) as (conn, cur):
                    # 2-1) åˆªé™¤åŒä¸€æ–‡ä»¶ + åŒä¸€ç‰ˆæœ¬çš„æ‰€æœ‰ snapshotsï¼ˆå«å‰›åŒæ­¥é‚£ä¸€ç­†ï¼‰
                    cur.execute("""
                        DELETE FROM rms_document_snapshots
                        WHERE document_id = %s
                          AND ABS(document_version - %s) < 1e-6
                    """, (doc_id, ver_value))

                    # 2-2) åˆªé™¤åŒä¸€æ–‡ä»¶ + åŒä¸€ç‰ˆæœ¬ã€ä½†ä¸æ˜¯é€™å€‹ token çš„ã€Œè‰ç¨¿/æœªç°½æ ¸ã€æ–‡ä»¶
                    cur.execute("""
                        DELETE FROM rms_document_attributes
                        WHERE document_id = %s
                          AND ABS(document_version - %s) < 1e-6
                          AND document_token <> %s
                          AND status IN (0, 1)
                    """, (doc_id, ver_value, latest_token))

                    # ------------------------------
                    # 2-3) â­ æ–°å¢ï¼šç°½æ ¸å¾ŒèˆŠç‰ˆæ•´ç†é‚è¼¯
                    # ------------------------------
                    # åªçœ‹ã€Œå·²ç°½æ ¸ç‰ˆæœ¬ã€(status = 2)ï¼ŒæŒ‰ç‰ˆæœ¬å¾æ–°åˆ°èˆŠæ’
                    cur.execute("""
                        SELECT document_token, document_version
                        FROM rms_document_attributes
                        WHERE document_id = %s
                          AND status = 2
                        ORDER BY document_version DESC
                    """, (doc_id,))
                    ver_rows = cur.fetchall() or []

                    if ver_rows:
                        # ä¿ç•™æœ€æ–°ç‰ˆ + å‰å…©ç‰ˆçš„ attributes
                        keep_rows = ver_rows[:3]  # æœ€å¤š 3 ç­†
                        keep_tokens = [r["document_token"] for r in keep_rows]

                        # æœ€æ–°ç‰ˆ tokenï¼ˆç†è«–ä¸Šå°±æ˜¯ latest_tokenï¼Œä½†é€™è£¡å†ä¿éšªæŠ“ä¸€æ¬¡ï¼‰
                        latest_attr_token = keep_tokens[0]

                        # è¦ä¿ç•™ attribute ä½†æ¸…æ‰å…§å®¹çš„èˆŠç‰ˆ tokenï¼šå‰å…©ç‰ˆï¼ˆindex 1,2ï¼‰
                        clear_tokens = [r["document_token"] for r in keep_rows[1:]]

                        # è¶…é 2 å€‹ç‰ˆæœ¬ä¹‹å‰çš„èˆŠç‰ˆï¼šæ•´å€‹ attributes ç›´æ¥åˆªæ‰ï¼ˆCASCADE æ‰å…§å®¹ï¼‰
                        delete_attr_tokens = [r["document_token"] for r in ver_rows[3:]]

                        # (a) åˆªé™¤èˆŠç‰ˆçš„ content / referencesï¼ˆä½†ä¿ç•™ attributesï¼‰
                        if clear_tokens:
                            ph = ",".join(["%s"] * len(clear_tokens))
                            cur.execute(f"""
                                DELETE FROM rms_block_content
                                WHERE document_token IN ({ph})
                            """, clear_tokens)
                            cur.execute(f"""
                                DELETE FROM rms_references
                                WHERE document_token IN ({ph})
                            """, clear_tokens)

                        # (b) åˆªé™¤æ¯”å‰å…©ç‰ˆæ›´èˆŠçš„ attributesï¼ˆrms_block_content / rms_references æœƒè·Ÿè‘— FK CASCADEï¼‰
                        if delete_attr_tokens:
                            ph = ",".join(["%s"] * len(delete_attr_tokens))
                            cur.execute(f"""
                                DELETE FROM rms_document_attributes
                                WHERE document_token IN ({ph})
                            """, delete_attr_tokens)

                        # (c) é‡æ–°æŠŠ MCR çš„ç¨‹å¼è™Ÿç¢¼ç¶å®šåˆ°ã€Œæœ€æ–°ç‰ˆã€çš„ document_token
                        _rebind_mcr_program_codes(cur, latest_attr_token)

                    conn.commit()

            # ----------------------------------------------------------
            # Case 3: æ²’æœ‰å·²ç°½æ ¸ï¼Œä½†æœ‰ã€Œå¦æ±º / é€€å›ç”³è«‹è€…ã€
            # ----------------------------------------------------------
            reject_rows = [
                r for r in rows_for_ver
                if (r[8] or "").strip() in {"å¦æ±º", "é€€å›ç”³è«‹è€…"}
            ]
            if not reject_rows:
                # æ­¤ç‰ˆæœ¬æ²’æœ‰å·²ç°½æ ¸ï¼Œä¹Ÿæ²’æœ‰å¦æ±º/é€€å› â†’ ä¸è™•ç†
                continue

            # æŒ‘å‡ºæœ€æ–°ä¸€ç­†ã€Œå¦æ±º/é€€å›ã€çš„ Oracle rowï¼ˆä½ çš„æ­¥é©Ÿ 3ï¼šlatestï¼‰
            target_row = max(reject_rows, key=_row_ts)
            target_rms_id = target_row[0]  # RMS_ID

            # æ‰¾å°æ‡‰ snapshotï¼šåŒ doc_id + version + rms_id
            snap_candidates = []
            for s in snaps:
                try:
                    s_ver = float(s.get("document_version") or 1.0)
                except (TypeError, ValueError):
                    s_ver = 1.0

                if ver is not None and abs(s_ver - ver) >= 1e-6:
                    continue

                if s.get("rms_id") == target_rms_id:
                    snap_candidates.append(s)

            if not snap_candidates:
                # æ‰¾ä¸åˆ°å°æ‡‰ rms_id çš„ snapshot æ™‚ï¼Œé€€ä¸€æ­¥åªç”¨ç‰ˆæœ¬ match
                for s in snaps:
                    try:
                        s_ver = float(s.get("document_version") or 1.0)
                    except (TypeError, ValueError):
                        s_ver = 1.0
                    if ver is None or abs(s_ver - ver) < 1e-6:
                        snap_candidates.append(s)

            if not snap_candidates:
                print("[sync-eip] no snapshot found for rejected doc", doc_id, ver, target_rms_id)
                continue

            snap = max(snap_candidates, key=lambda s: s.get("created_at") or datetime.datetime.min)

            # â˜… å°‡ rejecter / reject reason å›å¯«åˆ° attributesï¼ˆä½ çš„æ­¥é©Ÿ 3.2ï¼‰
            try:
                _apply_reject_status_to_main_attributes(snap, target_row)
                updated += 1
            except Exception as e:
                print("[sync-eip] apply reject-status failed", doc_id, ver, e)
                continue

            # â˜… å°‡æ­¤ snapshot æ¨™æˆ sync_status = 2ï¼Œä¸¦æ¸…æ‰èˆŠçš„é€€å› snapshotï¼ˆåŒ doc+verï¼‰
            with db(dict_cursor=True) as (conn, cur):
                # 3) å°‡å°æ‡‰çš„ rms_id çš„ sync_status æ”¹ç‚º 2ï¼ˆä½ çš„ 3ï¼‰
                cur.execute("""
                    UPDATE rms_document_snapshots
                    SET sync_status = 2, synced_at = NOW()
                    WHERE snapshot_id = %s
                """, (snap["snapshot_id"],))

                # 3.1) åˆªé™¤åŒä¸€æ–‡ä»¶ + ç‰ˆæœ¬ä¸‹ã€å…¶ä»– sync_status = 2 çš„èˆŠé€€å› snapshot
                #      ï¼ˆæ³¨æ„ï¼šä¸å‹• sync_status = 0 çš„è‰ç¨¿ / å·²ä¸‹è¼‰ / å¯©æ ¸ä¸­ï¼‰
                cur.execute("""
                    DELETE FROM rms_document_snapshots
                    WHERE document_id = %s
                    AND ABS(document_version - %s) < 1e-6
                    AND sync_status = 2
                    AND rms_id <> %s
                """, (
                    doc_id,
                    ver if ver is not None else float(snap.get("document_version") or 1.0),
                    target_rms_id,
                ))

                conn.commit()

            # ã€Œå¦æ±º/é€€å›ã€ç‰ˆæœ¬å¯ä»¥æœ‰å¤šæ¬¡ historyï¼Œä½†æˆ‘å€‘åªä¿ç•™æœ€æ–°é‚£å€‹ sync_status=2 çš„ snapshotï¼Œ
            # è‰ç¨¿å¿«ç…§ç•™çµ¦ä½¿ç”¨è€…ä¿®æ”¹å†é€ï¼Œä¸å†è™•ç†æ›´å¤š
            continue

    return jsonify({"success": True, "updated": updated})

def _get_pending_snapshots_grouped_by_doc_id():
    """
    å›å‚³:
    {
      "WMD001": [snap_row1, snap_row2, ...],
      "WMD002": [...],
    }
    åƒ…æŠ“ sync_status = 0 çš„ snapshotã€‚
    é€™è£¡åªéœ€è¦ metaï¼Œä¸è®€ payloadã€‚
    """
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            SELECT snapshot_id, document_token, rms_id,
                   document_id, document_version, document_name,
                   created_by, created_at, sync_status
            FROM rms_document_snapshots
            WHERE sync_status = 0
        """)
        rows = cur.fetchall() or []

    by_doc = {}
    for r in rows:
        doc_id = (r.get("document_id") or "").strip()
        if not doc_id:
            continue
        by_doc.setdefault(doc_id, []).append(r)
    return by_doc

def _fetch_oracle_rows_for_doc_ids(doc_ids, include_NULL = False):
    """
    doc_ids: list[str]
    å›å‚³ mapping: doc_id -> [oracle_row1, oracle_row2, ...]
    """
    if not doc_ids:
        return {}

    placeholders = ",".join([f":{i+1}" for i in range(len(doc_ids))])
    sql = f"""
        SELECT RMS_ID, RMS_DCCNO, RMS_VER, RMS_DCCNAME, RMS_INSDT, EIPNO, EIP_USER, EIP_CREATEDT, EIP_STATUS, DECISION_USER, DECISION_COMMENT
        FROM IDBUSER.RMS_DCC2EIP WHERE RMS_DCCNO IN ({placeholders})
    """

    if not include_NULL:
        sql += " AND EIP_STATUS IS NOT NULL"

    with odb() as cur_o:
        cur_o.execute(sql, doc_ids)
        rows = cur_o.fetchall() or []

    by_doc = {}
    for r in rows:
        doc_id = (r[1] or "").strip()  # RMS_DCCNO
        by_doc.setdefault(doc_id, []).append(r)
    return by_doc

# ----- Draft Function ----- #

@bp.post("/clear-doc-id")
def clear_doc_id():
    """
    å‰ç«¯åœ¨è®Šæ›´é©ç”¨å·¥ç¨‹å¾Œå‘¼å«ï¼Œæ¸…é™¤è©² token çš„ document_idã€‚
    """
    body = request.get_json(silent=True) or {}
    token = (body.get("token") or "").strip()
    if not token:
        return send_response(400, False, "missing token")

    with db() as (conn, cur):
        cur.execute("""
          UPDATE rms_document_attributes
          SET document_id=NULL
          WHERE document_token=%s
        """, (token,))
    return jsonify({"success": True})

@bp.get("/drafts")
def list_drafts():
    """
    Query params:
      - user_id      (required):  è¦æŸ¥çš„ä½œè€…/ä½¿ç”¨è€… id -> å°æ‡‰ DB æ¬„ä½ author_id
      - status       (optional):  é è¨­ 0 ç•¶ä½œè‰ç¨¿ï¼›å¦‚éœ€æŸ¥æ ¸/ç™¼ä½ˆå¯æ”¹å€¼
      - keyword      (optional):  é‡å° document_nameã€document_id æ¨¡ç³ŠæŸ¥è©¢
      - page         (optional):  é è¨­ 1
      - page_size    (optional):  é è¨­ 20
      - sort         (optional):  æ’åºæ¬„ä½ï¼Œå…è¨±: issue_date, document_version, document_name
      - order        (optional):  asc/descï¼Œé è¨­ desc
    Response:
      {
        "success": true,
        "items": [
          {
            "documentToken": "...",
            "documentName": "...",
            "documentVersion": 1.20,
            "author": "...",
            "authorId": "...",
            "issueDate": "2025-11-04T18:00:00",
            "documentId": "WMH250"          # æ–¹ä¾¿å‰ç«¯é¡¯ç¤ºï¼ˆå¯æ‹¿æ‰ï¼‰
          }
        ],
        "total": 123,
        "page": 1,
        "pageSize": 20
      }
    """
    user_id   = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    # defaults
    try:
        status    = int(request.args.get("status", 0))
    except ValueError:
        return jsonify({"success": False, "error": "status must be int"}), 400

    keyword   = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_map  = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_key  = request.args.get("sort", "issue_date").lower()
    order     = request.args.get("order", "desc").lower()
    sort_col  = sort_map.get(sort_key, "issue_date")
    order_sql = "DESC" if order not in ("asc", "ASC") else "ASC"

    offset = (page - 1) * page_size

    base_where = ["author_id = %s", "status = %s"]
    params = [user_id, status]

    if keyword:
        base_where.append("(document_name LIKE %s OR document_id LIKE %s)")
        like_kw = f"%{keyword}%"
        params.extend([like_kw, like_kw])

    where_sql = " AND ".join(base_where)

    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM rms_document_attributes
      WHERE {where_sql}
    """

    data_sql = f"""
      SELECT
        document_type, document_token, document_name, document_version, author, author_id, issue_date, document_id
      FROM rms_document_attributes
      WHERE {where_sql}
      ORDER BY {sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (conn, cur):
        # total count
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        # page data
        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(row):
        # issue_date è½‰ ISOï¼ˆæ²’æœ‰å°± Noneï¼‰
        iso_date = None
        if row.get("issue_date"):
            try:
                iso_date = row["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(row["issue_date"])

        # å›å‚³å‰ç«¯éœ€è¦çš„ camelCase
        return {
            "documentType": row["document_type"],
            "documentToken": row["document_token"],
            "documentName": row["document_name"],
            "documentVersion": float(row["document_version"]) if row["document_version"] is not None else None,
            "author": row["author"],
            "authorId": row["author_id"],
            "issueDate": iso_date,
            "documentId": row.get("document_id"),
        }

    items = [to_item(r) for r in rows]

    return jsonify({
        "success": True,
        "items": items,
        "total": total,
        "page": page,
        "pageSize": page_size,
    })

@bp.delete("/<document_token>")
def delete_draft(document_token):
    """
    Delete a draft by its document_token.
    Only rows with status = 0 (draft) can be deleted.

    Path:
      DELETE /docs/<document_token>

    Response:
      200 { success: True, deleted: 1 }
      404 { success: False, error: "not found" }              # no such token
      409 { success: False, error: "not a draft" }            # exists but status != 0
    """
    token = (document_token or "").strip()
    if not token:
        return jsonify({"success": False, "error": "document_token is required"}), 400

    with db(dict_cursor=True) as (conn, cur):
        # Is there a record?
        cur.execute("SELECT status FROM rms_document_attributes WHERE document_token=%s", (token,))
        row = cur.fetchone()

        if not row:
            return jsonify({"success": False, "error": "not found"}), 404

        # Only allow deleting drafts
        if int(row.get("status", 1)) != 0:
            return jsonify({"success": False, "error": "not a draft"}), 409

        # Delete
        cur.execute("DELETE FROM rms_document_attributes WHERE document_token=%s AND status=0", (token,))
        conn.commit()
        deleted = cur.rowcount or 0

    # (Optional) clean temp files if you keep any by token under BASE_DIR
    try:
        # Example: remove /docxTemp/<token>.docx if you create such files.
        # from pathlib import Path
        # p = Path(BASE_DIR) / f"{token}.docx"
        # if p.exists():
        #     p.unlink()
        pass
    except Exception:
        # Non-fatal: ignore file cleanup errors
        pass

    return jsonify({"success": True, "deleted": deleted}), 200

# ----- Document Search ----- #

def _build_keyword_predicate(keyword: str):
    """
    Returns (sql_snippet, params) for robust keyword search.
    - Matches: document_name, author, document_id (LIKE)
    - Also matches document_version:
        * if keyword is numeric (int/float), add exact equality on document_version
        * always also add LIKE(cast(document_version as char)) for partial text matches
    """
    if not keyword:
        return "", []

    likes = []
    params = []

    # name / id / author LIKE
    likes.append("document_name LIKE %s")
    params.append(f"%{keyword}%")
    likes.append("document_id LIKE %s")
    params.append(f"%{keyword}%")
    likes.append("author LIKE %s")
    params.append(f"%{keyword}%")

    # version: support numeric equality + textual LIKE
    numeric = None
    try:
        numeric = float(keyword)
    except Exception:
        pass

    # MySQL: CAST(document_version AS CHAR) for LIKE
    likes.append("CAST(document_version AS CHAR) LIKE %s")
    params.append(f"%{keyword}%")

    eq = []
    if numeric is not None:
        eq.append("document_version = %s")
        params.append(numeric)

    # Combine
    if eq:
        where_piece = "(" + " OR ".join(likes + eq) + ")"
    else:
        where_piece = "(" + " OR ".join(likes) + ")"
    return where_piece, params

def _parse_doc_types(s: str | None) -> list[str] | None:
    """
    Accepts:
      - None / ""  -> no filtering
      - single or comma list: "Instruction", "Specification", or mix
    Returns a normalized list using DB values: ["Instruction", "Specification"].
    Raises ValueError if any entry is invalid.
    """
    if s is None or str(s).strip() == "":
        return None

    allowed = {
        "instruction": 0,
        "specification": 1,
    }
    out = []
    for part in str(s).split(","):
        key = part.strip().lower()
        if not key:
            continue
        if key not in allowed:
            raise ValueError("document_type must be in {Instruction, Specification}")
        out.append(allowed[key])
    if not out:
        return None
    return out

def _parse_statuses(s: str) -> list[int]:
    if s is None or str(s).strip() == "":
        raise ValueError("status is required")
    out = []
    for part in str(s).split(","):
        part = part.strip()
        if not part:
            continue
        try:
            out.append(int(part))
        except ValueError:
            raise ValueError(f"invalid status: {part}")
    if not out:
        raise ValueError("status is required")
    return out

def _list_documents_impl(
    *,
    user_id: str | None,         # allow None for "all" search
    statuses: list[int],
    keyword: str = "",
    page: int = 1,
    page_size: int = 20,
    sort_key: str = "issue_date",
    order: str = "desc",
    doc_types: list[str] | None = None,
    scope: str = "mine",         # "mine" | "all"
    document_id: str | None = None,   # æ–°å¢ä½†æš«æ™‚åªæœ‰ /documents /submitted ç­‰ç”¨åˆ°
):
    sort_map = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_col = sort_map.get((sort_key or "issue_date").lower(), "issue_date")
    order_sql = "DESC" if (order or "desc").lower() not in ("asc", "ASC") else "ASC"

    where = []
    params = []

    # scope
    if scope == "mine":
        if not user_id:
            raise ValueError("user_id is required for scope=mine")
        where.append("author_id = %s")
        params.append(user_id)

    # ä¾ document_id éæ¿¾ï¼ˆå¯é¸ï¼‰
    if document_id:
        where.append("document_id = %s")
        params.append(document_id)

    # statuses (required)
    where.append(f"status IN ({', '.join(['%s'] * len(statuses))})")
    params.extend(statuses)

    # doc types (optional)
    if doc_types:
        where.append(f"document_type IN ({', '.join(['%s'] * len(doc_types))})")
        params.extend(doc_types)

    # robust keyword
    kw_sql, kw_params = _build_keyword_predicate(keyword)
    if kw_sql:
        where.append(kw_sql)
        params.extend(kw_params)

    where_sql = " AND ".join(where) if where else "1=1"
    offset = (page - 1) * page_size

    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM rms_document_attributes
      WHERE {where_sql}
    """
    data_sql = f"""
      SELECT
        document_type,
        document_token,
        document_name,
        document_version,
        author,
        author_id,
        issue_date,
        document_id,
        status,
        rejecter,
        reject_reason
      FROM rms_document_attributes
      WHERE {where_sql}
      ORDER BY {sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (_, cur):
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(r):
        iso_date = None
        if r.get("issue_date"):
            try:
                iso_date = r["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(r["issue_date"])
        return {
            "documentType": r["document_type"],
            "documentToken": r["document_token"],
            "documentName": r["document_name"],
            "documentVersion": float(r["document_version"]) if r["document_version"] is not None else None,
            "author": r["author"],
            "authorId": r["author_id"],
            "issueDate": iso_date,
            "documentId": r.get("document_id"),
            "status": r.get("status"),
            "rejecter": r.get("rejecter"),
            "rejectReason": r.get("reject_reason"),
        }

    return {
        "success": True,
        "items": [to_item(r) for r in rows],
        "total": total,
        "page": page,
        "pageSize": page_size,
    }

@bp.get("/all")
def list_all_documents():
    # statuses required (same as /documents)
    try:
        statuses = _parse_statuses(request.args.get("status"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    # ---- æ±ºå®šæ’åºæ¬„ä½ ----
    sort_map = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_col = sort_map.get((sort_key or "issue_date").lower(), "issue_date")
    order_sql = "DESC" if (order or "desc").lower() not in ("asc", "ASC") else "ASC"

    # ---- WHERE æ¢ä»¶ï¼šé€™è£¡æ²’æœ‰ author_id é™åˆ¶ï¼Œå› ç‚ºæ˜¯ all ----
    where = []
    params = []

    # statusesï¼ˆå¿…å¡«ï¼‰
    where.append(f"status IN ({', '.join(['%s'] * len(statuses))})")
    params.extend(statuses)

    # keyword
    kw_sql, kw_params = _build_keyword_predicate(keyword)
    if kw_sql:
        where.append(kw_sql)
        params.extend(kw_params)

    where_sql = " AND ".join(where) if where else "1=1"
    offset = (page - 1) * page_size

    # 1) total = ä¸åŒ document_id çš„æ•¸é‡ï¼ˆåœ¨åŒæ¨£çš„ where æ¢ä»¶ä¸‹ï¼‰
    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM (
        SELECT DISTINCT document_id
        FROM rms_document_attributes
        WHERE {where_sql}
      ) AS t
    """

    # 2) data = æ¯å€‹ document_id çš„ã€Œdocument_version æœ€å¤§ã€é‚£ä¸€ç­†
    data_sql = f"""
      SELECT
        a.document_type,
        a.document_token,
        a.document_name,
        a.document_version,
        a.author,
        a.author_id,
        a.issue_date,
        a.document_id,
        a.status,
        a.rejecter,
        a.reject_reason
      FROM (
        SELECT
          document_type,
          document_token,
          document_name,
          document_version,
          author,
          author_id,
          issue_date,
          document_id,
          status,
          rejecter,
          reject_reason,
          ROW_NUMBER() OVER (
            PARTITION BY document_id
            ORDER BY document_version DESC
          ) AS rn
        FROM rms_document_attributes
        WHERE {where_sql}
      ) AS a
      WHERE a.rn = 1
      ORDER BY a.{sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (_, cur):
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(r):
        iso_date = None
        if r.get("issue_date"):
            try:
                iso_date = r["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(r["issue_date"])
        return {
            "documentType": r["document_type"],
            "documentToken": r["document_token"],
            "documentName": r["document_name"],
            "documentVersion": float(r["document_version"]) if r["document_version"] is not None else None,
            "author": r["author"],
            "authorId": r["author_id"],
            "issueDate": iso_date,
            "documentId": r.get("document_id"),
            "status": r.get("status"),
            "rejecter": r.get("rejecter"),
            "rejectReason": r.get("reject_reason"),
        }

    return jsonify({
        "success": True,
        "items": [to_item(r) for r in rows],
        "total": total,
        "page": page,
        "pageSize": page_size,
    }), 200

@bp.get("/passed")
def list_passed():
    # å›ºå®š status = 2 (é€šé/å·²ç°½æ ¸)
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    # document_type: optional ("Instruction", "Specification"), comma-separated ok
    try:
        doc_types = _parse_doc_types(request.args.get("document_type"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    # ---- æ±ºå®šæ’åºæ¬„ä½ ----
    sort_map = {
        "issue_date": "issue_date",
        "document_version": "document_version",
        "document_name": "document_name",
    }
    sort_col = sort_map.get((sort_key or "issue_date").lower(), "issue_date")
    order_sql = "DESC" if (order or "desc").lower() not in ("asc", "ASC") else "ASC"

    # ---- whereï¼šauthor + status=2 + optional type + keyword ----
    where = ["author_id = %s", "status = 2"]
    params = [user_id]

    if doc_types:
        where.append(f"document_type IN ({', '.join(['%s'] * len(doc_types))})")
        params.extend(doc_types)

    kw_sql, kw_params = _build_keyword_predicate(keyword)
    if kw_sql:
        where.append(kw_sql)
        params.extend(kw_params)

    where_sql = " AND ".join(where) if where else "1=1"
    offset = (page - 1) * page_size

    # 1) total = ä¸åŒ document_id æ•¸é‡
    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM (
        SELECT DISTINCT document_id
        FROM rms_document_attributes
        WHERE {where_sql}
      ) AS t
    """

    # 2) data = æ¯å€‹ document_id çš„æœ€æ–°ç‰ˆæœ¬é‚£ä¸€ç­†
    data_sql = f"""
      SELECT
        a.document_type,
        a.document_token,
        a.document_name,
        a.document_version,
        a.author,
        a.author_id,
        a.issue_date,
        a.document_id,
        a.status,
        a.rejecter,
        a.reject_reason
      FROM (
        SELECT
          document_type,
          document_token,
          document_name,
          document_version,
          author,
          author_id,
          issue_date,
          document_id,
          status,
          rejecter,
          reject_reason,
          ROW_NUMBER() OVER (
            PARTITION BY document_id
            ORDER BY document_version DESC
          ) AS rn
        FROM rms_document_attributes
        WHERE {where_sql}
      ) AS a
      WHERE a.rn = 1
      ORDER BY a.{sort_col} {order_sql}
      LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (_, cur):
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    for index, r in enumerate(rows):
        print(f"{index}: {r}")

    def to_item(r):
        iso_date = None
        if r.get("issue_date"):
            try:
                iso_date = r["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(r["issue_date"])
        return {
            "documentType": r["document_type"],
            "documentToken": r["document_token"],
            "documentName": r["document_name"],
            "documentVersion": float(r["document_version"]) if r["document_version"] is not None else None,
            "author": r["author"],
            "authorId": r["author_id"],
            "issueDate": iso_date,
            "documentId": r.get("document_id"),
            "status": r.get("status"),
            "rejecter": r.get("rejecter"),
            "rejectReason": r.get("reject_reason"),
        }

    return jsonify({
        "success": True,
        "items": [to_item(r) for r in rows],
        "total": total,
        "page": page,
        "pageSize": page_size,
    }), 200

@bp.get("/documents")
def list_documents():
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    try:
        statuses = _parse_statuses(request.args.get("status"))
    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    data = _list_documents_impl(
        user_id=user_id,
        statuses=statuses,
        keyword=keyword,
        page=page,
        page_size=page_size,
        sort_key=sort_key,
        order=order,
        scope="mine",
    )
    return jsonify(data), 200

def _collect_submitted_items_for_user(user_id: str, keyword: str, sort_key: str, order: str):
    """
    å›å‚³å°šåœ¨ EIP å¯©æ ¸æµç¨‹ä¸­çš„æ–‡ä»¶ï¼š
      - ä¾†æºï¼šrms_document_snapshots.sync_status = 0ï¼ˆå°šæœªè¢« sync_eip çµæ¡ˆ/é€€å›ï¼‰
      - Oracleï¼šä¿ç•™ EIP_STATUS in ('å¯©æ ¸ä¸­', NULL) çš„æœ€æ–°é‚£ä¸€ç­†ç´€éŒ„ï¼ˆNULL è¦–ç‚ºã€Œå·²ä¸‹è¼‰ / å°šæœªæ›´æ–°ã€ï¼‰
      - åƒ…ä¿ç•™ author_id = user_id çš„æ–‡ä»¶
    """
    pending = _get_pending_snapshots_grouped_by_doc_id()
    if not pending:
        return []

    doc_ids = list(pending.keys())
    oracle_map = _fetch_oracle_rows_for_doc_ids(doc_ids, include_NULL = True)

    candidate_snaps = []  # (snap_row, oracle_row)

    for doc_id, snaps in pending.items():
        o_rows = oracle_map.get(doc_id) or []
        if not o_rows:
            continue

        # ---- ä¾ document_version é¸å‡ºã€Œæœ€æ–° snapshotã€ ----
        latest_snap_by_ver = {}
        for s in snaps:
            try:
                v = float(s.get("document_version") or 1.0)
            except (TypeError, ValueError):
                v = 1.0
            key = v
            cur_ts = s.get("created_at") or datetime.datetime.min
            if key not in latest_snap_by_ver:
                latest_snap_by_ver[key] = s
            else:
                old_ts = latest_snap_by_ver[key].get("created_at") or datetime.datetime.min
                if cur_ts > old_ts:
                    latest_snap_by_ver[key] = s

        for snap in latest_snap_by_ver.values():
            snap_ver = float(snap.get("document_version") or 1.0)

            # ç”¨ç‰ˆæœ¬å°æ‡‰åˆ° Oracle rows
            candidates = []
            for r in o_rows:
                try:
                    r_ver = float(r[2]) if r[2] is not None else snap_ver  # RMS_VER

                except ValueError:
                    r_ver = snap_ver

                if abs(r_ver - snap_ver) < 1e-6:
                    candidates.append(r)

            if not candidates:
                continue

            for r in candidates:
                eip_status = (r[8] or "").strip()
                if eip_status not in {"", "å¯©æ ¸ä¸­"}:
                    continue

                candidate_snaps.append((snap, r))

    if not candidate_snaps:
        return []

    # æ’ˆå‡ºé€™äº› snapshot å°æ‡‰çš„ attributes
    tokens = list({snap["document_token"] for (snap, _) in candidate_snaps})
    attrs_map = {}
    if tokens:
        placeholders = ", ".join(["%s"] * len(tokens))
        with db(dict_cursor=True) as (conn, cur):
            cur.execute(f"""
                SELECT document_token, document_type, document_id, document_name,
                       document_version, author, author_id, issue_date
                FROM rms_document_attributes
                WHERE document_token IN ({placeholders})
            """, tokens)
            for r in (cur.fetchall() or []):
                attrs_map[r["document_token"]] = r

    items = []
    kw = (keyword or "").strip()
    kw_lower = kw.lower()

    for snap, o_row in candidate_snaps:
        token = snap["document_token"]
        attr = attrs_map.get(token)
        if not attr:
            continue

        if attr.get("author_id") != user_id:
            continue

        # keywordï¼šåç¨± / ç·¨è™Ÿ
        if kw_lower:
            name = (attr.get("document_name") or "").lower()
            docid = (attr.get("document_id") or "").lower()
            if kw_lower not in name and kw_lower not in docid:
                continue

        issue_date = attr.get("issue_date")
        if issue_date is not None:
            try:
                issue_iso = issue_date.isoformat(timespec="seconds")
            except Exception:
                issue_iso = str(issue_date)
        else:
            issue_iso = None

        eip_status = (o_row[8] or "").strip()
        rms_id = o_row[0]  # "RMS_ID"

        items.append({
            "documentType": attr.get("document_type"),
            "documentToken": token,
            "documentName": attr.get("document_name"),
            "documentVersion": float(attr.get("document_version") or 1.0),
            "author": attr.get("author"),
            "authorId": attr.get("author_id"),
            "issueDate": issue_iso,
            "documentId": attr.get("document_id"),
            # çµ¦å‰ç«¯ç”¨ï¼š
            "rmsId": rms_id,
            "eipStatus": eip_status or "å·²ä¸‹è¼‰",  # å‰ç«¯é¡¯ç¤ºå¥½çœ‹ä¸€é»
        })

    # æ’åº
    order = (order or "desc").lower()
    reverse = (order != "asc")

    def sort_key_fn(x):
        if sort_key == "document_name":
            return (x.get("documentName") or "").lower()
        if sort_key == "document_version":
            return x.get("documentVersion") or 0.0
        if sort_key == "document_id":
            return (x.get("documentId") or "").lower()
        # default: issue_date
        return x.get("issueDate") or ""

    items.sort(key=sort_key_fn, reverse=reverse)
    return items

@bp.get("/submitted")
def list_submitted():
    """
    å·²é€å¯©ï¼š
      - åªé¡¯ç¤º EIP_STATUS in ('å¯©æ ¸ä¸­', 'å·²ä¸‹è¼‰') çš„æ–‡ä»¶
      - è³‡æ–™ä¾†æºï¼šsnapshot + Oracle + attributes
    """
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc")

    items = _collect_submitted_items_for_user(
        user_id=user_id,
        keyword=keyword,
        sort_key=sort_key,
        order=order,
    )

    total = len(items)
    start = (page - 1) * page_size
    end   = start + page_size
    page_items = items[start:end]

    return jsonify({
        "success": True,
        "items": page_items,
        "total": total,
        "page": page,
        "pageSize": page_size,
    }), 200

@bp.get("/rejected")
def list_rejected():
    """
    å·²é€€å›ï¼š
      - ä¾†æºï¼šrms_document_snapshots.sync_status = 2ï¼ˆåªçœ‹é€™å€‹è¡¨ï¼Œä¸å†ç®¡ Oracleï¼‰
      - join rms_document_attributes å–å¾—ä½œè€… / é€€å›äºº / ç†ç”±ç­‰
      - åƒ…é¡¯ç¤ºå±¬æ–¼æœ¬äºº (author_id = user_id) çš„æ–‡ä»¶
      - å›å‚³æ ¼å¼èˆ‡ /submitted çš„ items çµæ§‹ä¸€è‡´ï¼Œå¤šè£œ rejecter / rejectReason
    """
    user_id = request.args.get("user_id")
    if not user_id:
        return jsonify({"success": False, "error": "user_id is required"}), 400

    keyword = (request.args.get("keyword") or "").strip()
    try:
        page      = max(1, int(request.args.get("page", 1)))
        page_size = min(100, max(1, int(request.args.get("page_size", 20))))
    except ValueError:
        return jsonify({"success": False, "error": "page/page_size must be int"}), 400

    sort_key = (request.args.get("sort") or "issue_date")
    order    = (request.args.get("order") or "desc").lower()
    order_sql = "DESC" if order != "asc" else "ASC"

    # å’Œ submitted ä¸€æ¨£æ”¯æ´çš„ sort æ¬„ä½ï¼Œå†é¡å¤–å¤šä¸€å€‹ rejecter
    sort_map = {
        "issue_date":        "a.issue_date",
        "document_version":  "a.document_version",
        "document_name":     "a.document_name",
        "document_id":       "a.document_id",
        "rejecter":          "a.rejecter",
    }
    sort_col = sort_map.get(sort_key, "a.issue_date")

    # åªçœ‹ sync_status = 2ï¼ˆå·²é€€å›ï¼‰ï¼Œåªçœ‹è‡ªå·±
    where = ["s.sync_status = 2", "a.author_id = %s"]
    params = [user_id]

    # keywordï¼šå’Œ /submitted é¡ä¼¼ï¼Œå…ˆé–åœ¨åç¨± / ç·¨è™Ÿï¼›ä½ åŸæœ¬å¤šåŠ äº†é€€å›è€…/ç†ç”±ä¹Ÿå¯ä»¥ä¿ç•™
    if keyword:
        like_kw = f"%{keyword}%"
        where.append("""
          (
            a.document_name LIKE %s OR
            a.document_id   LIKE %s OR
            a.rejecter      LIKE %s OR
            a.reject_reason LIKE %s
          )
        """)
        params.extend([like_kw, like_kw, like_kw, like_kw])

    where_sql = " AND ".join(where)
    offset = (page - 1) * page_size

    # åªå¾ snapshots(sync_status = 2) + attributes æ’ˆï¼Œä¸ç¢° Oracle
    count_sql = f"""
      SELECT COUNT(*) AS cnt
      FROM rms_document_snapshots s
      JOIN rms_document_attributes a ON a.document_token = s.document_token
      WHERE {where_sql}
    """
    data_sql = f"""
        SELECT
            a.document_type,
            a.document_token,
            a.document_name,
            a.document_version,
            a.author,
            a.author_id,
            a.issue_date,
            a.document_id,
            a.rejecter,
            a.reject_reason,
            s.rms_id
        FROM rms_document_snapshots s
        JOIN rms_document_attributes a ON a.document_token = s.document_token
        WHERE {where_sql}
        ORDER BY {sort_col} {order_sql}
        LIMIT %s OFFSET %s
    """

    with db(dict_cursor=True) as (_, cur):
        cur.execute(count_sql, params)
        total = int(cur.fetchone()["cnt"])

        cur.execute(data_sql, params + [page_size, offset])
        rows = cur.fetchall() or []

    def to_item(r):
        # issueDateï¼šå…ˆæ²¿ç”¨ issue_dateï¼Œè·Ÿ submitted ä¸€æ¨£æ ¼å¼
        iso_date = None
        if r.get("issue_date"):
            try:
                iso_date = r["issue_date"].isoformat(timespec="seconds")
            except Exception:
                iso_date = str(r["issue_date"])

        return {
            # === å®Œå…¨å°é½Š /submitted çš„æ¬„ä½ ===
            "documentType":    r["document_type"],
            "documentToken":   r["document_token"],
            "documentName":    r["document_name"],
            "documentVersion": float(r["document_version"]) if r["document_version"] is not None else None,
            "author":          r["author"],
            "authorId":        r["author_id"],
            "issueDate":       iso_date,
            "documentId":      r.get("document_id"),
            "rmsId":           r.get("rms_id"),

            # eipStatusï¼šçµ¦ä¸€å€‹å›ºå®šå€¼ï¼Œæ–¹ä¾¿å‰ç«¯å¦‚æœè¦å…±ç”¨å…ƒä»¶
            "eipStatus":       "å·²é€€å›",

            # === å·²é€€å›å°ˆå±¬çš„æ¬„ä½ ===
            "rejecter":        r.get("rejecter"),
            "rejectReason":    r.get("reject_reason"),
        }

    return jsonify({
        "success": True,
        "items": [to_item(r) for r in rows],
        "total": total,
        "page": page,
        "pageSize": page_size,
    }), 200

def _build_doc_payload_from_token(token: str) -> dict:
    """
    çµ¦å®š document_tokenï¼š
      - çµ„å‡º data["attribute"]ï¼šç›®å‰ç‰ˆæœ¬ + æœ€å¤š 2 å€‹å‰ç‰ˆæœ¬ï¼ˆåªéœ€è¦ attribute / åŸºæœ¬æ¬„ä½ï¼‰
      - çµ„å‡º data["content"]ï¼šåªæœ‰ã€Œç›®å‰é€™ä¸€ä»½æ–‡ä»¶ã€çš„å…§å®¹ blocks + åƒæ•¸ blocks
      - çµ„å‡º data["reference"]ï¼šç›®å‰é€™ä¸€ä»½æ–‡ä»¶çš„ reference åˆ—è¡¨
    é€™å€‹çµæ§‹æœƒç›´æ¥ä¸Ÿçµ¦ get_docx ä½¿ç”¨ã€‚
    """
    with db(dict_cursor=True) as (conn, cur):
        # ---------- 1) attributesï¼šæ²¿ previous_document_token å¾€å›è¿½ ----------
        attrs = []
        hops = 0
        seen = set()
        current_token = token

        while current_token and current_token not in seen and hops < 3:  # ç›®å‰ + æœ€å¤š 2 ä»½èˆŠç‰ˆ = 3
            seen.add(current_token)
            cur.execute(
                "SELECT * FROM rms_document_attributes WHERE document_token=%s",
                (current_token,),
            )
            r = cur.fetchone()
            if not r:
                break

            attr_json = jload(r.get("attribute"), {}) or {}

            # é€™è£¡æˆ‘å€‘çµ„æˆä¸€å€‹ã€Œformã€é•·ç›¸çš„ dictï¼Œå°é½Šä½ å‰ç«¯é€é€² generate/word çš„çµæ§‹
            attrs.append({
                "documentType":     r["document_type"],
                "documentID":       r["document_id"] or "",
                "documentName":     r["document_name"] or "",
                "documentVersion":  float(r["document_version"] or 1.0),
                "attribute":        attr_json,                     # å“ç›® / å·¥ç¨‹ / å¼æ¨£ç­‰ç­‰
                "department":       r["department"] or "",
                "author_id":        r["author_id"] or "",
                "author":           r["author"] or "",
                "approver":         r["approver"] or "",
                "confirmer":        r["confirmer"] or "",
                "issueDate":        r["issue_date"].strftime("%Y/%m/%d") if r["issue_date"] else "",
                "reviseReason":     r["change_reason"] or "",
                "revisePoint":      r["change_summary"] or "",
                "documentPurpose":  r["purpose"] or "",
            })

            current_token = r.get("previous_document_token")
            hops += 1

        # attrs ç›®å‰æ˜¯ [æœ€æ–°, å‰ä¸€ç‰ˆ, å‰å‰ç‰ˆ...]ï¼Œç‚ºäº†è®“ REV1/2/3 æ¯”è¼ƒåƒã€Œç”±èˆŠåˆ°æ–°ã€ï¼Œ
        # æˆ‘å€‘å¯ä»¥ reverse ä¸€ä¸‹ï¼Œæœ€å¾Œä¸€å€‹å°±æ˜¯ get_docx çœ‹åˆ°çš„ã€Œæœ€æ–°ã€ã€‚
        attrs.reverse()
        if not attrs:
            raise ValueError("document not found")

        # ---------- 2) contentï¼šåªæœ‰ã€Œç›®å‰é€™ä»½ã€çš„ blocks + åƒæ•¸ ----------
        cur.execute("""
            SELECT step_type, tier_no, sub_no, content_type,
                   header_text, header_json,
                   content_text, content_json,
                   files, metadata
            FROM rms_block_content
            WHERE document_token=%s
            ORDER BY step_type ASC, tier_no ASC, sub_no ASC
        """, (token,))
        rows = cur.fetchall() or []

        # ä¸€èˆ¬ blocksï¼ˆè£½é€ æµç¨‹ / ç®¡ç†æ¢ä»¶ / å“è³ªå…§å®¹ / å…¶ä»– ç­‰ï¼‰
        block_groups = {}      # key = (step_type, tier_no)
        # åƒæ•¸ blocksï¼ˆstep_type 2: è£½é€ æ¢ä»¶åƒæ•¸ä¸€è¦½è¡¨ / 5: è£½é€ åƒæ•¸ä¸€è¦½è¡¨ï¼‰
        param_groups = {}      # key = tier_no

        for r in rows:
            st  = int(r["step_type"])
            t   = int(r["tier_no"])
            sub = int(r["sub_no"])

            # åƒæ•¸é¡ï¼šè·Ÿ load_params çš„é‚è¼¯ä¸€æ¨£ï¼ŒæŠŠ sub 0/1 ç¸«å›å»
            if st in (2, 5):
                g = param_groups.setdefault(t, {
                    "step_type":            st,
                    "tier_no":              t,
                    "code":                 f"XXXX{t}",
                    "jsonParameterContent": None,
                    "arrayParameterData":   [],
                    "jsonConditionContent": None,
                    "arrayConditionData":   [],
                    "metadata":             None,
                })
                if sub == 0:
                    g["code"]                 = r["header_text"] or g["code"]
                    g["arrayParameterData"]   = jload(r["content_text"], []) or []
                    g["jsonParameterContent"] = jload(r["content_json"])
                    g["metadata"]             = jload(r["metadata"])
                elif sub == 1:
                    g["arrayConditionData"]   = jload(r["content_text"], []) or []
                    g["jsonConditionContent"] = jload(r["content_json"])
                continue

            # ä¸€èˆ¬å…§å®¹é¡ï¼šè·Ÿ /<token>/blocks çš„ grouped çµæ§‹ä¸€æ¨£
            g = block_groups.setdefault((st, t), {
                "step_type": st,
                "tier":      t,
                "data":      [],
            })
            g["data"].append({
                "option":      int(r["content_type"]),
                "jsonHeader":  jload(r["header_json"]),
                "jsonContent": jload(r["content_json"]),
                "files":       jload(r["files"], []) or [],
            })

        contents = []
        # blocks æŒ‰ step_type, tier_no æ’åº
        for (st, t) in sorted(block_groups.keys()):
            contents.append(block_groups[(st, t)])
        # åƒæ•¸ blocks æŒ‰ tier æ’åº
        for t in sorted(param_groups.keys()):
            contents.append(param_groups[t])

        # ---------- 3) references ----------
        cur.execute("""
            SELECT refer_type, refer_document, refer_document_name
            FROM rms_references
            WHERE document_token=%s
            ORDER BY refer_type ASC, id ASC
        """, (token,))
        ref_rows = cur.fetchall() or []
        references = [
            {
                "referenceType":        int(r["refer_type"]),
                "referenceDocumentID":  r["refer_document"],
                "referenceDocumentName": r["refer_document_name"],
            }
            for r in ref_rows
        ]

    return {
        "attribute": attrs,     # list[form-like dict]
        "content":   contents,  # list[blocks + params]
        "reference": references,
    }

@bp.get("/view/<token>/docx")
def view_docx_from_token(token):
    """
    ä¾ document_token å¾ DB æ’ˆå‡º attribute/content/referenceï¼Œ
    ä¸²æˆ payload ä¸Ÿçµ¦ get_docxï¼Œç”¢ç”Ÿä¸€ä»½æš«å­˜ DOCXï¼Œ
    å›å‚³çµ¦å‰ç«¯åšã€Œå…¨é é è¦½ã€ï¼ˆå‰ç«¯ç›´æ¥ window.open é€™å€‹ URLï¼‰ã€‚
    """
    try:
        data = _build_doc_payload_from_token(token)
    except Exception as e:
        print("[view_docx_from_token] error:", e)
        return jsonify({"ok": False, "error": "document not found"}), 404

    # æª”åï¼šå„ªå…ˆç”¨æ–‡ä»¶åç¨± / ç·¨è™Ÿ
    try:
        attr_last = data["attribute"][-1]
        raw_name  = attr_last.get("documentName") or attr_last.get("documentID") or token
        doc_name  = _safe_docname(raw_name)
    except Exception:
        doc_name = token

    # æš«å­˜ç›®éŒ„
    view_dir = os.path.join(BASE_DIR, "_view")
    os.makedirs(view_dir, exist_ok=True)

    fname    = f"{doc_name}-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.docx"
    out_path = os.path.join(view_dir, fname)

    # ç”¢ç”Ÿ Word
    if data["attribute"][-1]["documentType"] == 1:
        get_docx(out_path, data, "docx-template/SpecificationDocument.docx")
    else:
        get_docx(out_path, data, "docx-template/InstructionDocument.docx")

    # å›å‚³å¾Œåˆªæ‰æš«å­˜æª”
    @after_this_request
    def remove_file(response):
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception as e:
            print("[view_docx_from_token] remove temp file error:", e)
        return response

    return send_file(
        out_path,
        as_attachment=False,  # ğŸ”‘ ä¸å¼·åˆ¶ä¸‹è¼‰ï¼Œè®“ç€è¦½å™¨ï¼ç³»çµ±è‡ªå·±æ±ºå®šç”¨ä»€éº¼é–‹
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

def _safe_docname(name: str) -> str:
    name = (name or "").strip()
    if not name:
        return "document"
    # ç°¡å–®å»æ‰ä¸é©åˆç•¶æª”åçš„å­—å…ƒ
    name = re.sub(r'[\\/:*?"<>|]+', "_", name)
    return name[:80]

# ----- Generate Word ----- #

def _normalize_for_json(obj):
    """
    æŠŠ dict/list è£¡é¢çš„ Decimalã€datetime ä¹‹é¡è½‰æˆå¯è¢« json.dumps çš„å‹åˆ¥ã€‚
    åªåœ¨ snapshot æ™‚ç”¨ï¼Œä¸æœƒå½±éŸ¿å…¶å®ƒåœ°æ–¹ã€‚
    """
    from datetime import datetime, date

    if isinstance(obj, Decimal):
        return float(obj)

    if isinstance(obj, (datetime, date)):
        # ä½ è¦ä¹Ÿå¯ä»¥æ”¹æˆ str(obj) æˆ–è‡ªè¨‚æ ¼å¼
        return obj.isoformat()

    if isinstance(obj, dict):
        return {k: _normalize_for_json(v) for k, v in obj.items()}

    if isinstance(obj, list):
        return [_normalize_for_json(v) for v in obj]

    if isinstance(obj, tuple):
        return tuple(_normalize_for_json(v) for v in obj)

    if isinstance(obj, set):
        return [_normalize_for_json(v) for v in obj]   # set æ”¹æˆ list

    return obj

make_rms_id = lambda: uuid.uuid4().hex[:15]

def create_snapshot_and_oracle_row(token: str, rms_id: str, user_emp_no: str):
    """
    1) å¾ MySQL æ’ˆå‡ºç›®å‰ token çš„ document_row / blocks_rows / references_rows
    2) å…ˆåœ¨ Oracle.IDBUSER.RMS_DCC2EIP æ–°å¢ RMS_* ä¸€ç­†
    3) å†å¯«å…¥ sfdb.rms_document_snapshots (meta) + rms_document_snapshot_payloads (JSON)
    """
    # --- 1) è®€ MySQL ç¾æ³ï¼ˆåªè®€ï¼Œä¸å‹•è³‡æ–™ï¼‰ ---
    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            SELECT * FROM rms_document_attributes
            WHERE document_token=%s
        """, (token,))
        doc_row = cur.fetchone()
        if not doc_row:
            raise RuntimeError(f"document_token {token} not found for snapshot")

        doc_id   = doc_row.get("document_id")
        doc_ver  = float(doc_row.get("document_version") or 1.0)
        doc_name = doc_row.get("document_name") or ""
        issue_dt = doc_row.get("issue_date") or datetime.datetime.now()

        cur.execute("""
            SELECT * FROM rms_block_content WHERE document_token=%s
            ORDER BY step_type, tier_no, sub_no
        """, (token,))
        blocks_rows = cur.fetchall() or []

        cur.execute("""
            SELECT * FROM rms_references WHERE document_token=%s
            ORDER BY refer_type, id
        """, (token,))
        ref_rows = cur.fetchall() or []

    # --- 2) å…ˆå¯« Oracle.RMS_DCC2EIP ---
    with odb() as cur_o:
        cur_o.execute("""
            INSERT INTO IDBUSER.RMS_DCC2EIP (RMS_ID, RMS_DCCNO, RMS_VER, RMS_DCCNAME, RMS_INSDT)
            VALUES (:1, :2, :3, :4, :5)
        """, (rms_id, doc_id, doc_ver, doc_name, issue_dt))
        cur_o.connection.commit()

    # --- 3) å†å¯« MySQL snapshotï¼ˆmeta + payload åˆ†å…©å¼µè¡¨ï¼‰ ---
    doc_row_json = _normalize_for_json(doc_row)
    blocks_json  = _normalize_for_json(blocks_rows)
    refs_json    = _normalize_for_json(ref_rows)

    try:
        doc_row_str = jdump(doc_row_json)
        blocks_str  = jdump(blocks_json)
        refs_str    = jdump(refs_json)
    except TypeError as e:
        print("[snapshot DEBUG] json dump failed:", e)
        raise

    with db(dict_cursor=True) as (conn, cur):
        # 3-1) å…ˆæ’å…¥è¼•é‡çš„ snapshotsï¼ˆæ‹¿åˆ° snapshot_idï¼‰
        cur.execute("""
            INSERT INTO rms_document_snapshots
            (document_token, rms_id, document_id, document_version, document_name, created_by)
            VALUES (%s,%s,%s,%s,%s,%s)
        """, (token, rms_id, doc_id, doc_ver, doc_name, user_emp_no))
        snapshot_id = cur.lastrowid

        # 3-2) å†æ’å…¥ payload
        cur.execute("""
            INSERT INTO rms_document_snapshot_payloads
            (snapshot_id, document_row, blocks_rows, references_rows)
            VALUES (%s,%s,%s,%s)
        """, (snapshot_id, doc_row_str, blocks_str, refs_str))

        conn.commit()

def next_document_id(prefix: str) -> str:
    """
    ä¾ç…§ PROJECT_CODE å‰ä¸‰ç¢¼ + ä¸‰ä½æµæ°´è™Ÿç”¢ç”Ÿ document_idï¼š
      WMA â†’ WMA001, WMA002, ...
    """
    if not prefix or len(prefix) < 3:
        prefix = "XXX"
    prefix = prefix[:3]

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT document_id
          FROM rms_document_attributes
          WHERE document_id LIKE %s
          ORDER BY document_id DESC
          LIMIT 1
        """, (prefix + "%",))
        row = cur.fetchone()

        if not row or not row["document_id"]:
            return f"{prefix}001"

        tail = row["document_id"][-3:]
        try:
            num = int(tail)
        except ValueError:
            num = 0

        return f"{prefix}{num + 1:03d}"

def next_monthly_document_id(prefix: str = "W") -> str:
    """
    ä¾ç…§ W_YY_MM_XXX è¦å‰‡ç”¢ç”Ÿ document_idï¼š
      W_25_11_001, W_25_11_002, ...
    """
    now = datetime.datetime.now()
    yy = now.year % 100
    mm = now.month

    base = f"{prefix}_{yy:02d}_{mm:02d}_"

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
          SELECT document_id
          FROM rms_document_attributes
          WHERE document_id LIKE %s
          ORDER BY document_id DESC
          LIMIT 1
        """, (base + "%",))
        row = cur.fetchone()

        if not row or not row["document_id"]:
            return f"{base}001"

        tail = row["document_id"][-3:]
        try:
            num = int(tail)
        except ValueError:
            num = 0

        return f"{base}{num + 1:03d}"

def _update_attributes_from_latest_attr(token, latest_attr):
    f = {
        "document_type": int(latest_attr.get("documentType", 0) or 0),
        "doc_id": none_if_blank(latest_attr.get("documentID")),
        "doc_name": none_if_blank(latest_attr.get("documentName")),
        "doc_ver": dver(latest_attr.get("documentVersion", 1.0)),
        "dept": none_if_blank(latest_attr.get("department")),
        "author_id": none_if_blank(latest_attr.get("author_id")),
        "author": none_if_blank(latest_attr.get("author")),
        "approver": none_if_blank(latest_attr.get("approver")),
        "confirmer": none_if_blank(latest_attr.get("confirmer")),
        "chg_reason": none_if_blank(latest_attr.get("reviseReason")),
        "chg_summary": none_if_blank(latest_attr.get("revisePoint")),
        "purpose": none_if_blank(latest_attr.get("documentPurpose")),
    }

    with db() as (conn, cur):
        cur.execute("""
          UPDATE rms_document_attributes
          SET document_type=%s,
              document_id=%s, document_name=%s, document_version=%s,
              department=%s, author_id=%s, author=%s,
              approver=%s, confirmer=%s,
              change_reason=%s, change_summary=%s, purpose=%s
          WHERE document_token=%s
        """, (
            f["document_type"], f["doc_id"], f["doc_name"], f["doc_ver"],
            f["dept"], f["author_id"], f["author"],
            f["approver"], f["confirmer"],
            f["chg_reason"], f["chg_summary"], f["purpose"],
            token,
        ))
        conn.commit()

@bp.post("/generate/word")
def generate_word():
    """
    Accept JSON body {token, attribute, content, reference}ï¼š
    - è‹¥æœ‰ tokenï¼š
        1) ç”¨ _build_doc_payload_from_token(token) æŠŠã€Œå‰å¹¾ç‰ˆ + ç›®å‰ç‰ˆã€æ’ˆå‡ºä¾†
        2) ç”¨å‰ç«¯å‚³é€²ä¾†çš„æœ€æ–° attribute/content/reference è¦†è“‹ã€Œæœ€æ–°é‚£ä¸€ç‰ˆã€
        3) è‹¥ç‚ºåˆç‰ˆä¸”å°šç„¡ document_id â†’ ä¾é©ç”¨å·¥ç¨‹å‰ä¸‰ç¢¼ç”¢ç”Ÿä¸€å€‹ï¼Œå¯«å› DB
    - è‹¥æ²’æœ‰ tokenï¼šé€€å›èˆŠè¡Œç‚ºï¼Œç›´æ¥ç”¨ body çš„è³‡æ–™ç”¢ç”Ÿ Word
    """
    if not request.is_json:
        return jsonify({"ok": False, "error": "JSON body required"}), 400

    data = request.get_json(silent=True) or {}
    data.setdefault("attribute", [])
    data.setdefault("content", [])
    data.setdefault("reference", [])

    token = (data.get("token") or "").strip()

    if token:
        try:
            # A) ä¸€é–‹å§‹å°±å¾ DB æ’ˆã€Œå‰å¹¾ç‰ˆ + æœ€æ–°ç‰ˆã€payload
            payload = _build_doc_payload_from_token(token)
        except Exception as e:
            print("[generate_word] _build_doc_payload_from_token error:", e)
            return send_response(404, False, "document not found")

        latest_attr = payload["attribute"][-1]

        # B) å‰ç«¯æœ‰é€ attributeï¼Œå°±è¦†è“‹ã€Œæœ€æ–°ç‰ˆã€æ¬„ä½
        if data["attribute"]:
            override_attr = data["attribute"][-1]
            for k, v in override_attr.items():
                latest_attr[k] = v

        # C) content / reference è‹¥å‰ç«¯æœ‰å‚³ï¼Œå°±è¦†è“‹ DB çš„ï¼ˆåªå½±éŸ¿æœ€æ–°ç‰ˆï¼‰
        if data["content"]:
            payload["content"] = data["content"]
        if data["reference"]:
            payload["reference"] = data["reference"]

        # 4) è¨ˆç®—/æ›´æ–° document_id + documentKeyï¼ˆåªçœ‹æœ€æ–°é‚£ä¸€ç‰ˆï¼‰
        with db(dict_cursor=True) as (conn, cur):
            cur.execute("""
                SELECT document_type, document_id, document_version, attribute, author_id, document_name
                FROM rms_document_attributes
                WHERE document_token=%s
            """, (token,))
            r = cur.fetchone()
            if not r:
                return send_response(404, False, "document not found")

            doc_type  = int(r["document_type"] or 0)
            doc_id    = r["document_id"]
            doc_ver   = float(r["document_version"] or 1.0)
            attr_json = jload(r["attribute"], {}) or {}
            author_id = (r.get("author_id") or "").strip()
            doc_name0 = r.get("document_name") or ""

            latest_attr_json = latest_attr.get("attribute") or {}
            attr_json.update(latest_attr_json)

            # åˆç‰ˆä¸”å°šç„¡ document_id â†’ ä¾æ–‡ä»¶é¡å‹æ±ºå®šç·¨ç¢¼è¦å‰‡
            if doc_ver == 1.0 and not doc_id:
                if doc_type == 1:
                    doc_id = next_monthly_document_id("W")
                else:
                    apply_project = (attr_json.get("applyProject") or "").strip()
                    prefix = (apply_project[:3] or "XXX").upper()
                    doc_id = next_document_id(prefix)

            # 4.1 ç”Ÿæˆ RMS_ID / documentKey
            rms_id = make_rms_id()
            attr_json["documentKey"] = rms_id

            cur.execute("""
                UPDATE rms_document_attributes
                SET document_id=%s, attribute=%s
                WHERE document_token=%s
            """, (doc_id, jdump(attr_json), token))
            conn.commit()

        # 5) æŠŠ docID & documentKey å¡å›ã€Œæœ€æ–°ç‰ˆ attributeã€ï¼ˆåœ¨ payload ä¸Šï¼‰
        latest_attr["documentID"] = doc_id or ""
        latest_attr["documentKey"] = rms_id

        # å¦‚æœä½ é‚„æœ‰æƒ³è®“å‰ç«¯å›æ”¶çš„ dataï¼Œä¹Ÿå¯ä»¥åŒæ­¥æ›´æ–°ï¼š
        if data["attribute"]:
            data["attribute"][-1]["documentID"] = doc_id or ""
            data["attribute"][-1]["documentKey"] = rms_id

        # 5.5) æš«å­˜å…§å®¹ï¼ˆå¯«å› rms_document_attributesï¼‰
        _update_attributes_from_latest_attr(token, latest_attr)

        # 6) æª”å
        try:
            doc_name = _safe_docname(
                f'{latest_attr.get("documentName")}{latest_attr.get("documentVersion"):.1f}'
            )
        except Exception:
            doc_name = "document"

        # 7) å…ˆåš Oracle / snapshotï¼ˆå¦‚æœå¤±æ•— â†’ ä¸ç”¢ DOCXï¼Œç›´æ¥å›éŒ¯èª¤ï¼‰
        try:
            create_snapshot_and_oracle_row(token=token, rms_id=rms_id, user_emp_no=author_id or "UNKNOWN")
        except Exception as e:
            print("[generate_word] create_snapshot_and_oracle_row FAILED:", e)
            return send_response(
                500,
                False,
                f"EIP å»ºæª” / æ­·å²å¿«ç…§å¤±æ•—ï¼Œè«‹è¯çµ¡ç³»çµ±ç®¡ç†å“¡ã€‚è©³ç´°è¨Šæ¯ï¼š{e}"
            )

        # 8) Oracle + snapshot éƒ½æˆåŠŸå¾Œï¼Œæ‰ç”¢ç”Ÿ Word
        out_path = os.path.join(BASE_DIR, f"{doc_name}.docx")

        # ğŸ”‘ ç”¨ payloadï¼ˆåŒ…å«æ­·å²ç‰ˆæœ¬ attributesï¼‰ï¼Œè€Œä¸æ˜¯ data
        doc_type_for_word = latest_attr.get("documentType", 0)
        if doc_type_for_word == 1:
            get_docx(out_path, payload, "docx-template/SpecificationDocument.docx")
        else:
            get_docx(out_path, payload, "docx-template/InstructionDocument.docx")

        @after_this_request
        def add_docid_header(response):
            if doc_id:
                response.headers["X-Document-ID"] = doc_id
            existing = response.headers.get("Access-Control-Expose-Headers", "")
            expose = "X-Document-ID"
            if existing:
                if expose not in existing:
                    response.headers["Access-Control-Expose-Headers"] = existing + "," + expose
            else:
                response.headers["Access-Control-Expose-Headers"] = expose
            return response

        return send_file(
            out_path,
            as_attachment=True,
            download_name=f"{doc_name}.docx",
            mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

@bp.post("/preview/docx")
def preview_docx():
    """
    æ¥æ”¶ {token?, attribute?, content?, reference?}ï¼š
      - è‹¥æœ‰ tokenï¼š
          1) å…ˆç”¨ _build_doc_payload_from_token(token) â†’ å¸¶å‡ºå‰å¹¾ç‰ˆ + ç›®å‰ç‰ˆ
          2) å‰ç«¯è‹¥å‚³ attribute/content/referenceï¼Œå°±è¦†è“‹ã€Œæœ€æ–°é‚£ä¸€ç‰ˆã€åŠå…¶å…§å®¹
      - è‹¥ç„¡ tokenï¼š
          ä¿ç•™èˆŠè¡Œç‚ºï¼Œç›´æ¥ç”¨ body çš„è³‡æ–™ previewã€‚
    """
    if not request.is_json:
        return jsonify({"ok": False, "error": "JSON body required"}), 400

    data = request.get_json(silent=True) or {}
    data.setdefault("attribute", [])
    data.setdefault("content", [])
    data.setdefault("reference", [])

    token = (data.get("token") or "").strip()

    # -------------------------------------------------------
    # A) æœ‰ tokenï¼šç”¨ DB + å‰å¹¾ç‰ˆ + å‰ç«¯è¦†è“‹æœ€æ–°ç‰ˆ
    # -------------------------------------------------------
    if token:
        try:
            payload = _build_doc_payload_from_token(token)
        except Exception as e:
            print("[preview_docx] _build_doc_payload_from_token error:", e)
            return jsonify({"ok": False, "error": "document not found"}), 404

        latest_attr = payload["attribute"][-1]

        # å‰ç«¯è‹¥æœ‰å‚³ attributeï¼Œå°±è¦†è“‹æœ€æ–°ç‰ˆæ¬„ä½
        if data["attribute"]:
            override_attr = data["attribute"][-1]
            for k, v in override_attr.items():
                latest_attr[k] = v

        # content/reference è‹¥å‰ç«¯æœ‰å‚³ï¼Œå°±è¦†è“‹ DB çš„
        if data["content"]:
            payload["content"] = data["content"]
        if data["reference"]:
            payload["reference"] = data["reference"]

        base_payload = payload

    else:
        # ---------------------------------------------------
        # B) æ²’ tokenï¼šç¶­æŒèˆŠæœ‰è¡Œç‚ºï¼Œç›´æ¥ç”¨ body
        # ---------------------------------------------------
        base_payload = data


    # ç”¢ç”Ÿä¸€å€‹ payload_idï¼Œç•¶æš«å­˜æª”åçš„ä¸€éƒ¨åˆ†
    payload_id = f"{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"

    # å–æª”æ¡ˆåç¨±ï¼šå„ªå…ˆç”¨ã€Œæœ€æ–°ç‰ˆã€çš„æ–‡ä»¶åç¨± / æ–‡ç®¡ç·¨è™Ÿ
    try:
        if base_payload["attribute"]:
            attr_last = base_payload["attribute"][-1]
        else:
            attr_last = {}
        raw_name = attr_last.get("documentName") or attr_last.get("documentID") or payload_id
        doc_name = _safe_docname(raw_name)
    except Exception:
        doc_name = payload_id

    preview_dir = os.path.join(BASE_DIR, "_preview")
    os.makedirs(preview_dir, exist_ok=True)

    out_path = os.path.join(preview_dir, f"{doc_name}-{payload_id}.docx")

    # ç”¢ç”Ÿ Word â†’ ç”¨ base_payloadï¼Œè€Œä¸æ˜¯ data
    attr_list = base_payload.get("attribute") or []
    doc_type = 0
    if attr_list:
        doc_type = attr_list[-1].get("documentType", 0)

    if doc_type == 1:
        get_docx(out_path, base_payload, "docx-template/SpecificationDocument.docx")
    else:
        get_docx(out_path, base_payload, "docx-template/InstructionDocument.docx")

    @after_this_request
    def remove_file(response):
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception as e:
            print("[preview_docx] remove temp file error:", e)
        return response

    return send_file(
        out_path,
        as_attachment=False,
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

def _build_payload_for_docx_from_snapshot(snap_row):
    token   = snap_row["document_token"]
    snap_id = snap_row["snapshot_id"]

    payload   = _load_snapshot_payload(snap_id)
    doc_row   = payload["document_row"]
    blocks_rs = payload["blocks_rows"]
    refs_rs   = payload["references_rows"]

    # ---------- 1.1 æ­·å²ç‰ˆæœ¬ï¼ˆå·²ç¶“æ˜¯ yyyy/mm/ddï¼Œå°±ä¿ç•™ä½ ç¾åœ¨çš„å¯¦ä½œï¼‰ ----------
    attrs: list[dict] = []

    prev_token = doc_row.get("previous_document_token")
    hops = 0
    seen = set()

    if prev_token:
        with db(dict_cursor=True) as (conn, cur):
            while prev_token and prev_token not in seen and hops < 2:
                seen.add(prev_token)
                cur.execute(
                    "SELECT * FROM rms_document_attributes WHERE document_token=%s",
                    (prev_token,),
                )
                r = cur.fetchone()
                if not r:
                    break

                attr_json = jload(r.get("attribute"), {}) or {}
                issue = r.get("issue_date")
                if hasattr(issue, "strftime"):
                    # âœ… æ­·å²ç‰ˆæœ¬ï¼šyyyy/mm/dd
                    issue_str = issue.strftime("%Y/%m/%d")
                else:
                    issue_str = issue or ""

                attrs.append({
                    "documentType":     r.get("document_type") or 0,
                    "documentID":       r.get("document_id") or "",
                    "documentName":     r.get("document_name") or "",
                    "documentVersion":  float(r.get("document_version") or 1.0),
                    "attribute":        attr_json,
                    "department":       r.get("department") or "",
                    "author_id":        r.get("author_id") or "",
                    "author":           r.get("author") or "",
                    "approver":         r.get("approver") or "",
                    "confirmer":        r.get("confirmer") or "",
                    "issueDate":        issue_str,   # ğŸ”‘ çµ±ä¸€ç”¨ issueDate
                    "reviseReason":     r.get("change_reason") or "",
                    "revisePoint":      r.get("change_summary") or "",
                    "documentPurpose":  r.get("purpose") or "",
                })

                prev_token = r.get("previous_document_token")
                hops += 1

    attrs.reverse()

    # ---------- 1.2 ç›®å‰é€™ä¸€ç‰ˆï¼ˆsnapshot å°æ‡‰çš„ç‰ˆæœ¬ï¼‰ ----------
    issue = doc_row.get("issue_date")

    if isinstance(issue, str):
        # å„ªå…ˆè©¦è‘—ç•¶ ISO è§£æï¼ˆå« T çš„æƒ…æ³ï¼‰
        try:
            dt = datetime.datetime.fromisoformat(issue)
            issue_str = dt.strftime("%Y/%m/%d")
        except Exception:
            # é€€è€Œæ±‚å…¶æ¬¡ï¼šç›´æ¥å–å‰ 10 ç¢¼ï¼Œè½‰ yyyy/mm/dd
            # æ”¯æ´ "2025-12-03 09:03:28" æˆ– "2025-12-03T09:03:28"
            s = issue[:10]
            issue_str = s.replace("-", "/")
    elif hasattr(issue, "strftime"):
        # MySQL datetime ç‰©ä»¶
        issue_str = issue.strftime("%Y/%m/%d")
    else:
        issue_str = ""

    attr_json = jload(doc_row.get("attribute"), {}) or {}

    latest_form = {
        "documentType":     doc_row.get("document_type") or 0,
        "documentID":       doc_row.get("document_id") or "",
        "documentName":     doc_row.get("document_name") or "",
        "documentVersion":  float(doc_row.get("document_version") or 1.0),
        "attribute":        attr_json,
        "department":       doc_row.get("department") or "",
        "author_id":        doc_row.get("author_id") or "",
        "author":           doc_row.get("author") or "",
        "approver":         doc_row.get("approver") or "",
        "confirmer":        doc_row.get("confirmer") or "",
        "documentPurpose":  doc_row.get("purpose") or "",
        "reviseReason":     doc_row.get("change_reason") or "",
        "revisePoint":      doc_row.get("change_summary") or "",
        "issueDate":        issue_str,  # âœ… ç¾åœ¨ä¸€å®šæ˜¯ yyyy/mm/dd
        "previousDocumentToken": doc_row.get("previous_document_token") or "",
    }

    attrs.append(latest_form)

    # ---------- 2) blocks / paramsï¼šåªç”¨ snapshot çš„ blocks_rs ----------
    by_step = {}
    for r in blocks_rs:
        try:
            st = int(r.get("step_type"))
        except (TypeError, ValueError):
            continue
        by_step.setdefault(st, []).append(r)

    content_items = []

    for st, rows in by_step.items():
        if st in (2, 5):
            # MCR åƒæ•¸é¡...
            merged = {}
            for r in rows:
                try:
                    t = int(r.get("tier_no"))
                    sub = int(r.get("sub_no"))
                except (TypeError, ValueError):
                    continue
                merged.setdefault(t, {
                    "jsonParameterContent": None,
                    "arrayParameterData": [],
                    "jsonConditionContent": None,
                    "arrayConditionData": [],
                    "metadata": None,
                })
                if sub == 0:
                    merged[t]["arrayParameterData"]   = jload(r.get("content_text"), []) or []
                    merged[t]["jsonParameterContent"] = _normalize_metadata(r.get("content_json"))
                    merged[t]["metadata"]             = _normalize_metadata(r.get("metadata"))
                elif sub == 1:
                    merged[t]["arrayConditionData"]   = jload(r.get("content_text"), []) or []
                    merged[t]["jsonConditionContent"] = _normalize_metadata(r.get("content_json"))

            for t in sorted(merged.keys()):
                b = merged[t]
                content_items.append({
                    "step_type": st,
                    "tier_no": t,
                    "jsonParameterContent": b["jsonParameterContent"],
                    "arrayParameterData": b["arrayParameterData"],
                    "jsonConditionContent": b["jsonConditionContent"],
                    "arrayConditionData": b["arrayConditionData"],
                    "metadata": b["metadata"],
                })
        else:
            grouped = {}
            for r in rows:
                try:
                    t = int(r.get("tier_no"))
                except (TypeError, ValueError):
                    continue
                grouped.setdefault(t, []).append({
                    "option": int(r.get("content_type") or 0),
                    "jsonHeader": _normalize_metadata(r.get("header_json")),
                    "jsonContent": _normalize_metadata(r.get("content_json")),
                    "files": _normalize_metadata(r.get("files")) or [],
                })

            for t in sorted(grouped.keys()):
                content_items.append({
                    "step_type": st,
                    "tier": t,
                    "data": grouped[t],
                })

    # ---------- 3) referencesï¼šåªç”¨ snapshot çš„ refs_rs ----------
    references = []
    for r in refs_rs:
        try:
            ref_type = int(r.get("refer_type") or 0)
        except (TypeError, ValueError):
            ref_type = 0

        references.append({
            "referenceType": ref_type,
            "referenceDocumentID": r.get("refer_document"),
            "referenceDocumentName": r.get("refer_document_name"),
        })

    return {
        "token": token,
        "attribute": attrs,           # ğŸ”‘ ä¸å†åªæœ‰ä¸€å€‹ formï¼Œè€Œæ˜¯ [èˆŠç‰ˆ..., æœ€æ–°ç‰ˆ]
        "content": content_items,
        "reference": references,
    }

@bp.get("/preview/<token>")
def preview_docx_from_snapshot(token):
    rms_id = request.args.get("rms_id")

    with db(dict_cursor=True) as (conn, cur):
        if rms_id:
            cur.execute("""
                SELECT *
                FROM rms_document_snapshots
                WHERE document_token = %s AND rms_id = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (token, rms_id))
        else:
            cur.execute("""
                SELECT *
                FROM rms_document_snapshots
                WHERE document_token = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (token,))

        snap = cur.fetchone()

    if not snap:
        return jsonify({"ok": False, "error": "snapshot not found"}), 404

    # ğŸ”¹ é€™è£¡çš„ snap æ˜¯ã€Œè¼•é‡ metaã€ï¼ŒçœŸæ­£çš„ JSON åœ¨ _build_payload_for_docx_from_snapshot è£¡è®€
    payload = _build_payload_for_docx_from_snapshot(snap)

    # å–æ–‡ä»¶é¡å‹ & åç¨±
    attr_list = payload.get("attribute") or []
    if attr_list:
        last_attr = attr_list[-1]
        doc_type = last_attr.get("documentType", 0)
        raw_name = last_attr.get("documentName") or last_attr.get("documentID") or "snapshot"
    else:
        doc_type = 0
        raw_name = "snapshot"
    doc_name = _safe_docname(raw_name)

    preview_dir = os.path.join(BASE_DIR, "_preview")
    os.makedirs(preview_dir, exist_ok=True)
    out_path = os.path.join(preview_dir, f"{doc_name}-{uuid.uuid4().hex[:8]}.docx")

    if doc_type == 1:
        get_docx(out_path, payload, "docx-template/SpecificationDocument.docx")
    else:
        get_docx(out_path, payload, "docx-template/InstructionDocument.docx")

    @after_this_request
    def remove_file(response):
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception as e:
            print("[preview_docx_from_snapshot] remove temp file error:", e)
        return response

    return send_file(
        out_path,
        as_attachment=False,
        download_name=f"{doc_name}.docx",
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    )

# ----------------------------------------------------------------------------------
def build_prefix(spec_code: str) -> str:
    """
    spec_code: e.g. "R221-01"
    å›å‚³å‰ 8 ç¢¼ï¼Œä¾‹å¦‚: "RER22101"
    """
    sc = (spec_code or "").replace("-", "")[:6]  # R221-01 â†’ R22101
    return f"RE{sc:0<6}"                         # ä¸è¶³è£œ 0

@bp.post("/program-codes/allocate")
def allocate_program_code():
    """
    body: { specCode, document_token }
    å›å‚³: { specCode, programCode, prefix, serial }
    """
    body = request.get_json(silent=True) or {}
    spec_code = (body.get("specCode") or "").strip()
    document_token = (body.get("document_token") or "").strip()

    if not spec_code or not document_token:
        return send_response(400, False, "specCode & document_token ç‚ºå¿…å¡«", None)

    prefix = build_prefix(spec_code)

    with db(dict_cursor=True) as (conn, cur):
        # âŒ ä¸è¦ç”¨ conn.start_transaction()ï¼ŒMySQLdb æ²’é€™å€‹ method
        # conn.start_transaction()

        # 1) å…ˆçœ‹æœ‰æ²’æœ‰èˆŠçš„é‡‹æ”¾è™Ÿç¢¼å¯ä»¥é‡ç”¨ï¼ˆstatus=9ï¼‰
        cur.execute("""
            SELECT id, serial_no, program_code
            FROM rms_program_code
            WHERE spec_code = %s AND status = 9
            ORDER BY serial_no ASC
            LIMIT 1
            FOR UPDATE
        """, (spec_code,))
        row = cur.fetchone()

        if row:
            # é‡ç”¨èˆŠè™Ÿç¢¼ï¼Œæ”¹æˆ reserved ç‹€æ…‹
            cur.execute("""
                UPDATE rms_program_code
                SET status = 0,
                    document_token = %s
                WHERE id = %s
            """, (document_token, row["id"]))
            # é€™è£¡å¯ä»¥ä¸å¯« conn.commit()ï¼Œäº¤çµ¦ db() åš
            serial = row["serial_no"]
            program_code = row["program_code"]
        else:
            # 2) æ²’æœ‰å¯é‡ç”¨ â†’ å–æœ€å¤§ serial_no + 1
            cur.execute("""
                SELECT MAX(serial_no) AS max_serial
                FROM rms_program_code
                WHERE spec_code = %s
                FOR UPDATE
            """, (spec_code,))
            r = cur.fetchone()
            max_serial = r["max_serial"] or 0
            serial = max_serial + 1
            program_code = f"{prefix}{serial:03d}"

            # å¯«å…¥è³‡æ–™è¡¨
            cur.execute("""
                INSERT INTO rms_program_code
                    (spec_code, serial_no, program_code, document_token, status)
                VALUES (%s, %s, %s, %s, 0)
            """, (spec_code, serial, program_code, document_token))
            # ä¸€æ¨£å¯ä»¥ä¸ç”¨æ‰‹å‹• conn.commit()

    data = {
        "specCode": spec_code,
        "programCode": program_code,
        "prefix": prefix,
        "serial": serial,
    }
    return send_response(200, True, "ç¨‹å¼è™Ÿç¢¼é…è™ŸæˆåŠŸ", data)

@bp.post("/program-codes/release")
def release_program_code():
    """
    body: { programCode }
    å°‡ status æ”¹æˆ 9ï¼Œdocument_token æ¸…ç©º â†’ ä¹‹å¾Œå¯é‡ç”¨
    """
    body = request.get_json(silent=True) or {}
    program_code = (body.get("programCode") or "").strip()

    if not program_code:
        return send_response(400, False, "programCode ç‚ºå¿…å¡«", None)

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            UPDATE rms_program_code
            SET status = 9, document_token = NULL
            WHERE program_code = %s
        """, (program_code,))
        # ä½ ä¹Ÿå¯ä»¥æª¢æŸ¥ rowcount åˆ¤æ–·æœ‰æ²’æœ‰çœŸçš„æ›´æ–°åˆ°
        conn.commit()

    return send_response(200, True, "ç¨‹å¼è™Ÿç¢¼å·²é‡‹æ”¾", {"programCode": program_code})

@bp.post("/program-codes/release-by-document")
def release_program_codes_by_document():
    """
    body: { document_token }
    å°‡è©²æ–‡ä»¶åº•ä¸‹ status=0(reserved) çš„ç¨‹å¼è™Ÿç¢¼å…¨éƒ¨æ”¹æˆ 9 ä¸¦æ¸…ç©º document_token
    ç”¨åœ¨ï¼šåˆªé™¤è‰ç¨¿ / ä½œå»¢æ–‡ä»¶æ™‚
    """
    body = request.get_json(silent=True) or {}
    document_token = (body.get("document_token") or "").strip()

    if not document_token:
        return send_response(400, False, "document_token ç‚ºå¿…å¡«", None)

    with db(dict_cursor=True) as (conn, cur):
        cur.execute("""
            UPDATE rms_program_code
            SET status = 9, document_token = NULL
            WHERE document_token = %s AND status = 0
        """, (document_token,))
        conn.commit()

    return send_response(200, True, "ç¨‹å¼è™Ÿç¢¼å·²é‡‹æ”¾", {"document_token": document_token})

# ===== helper function ===== #
def _nz(s):
    return (s or "").strip()

def _load_machine_pms_signature(machine_code: str) -> set[str]:
    """
    å¾ Oracle æ’ˆå‡ºç›®å‰æ©Ÿå°çš„ PMS baselineï¼Œè½‰æˆä¸€å€‹ set ç”¨ä¾†æ¯”å°ã€‚
    key: f"{slot_name}|{parameter_desc}({unit})"
    é€™æ¨£æ‰æœƒè·Ÿæ–‡ä»¶ content_text çš„ã€Œæ§½é«” / ç®¡ç†é …ç›®ã€å°å¾—èµ·ä¾†ã€‚
    """
    sig = set()
    if not machine_code:
        return sig

    with odb() as cur:
        cur.execute(
            """
            SELECT
                TRIM(SLOT_NAME)      AS SLOT_NAME,
                TRIM(PARAMETER_DESC) AS PARAMETER_DESC,
                TRIM(UNIT)           AS UNIT
            FROM IDBUSER.RMS_FLEX_PMS
            WHERE MACHINE_CODE = :c
              AND NVL(PARAM_COMPARE, 'N') = 'Y'
              AND NVL(SET_ATTRIBUTE, 'N') = 'Y'
            ORDER BY SLOT_NAME, PARAMETER_DESC
            """,
            c=machine_code
        )
        rows = cur.fetchall()

    for slot_name, parameter_desc, unit in rows or []:
        slot  = _nz(slot_name)
        pdesc = _nz(parameter_desc)
        u     = _nz(unit)

        # é€™è£¡å’Œ get_machine_pms_parameters_set_attribute å®Œå…¨ä¸€æ¨£
        mgmt = f"{pdesc}({u})" if u else f"{pdesc}()"
        if slot or mgmt:
            sig.add(f"{slot}|{mgmt}")
    return sig

def _load_machine_condition_signature(machine_code: str) -> set[str]:
    """
    å¾ MySQL æ’ˆå‡ºç›®å‰æ©Ÿå°çš„ Condition baselineï¼Œè½‰æˆä¸€å€‹ set ç”¨ä¾†æ¯”å°ã€‚
    key: condition_name  â†’ å°æ‡‰åˆ°æ–‡ä»¶ condition table headerã€‚
    """
    sig = set()
    if not machine_code:
        return sig

    with db() as (conn, cur):
        cur.execute(
            """
            SELECT DISTINCT
                t1.condition_name
            FROM rms_conditions t1
            INNER JOIN rms_condition_groups t2
                ON t1.condition_id = t2.condition_id
            INNER JOIN rms_group_machines t3
                ON t2.group_id = t3.group_id AND t2.condition_id = t3.condition_id
            WHERE t3.machine_id = %s
            """,
            (machine_code,)
        )
        rows = cur.fetchall()

    for (cname,) in rows or []:
        name = _nz(cname)
        if name:
            sig.add(name)
    return sig

def _parse_2d_from_text(text: str):
    """
    content_text è£¡å­˜çš„æ˜¯åƒï¼š
    [
      ["æ§½é«”","ç®¡ç†é …ç›®",...],
      ["å‰è†œ1","ä¸Šå™´é—œæ§½()", ...],
      ...
    ]
    é€™è£¡æŠŠå®ƒè½‰æˆ Python list[list[str]]ï¼Œä¸åˆæ³•å°±å› []
    """
    if not text:
        return []
    try:
        data = json.loads(text)
        if isinstance(data, list):
            return data
        return []
    except Exception:
        return []

def _build_doc_pms_signature_from_text(param_text: str) -> set[str]:
    """
    å¾åƒæ•¸è¡¨ content_text å–å‡ºã€Œæ§½é«” + ç®¡ç†é …ç›®ã€çµ„åˆç•¶ä½œæ–‡ä»¶ç•¶æ™‚çš„ PMS Signatureã€‚
    ä¾ä½ èªªçš„ï¼šç”¨ content_text çš„ [:,0:2] å°±å¤ äº†ï¼ˆç¬¬ä¸€åˆ—æ˜¯ headerï¼Œå¾ç¬¬äºŒåˆ—é–‹å§‹ï¼‰ã€‚
    """
    sig = set()
    table = _parse_2d_from_text(param_text)
    if len(table) <= 1:
        return sig

    # è·³é header (ç¬¬ 0 åˆ—)ï¼Œå¾ç¬¬ 1 åˆ—é–‹å§‹
    for row in table[1:]:
        if not isinstance(row, list) or len(row) < 2:
            continue
        tank = _nz(row[0])
        param = _nz(row[1])
        if tank or param:
            sig.add(f"{tank}|{param}")
    return sig

def _build_doc_condition_signature_from_text(cond_text: str) -> set[str]:
    """
    å¾æ¢ä»¶è¡¨ content_text å–å‡º header ç¬¬ä¸€åˆ—çš„ [1:-1] ç•¶ä½œæ–‡ä»¶ç•¶æ™‚çš„ Condition Signatureã€‚
    ä¾‹å¦‚ï¼š
      [["æ¢ä»¶åç¨±","ééŠ…åšåº¦"], ["1","10"]]
    => header = ["æ¢ä»¶åç¨±","ééŠ…åšåº¦"]
       => å– header[1:] = ["ééŠ…åšåº¦"]
    """
    sig = set()
    table = _parse_2d_from_text(cond_text)
    if not table:
        return sig

    header = table[0]
    if not isinstance(header, list) or len(header) <= 1:
        return sig

    # é€™é‚Šä¾ç…§ä½ è¬›çš„ [1:-1] æˆ– [1:] éƒ½å¯ä»¥ï¼Œçœ‹ä½ è¦ä¸è¦æ’é™¤æœ€å¾Œä¸€æ¬„
    # æˆ‘å…ˆæ¡ç”¨ [1:]ï¼ˆé€šå¸¸æœ€å¾Œä¹Ÿæœƒæ˜¯æ¢ä»¶ï¼‰ï¼Œå¦‚æœä½ æœ€å¾Œä¸€æ¬„æ˜¯ç‰¹åˆ¥æ¬„ä½ï¼Œå°±æ”¹æˆ header[1:-1]
    for col in header[1:]:
        name = _nz(col)
        if name:
            sig.add(name)
    return sig

@bp.post("/parameters/copy-source")
def copy_source_mcr():
    """
    åŠŸèƒ½ï¼šå¾å·²ç°½æ ¸çš„ Instruction æ–‡ä»¶ä¸­è¤‡è£½åƒæ•¸èˆ‡æ¢ä»¶è¡¨ã€‚
    é™åˆ¶ï¼š
    1. program_code å¿…é ˆå­˜åœ¨ã€‚
    2. ä¾†æºæ–‡ä»¶çš„æ©Ÿå°å¿…é ˆèˆ‡ base_machine_code å…·æœ‰ç›¸åŒçš„ PMS Slot è¨­ç½® (Oracle)ã€‚
    3. ä¾†æºæ–‡ä»¶çš„æ©Ÿå°å¿…é ˆèˆ‡ base_machine_code å…·æœ‰ç›¸åŒçš„ Condition Signature (MySQL)ã€‚
    4. âœ… æ–°å¢ï¼šä¾†æºæ–‡ä»¶ç•¶æ™‚çš„ PMS / Condition å…§å®¹å¿…é ˆèˆ‡ã€Œç›®å‰ baselineã€ç›¸åŒ
              ï¼ˆé¿å…è¤‡è£½åˆ°å·²ç¶“éæœŸçš„è¦æ ¼ï¼‰ã€‚
    """
    body = request.get_json(silent=True) or {}
    program_code = (body.get("program_code") or "").strip()
    base_machine_code = (body.get("base_machine_code") or "").strip()

    if not program_code or not base_machine_code:
        return send_response(400, False, "ç¼ºå°‘å¿…è¦åƒæ•¸", {"message": "è«‹æä¾›ç¨‹å¼ä»£ç¢¼èˆ‡ Base Machine Code"})

    # print(f"[DEBUG] copy_source_mcr start: program={program_code}, base={base_machine_code}")

    # ==========================================
    # STEP 1: æ‰¾å‡ºæ‰€æœ‰ "PMS ç›¸å®¹" çš„æ©Ÿå° (Oracle)
    # ==========================================
    pms_compatible_machines = set()
    try:
        with odb() as cur:
            sql = """
            WITH target_slots AS (
                SELECT SLOT_NAME FROM IDBUSER.RMS_FLEX_PMS WHERE MACHINE_CODE = :base_code
            ),
            target_count AS ( SELECT COUNT(*) as cnt FROM target_slots ),
            candidates AS (
                SELECT MACHINE_CODE, SLOT_NAME FROM IDBUSER.RMS_FLEX_PMS
            )
            SELECT DISTINCT A.MACHINE_CODE
            FROM IDBUSER.RMS_SYS_MACHINE A
            JOIN target_count tc ON 1=1
            WHERE A.ENABLED = 'Y' AND A.EQM_ID <> 'NA'
            AND (
                (tc.cnt > 0 
                 AND EXISTS (SELECT 1 FROM candidates c WHERE c.MACHINE_CODE = A.MACHINE_CODE)
                 AND NOT EXISTS (
                    SELECT 1 FROM target_slots ts 
                    WHERE NOT EXISTS (SELECT 1 FROM candidates c WHERE c.MACHINE_CODE = A.MACHINE_CODE AND c.SLOT_NAME = ts.SLOT_NAME)
                 )
                 AND NOT EXISTS (
                    SELECT 1 FROM candidates c 
                    WHERE c.MACHINE_CODE = A.MACHINE_CODE 
                    AND NOT EXISTS (SELECT 1 FROM target_slots ts WHERE ts.SLOT_NAME = c.SLOT_NAME)
                 )
                )
                OR
                (tc.cnt = 0 AND NOT EXISTS (SELECT 1 FROM candidates c WHERE c.MACHINE_CODE = A.MACHINE_CODE))
            )
            """
            cur.execute(sql, {"base_code": base_machine_code})
            rows = cur.fetchall()
            pms_compatible_machines = {row[0] for row in rows}
            pms_compatible_machines.add(base_machine_code)

    except Exception as e:
        print(f"[ERROR] Oracle PMS check failed: {e}")
        return send_response(400, False, "PMS è³‡æ–™æ¯”å°å¤±æ•—", {"message": "ç„¡æ³•é©—è­‰æ©Ÿå° PMS ç›¸å®¹æ€§"})

    # ==========================================
    # STEP 2: æ‰¾å‡º "Condition ç›¸å®¹" çš„æ©Ÿå° (MySQL)
    # ==========================================
    final_compatible_machines = []
    if not pms_compatible_machines:
        final_compatible_machines = [base_machine_code]
    else:
        try:
            with db() as (conn, cur):
                pms_list = list(pms_compatible_machines)
                union_parts = [f"SELECT '{m}' as m_code" for m in pms_list]
                union_sql = " UNION ALL ".join(union_parts)

                sql = f"""
                WITH input_machines AS (
                    {union_sql}
                ),
                machine_sigs AS (
                    SELECT 
                        im.m_code,
                        (
                            SELECT GROUP_CONCAT(rgm.condition_id ORDER BY rgm.condition_id SEPARATOR ',')
                            FROM sfdb.rms_group_machines rgm
                            WHERE rgm.machine_id = im.m_code
                        ) as sig
                    FROM input_machines im
                ),
                base_sig AS (
                    SELECT sig FROM machine_sigs WHERE m_code = %s
                )
                SELECT ms.m_code
                FROM machine_sigs ms
                JOIN base_sig bs ON (ms.sig IS NULL AND bs.sig IS NULL) OR (ms.sig = bs.sig)
                """
                cur.execute(sql, (base_machine_code,))
                rows = cur.fetchall()
                final_compatible_machines = [r[0] for r in rows]

        except Exception as e:
            print(f"[ERROR] MySQL Condition check failed: {e}")
            final_compatible_machines = [base_machine_code]

    # print(f"[DEBUG] Allowed machines: {final_compatible_machines}")

    # ==========================================
    # STEP 3: æŸ¥è©¢å·²ç°½æ ¸æ–‡ä»¶ (Source Document)
    #      + âœ… æ–°å¢ã€Œæ–‡ä»¶å…§å®¹ vs baselineã€æ¯”å°
    # ==========================================
    try:
        with db() as (conn, cur):
            sql = """
            SELECT 
                bc.document_token,
                d.attribute,
                bc.content_text      AS param_text,
                bc.content_json      AS param_json,
                (
                    SELECT sub.content_text 
                    FROM sfdb.rms_block_content sub 
                    WHERE sub.document_token = bc.document_token 
                      AND sub.step_type = 2 
                      AND sub.sub_no = 1 
                    LIMIT 1
                ) as cond_text,
                (
                    SELECT sub.content_json 
                    FROM sfdb.rms_block_content sub 
                    WHERE sub.document_token = bc.document_token 
                      AND sub.step_type = 2 
                      AND sub.sub_no = 1 
                    LIMIT 1
                ) as cond_json,
                bc.metadata
            FROM sfdb.rms_block_content bc
            JOIN sfdb.rms_document_attributes d ON d.document_token = bc.document_token
            WHERE d.status = 2
              AND d.document_type = 0
              AND bc.step_type = 2
              AND bc.sub_no = 0
              AND JSON_UNQUOTE(JSON_EXTRACT(bc.metadata, '$.kind')) = 'mcr-parameter'
              AND JSON_SEARCH(bc.metadata, 'one', %s, NULL, '$.programs[*].programCode') IS NOT NULL
            ORDER BY d.issue_date DESC
            """

            cur.execute(sql, (program_code,))
            candidates = cur.fetchall()

            target_param_json = None
            target_cond_json = None
            target_programs = []
            found_valid_doc = False

            # é å…ˆç®—å¥½ã€Œbase machine çš„ baseline signatureã€ï¼Œå¦‚æœä½ è¦ç”¨ base åšæ¯”è¼ƒä¹Ÿå¯ä»¥ï¼›
            # é€™è£¡æˆ‘æœƒç”¨ã€Œæ–‡ä»¶è£¡å¯¦éš›ä½¿ç”¨çš„é‚£å° machineã€ç•¶ baselineã€‚
            # base_pms_sig  = _load_machine_pms_signature(base_machine_code)
            # base_cond_sig = _load_machine_condition_signature(base_machine_code)

            for row in candidates:
                (doc_token, attr_str, param_text, param_json_str, cond_text, cond_json_str, meta_str) = row

                # è§£æ Attribute å–å¾—é€™ä»½æ–‡ä»¶æ‰€æ›çš„ machine æ¸…å–®
                try:
                    attr = json.loads(attr_str) if attr_str else {}
                    doc_machines = attr.get("machines", [])
                    doc_machine_codes = {m.get("code") for m in doc_machines if m.get("code")}
                except Exception as e:
                    print(f"[WARN] parse attribute failed for {doc_token}: {e}")
                    continue

                # åªæ¥å—ã€Œæ–‡ä»¶ä½¿ç”¨çš„æ©Ÿå°ã€è£¡ï¼Œè‡³å°‘æœ‰ä¸€å°åœ¨ final_compatible_machines åå–®å…§
                compatible_in_doc = doc_machine_codes.intersection(set(final_compatible_machines))
                if not compatible_in_doc:
                    continue

                # âœ… é¸ä¸€å°ã€Œæ–‡ä»¶å¯¦éš›ä½¿ç”¨ + èˆ‡ baseline ç›¸å®¹ã€çš„æ©Ÿå°ç•¶ä½œ baseline æ¯”å°æ¨™çš„
                #    ï¼ˆé€™è£¡ç°¡å–®é¸ç¬¬ä¸€å€‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¹æˆ if base_machine_code in compatible_in_doc å„ªå…ˆç”¨ baseï¼‰
                doc_machine_for_compare = None
                if base_machine_code in compatible_in_doc:
                    doc_machine_for_compare = base_machine_code
                else:
                    doc_machine_for_compare = next(iter(compatible_in_doc))

                # ------- 3.1 æ’ˆç›®å‰ baselineï¼ˆé€™å°æ©Ÿå°ï¼‰çš„ signature -------
                current_pms_sig  = _load_machine_pms_signature(doc_machine_for_compare)
                current_cond_sig = _load_machine_condition_signature(doc_machine_for_compare)

                # ------- 3.2 å¾æ–‡ä»¶å…§å®¹æŠ½å‡ºç•¶æ™‚çš„ signature -------
                doc_pms_sig  = _build_doc_pms_signature_from_text(param_text or "")
                doc_cond_sig = _build_doc_condition_signature_from_text(cond_text or "")

                # ------- 3.3 åšæ¯”å° -------
                # ------- 3.3 åšæ¯”å° + DEBUG -------
                if current_pms_sig != doc_pms_sig or current_cond_sig != doc_cond_sig:
                    # print(f"[DEBUG] doc {doc_token} skipped: PMS/Cond signature not matched.")
                    # print(f"[DEBUG]   machine_for_compare = {doc_machine_for_compare}")

                    # print(f"[DEBUG]   PMS current size = {len(current_pms_sig)}, doc size = {len(doc_pms_sig)}")
                    only_in_current_pms = list(current_pms_sig - doc_pms_sig)[:10]
                    only_in_doc_pms     = list(doc_pms_sig - current_pms_sig)[:10]
                    # print(f"[DEBUG]   PMS only_in_current (first 10): {only_in_current_pms}")
                    # print(f"[DEBUG]   PMS only_in_doc     (first 10): {only_in_doc_pms}")

                    # print(f"[DEBUG]   COND current size = {len(current_cond_sig)}, doc size = {len(doc_cond_sig)}")
                    only_in_current_cond = list(current_cond_sig - doc_cond_sig)[:10]
                    only_in_doc_cond     = list(doc_cond_sig - current_cond_sig)[:10]
                    # print(f"[DEBUG]   COND only_in_current (first 10): {only_in_current_cond}")
                    # print(f"[DEBUG]   COND only_in_doc     (first 10): {only_in_doc_cond}")

                    continue

                # ------- 3.4 é€šéæ¯”å° â†’ é€™ä»½æ–‡ä»¶æ‰æ˜¯åˆæ³•ä¾†æº -------
                found_valid_doc = True
                try:
                    target_param_json = json.loads(param_json_str) if param_json_str else None
                except Exception:
                    target_param_json = None

                try:
                    target_cond_json = json.loads(cond_json_str) if cond_json_str else None
                except Exception:
                    target_cond_json = None

                try:
                    meta = json.loads(meta_str) if meta_str else {}
                    target_programs = meta.get("programs") or []
                except Exception as e:
                    print(f"[WARN] Parse metadata failed: {e}")
                    target_programs = []

                # print(f"[DEBUG] Found compatible + up-to-date doc: {doc_token}, machines: {compatible_in_doc}")
                break

            if not found_valid_doc:
                return send_response(200, False, "æ¢ä»¶åƒæ•¸ä¸åŒç„¡æ³•è¤‡è£½", {
                    "message": "é›–æœ‰æ­¤ç¨‹å¼ä»£ç¢¼ï¼Œä½†ä¾†æºæ–‡ä»¶çš„ PMS/æ¢ä»¶å…§å®¹å·²èˆ‡ç›®å‰ baseline ä¸ä¸€è‡´ï¼Œç„¡æ³•è¤‡è£½ã€‚"
                })

            return send_response(200, True, "è¤‡è£½æˆåŠŸ", {
                "blocks": {
                    "param_json": target_param_json,
                    "cond_json": target_cond_json,
                    "source_programs": target_programs
                }
            })

    except Exception as e:
        print(f"[ERROR] Fetch doc failed: {e}")
        return send_response(500, False, "ç³»çµ±éŒ¯èª¤", {"message": str(e)})

@bp.post("/parameters/copy-spec-source")
def copy_spec_source_mcr():
    """
    è™•ç†éœ€æ±‚ 7: å¾ Specification Document è¤‡è£½åƒæ•¸
    """
    body = request.get_json(silent=True) or {}
    program_code = (body.get("program_code") or "").strip()

    if not program_code:
        return send_response(400, False, "è«‹è¼¸å…¥ç¨‹å¼ä»£ç¢¼", None)

    try:
        # -------------------------------------------------------
        # STEP 1: æ‰¾å‡ºå°æ‡‰çš„ Source Block [éœ€æ±‚ 7 & 7.4]
        # -------------------------------------------------------
        with db() as (conn, cur):
            sql = """
            SELECT 
                bc.content_json,
                bc.content_text,  -- ç”¨æ–¼è§£æç•¶ä¸‹çš„ PMS çµæ§‹
                bc.metadata,
                d.document_token
            FROM sfdb.rms_block_content bc
            JOIN sfdb.rms_document_attributes d ON d.document_token = bc.document_token
            WHERE d.status = 2            -- [éœ€æ±‚ 7] status = 2 (å·²ç°½æ ¸)
              AND d.document_type = 1     -- [éœ€æ±‚ 7] document_type = 1 (Spec Doc)
              AND bc.step_type = 5        -- [éœ€æ±‚ 7.4] step_type = 5
              AND bc.sub_no = 0           -- [éœ€æ±‚ 7.4] sub_no = 0
              AND JSON_UNQUOTE(JSON_EXTRACT(bc.metadata, '$.kind')) = 'mcr-parameter'
              AND JSON_SEARCH(bc.metadata, 'one', %s, NULL, '$.programs[*].programCode') IS NOT NULL
            LIMIT 1
            """
            cur.execute(sql, (program_code,))
            row = cur.fetchone()

            if not row:
                return send_response(200, False, "æŸ¥ç„¡æ­¤ä»£ç¢¼æˆ–æ–‡ä»¶ä¸ç¬¦åˆè¤‡è£½æ¢ä»¶ (éœ€ç‚ºå·²ç°½æ ¸è¦æ ¼æ›¸)", None)

            content_json_str, content_text_str, meta_str, doc_token = row
            
            meta = json.loads(meta_str) if meta_str else {}
            machine_code = meta.get("machine") or ""
            group_code = meta.get("machineGroup") or ""

            if not machine_code:
                return send_response(200, False, "ä¾†æºè³‡æ–™ç•°å¸¸ï¼šç„¡æ©Ÿå°è³‡è¨Š", None)

            # -------------------------------------------------------
            # STEP 2: [éœ€æ±‚ 7.1 & 7.3] PMS æ¯”å°
            # -------------------------------------------------------
            
            # 2.1 å–å¾— Oracle ç›®å‰æœ€æ–°çš„ PMS
            # [éœ€æ±‚ 7.1] PARAM_COMPARE='Y' AND SET_ATTRIBUTE='Y'
            current_pms_signature = set()
            try:
                with odb() as ora:
                    ora.execute("""
                        SELECT TRIM(SLOT_NAME), TRIM(PARAMETER_DESC)
                        FROM IDBUSER.RMS_FLEX_PMS
                        WHERE MACHINE_CODE = :m 
                          AND NVL(PARAM_COMPARE, 'N') = 'Y' 
                          AND NVL(SET_ATTRIBUTE, 'N') = 'Y'
                    """, {"m": machine_code})
                    for r in ora.fetchall():
                        # [éœ€æ±‚ 7.3] æ¯”è¼ƒ SLOT_NAME èˆ‡ PARAMETER_DESC
                        current_pms_signature.add((r[0], r[1]))
            except Exception as e:
                print(f"[PMS Check] Oracle Error: {e}")
                return send_response(400, False, "PMS é©—è­‰å¤±æ•—ï¼šç„¡æ³•é€£æ¥ MES", None)

            # 2.2 è§£æ Source Block çš„ PMS çµæ§‹ (å¾ content_text)
            source_pms_signature = set()
            try:
                # content_text æ ¼å¼ç¯„ä¾‹: [["Slot","Param",...], ["SlotA","ParamA",...]]
                text_arr = json.loads(content_text_str) if content_text_str else []
                
                # è·³é Header (ç¬¬ä¸€åˆ—)
                if len(text_arr) > 1:
                    for row_data in text_arr[1:]:
                        if len(row_data) >= 2:
                            slot = str(row_data[0]).strip()
                            # éœ€æ³¨æ„ï¼šå‰ç«¯è¡¨æ ¼ä¸­çš„ Parameter Desc å¯èƒ½åŒ…å« "(å–®ä½)"
                            # å¦‚æœ Oracle çš„ DESC æ²’æœ‰å–®ä½ï¼Œé€™è£¡æ¯”å°æœƒå¤±æ•—ã€‚
                            # å»ºè­°ï¼šå…ˆå˜—è©¦æ¯”å° Slot Nameï¼Œé€™æœ€æº–ç¢ºä¸”ä¸æ˜“å—å–®ä½é¡¯ç¤ºå½±éŸ¿
                            # [éœ€æ±‚ 7.3] è‹¥è¦åš´æ ¼æ¯”å° Descï¼Œéœ€ç¢ºä¿æ ¼å¼ä¸€è‡´
                            # é€™è£¡æˆ‘å€‘å…ˆæ¡ç”¨ Slot Name æ¯”å°ä½œç‚ºä¸»è¦ä¾æ“šï¼Œå› ç‚ºé€™æ˜¯ç¡¬é«”çµæ§‹
                            if slot:
                                source_pms_signature.add(slot)
            except Exception as e:
                print(f"[PMS Check] Parse JSON Error: {e}")

            # 2.3 åŸ·è¡Œæ¯”å°
            # ç‚ºäº†é¿å…å–®ä½æ‹¬è™Ÿé€ æˆçš„èª¤åˆ¤ï¼Œæˆ‘å€‘é€™è£¡ä¸»è¦æ¯”å° Slot æ˜¯å¦ä¸€è‡´
            current_slots = {k[0] for k in current_pms_signature}
            
            # å¦‚æœ Slot é›†åˆä¸ä¸€è‡´ï¼Œè¦–ç‚º PMS è®Šæ›´
            if source_pms_signature != current_slots:
                 return send_response(200, False, "PMSç‰ˆæœ¬ä¸ç¬¦", {
                    "message": f"æ©Ÿå° PMS è¨­å®šå·²è®Šæ›´ï¼Œç„¡æ³•è¤‡è£½ã€‚\n(ä¾†æº Slot èˆ‡ç›®å‰ MES è¨­å®šä¸ç¬¦)"
                })

            # -------------------------------------------------------
            # STEP 3: å›å‚³è³‡æ–™
            # -------------------------------------------------------
            return send_response(200, True, "è¤‡è£½æˆåŠŸ", {
                "blocks": {
                    "content_json": json.loads(content_json_str) if content_json_str else None,
                    "machine": machine_code,
                    "machineGroup": group_code,
                    # æ³¨æ„ï¼šæˆ‘å€‘ä¸å›å‚³ programCodeï¼Œå› ç‚ºå‰ç«¯è¦è‡ªå·±é…æ–°çš„ (éœ€æ±‚ 7.5)
                }
            })

    except Exception as e:
        print(f"[ERROR] copy_spec_source: {e}")
        return send_response(500, False, "ç³»çµ±éŒ¯èª¤", {"message": str(e)})
    